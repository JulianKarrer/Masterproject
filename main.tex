\RequirePackage{fix-cm}
\documentclass[oneside, a4paper]{book}
\usepackage[a4paper,width=150mm,top=25mm,bottom=25mm,bindingoffset=6mm]{geometry}

% Required for inserting images
\usepackage{eso-pic,graphicx}
% misc
\usepackage{caption}
\DeclareCaptionType{equ}[][]
%\captionsetup[equ]{labelformat=empty}
\usepackage{subcaption}
\usepackage{multicol}
\usepackage{float}
\usepackage{adjustbox} % oversized table

% appendix
\usepackage[toc]{appendix}

% Default font to sans serif
\renewcommand{\familydefault}{\sfdefault}
\RequirePackage[T1]{fontenc} 
\RequirePackage[tt=false, type1=true]{libertine} 
\RequirePackage[varqu]{zi4} 
% \RequirePackage[libertine]{newtxmath}

% chapters and headers
\usepackage[Conny]{fncychap}

\usepackage{fancyhdr}
\pagestyle{fancy}
\renewcommand{\headrulewidth}{0.1pt}
\renewcommand{\chaptermark}[1]{\markboth{\MakeUppercase{\textsf{{#1}}}}{}}
\renewcommand{\sectionmark}[1]{\markright{\MakeUppercase{\textsf{\thesection\ #1}}}{} }

% section styling
\usepackage{titlesec}

% algorithms
\usepackage{algorithm}
\usepackage{algpseudocode}

% theorems and definitions
\usepackage{amsthm}
\newtheorem{definition}{Definition}


% FAT FONTS
\usepackage{bm} % bold fonts in math mode
\newcommand\fat[1]{{\boldmath{\textbf{#1}}}}
\newcommand\emphasis[1]{{\scshape\bfseries#1}}

% mathematical fonts and graphics
\usepackage{mathtools}
\usepackage{xfrac} % sfrac for diagonal slashes in fractions
\usepackage{amsfonts} % math fonts
\usepackage{amssymb} % more math symbols (supsetneq etc.)
\usepackage{dbnsymb}
\usepackage{dsfont} % math fonts
\usepackage{bbm} % mathbb fonts
\usepackage{mathrsfs} % fancy swirly font
\usepackage{gensymb} % degree sign

% draw graphs
\usepackage[inline]{asymptote}
\usepackage{qtree}
\usepackage{tikz}

% nicer fractions
\usepackage{xfrac}
% cancel terms
\usepackage{cancel}

% units
\usepackage{siunitx}

% plots
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepgfplotslibrary{external}
\usepgfplotslibrary{patchplots}
\usepgfplotslibrary{groupplots}
\pgfplotsset{compat=1.18} 
\tikzexternalize[prefix=tikz,optimize=false]


% define the Spectral_r color scheme
\usepackage{xcolor}
\definecolor{Spectral0}{RGB}{158, 1, 66}
\definecolor{Spectral1}{RGB}{213, 62, 79}
\definecolor{Spectral2}{RGB}{244, 109, 67}
\definecolor{Spectral3}{RGB}{253, 174, 97}
\definecolor{Spectral4}{RGB}{254,224,139}
\definecolor{Spectral5}{RGB}{255,255,191}
\definecolor{Spectral6}{RGB}{230,245,152}
\definecolor{Spectral7}{RGB}{171,221,164}
\definecolor{Spectral8}{RGB}{102,204,204}
\definecolor{Spectral9}{RGB}{50,136,189}
\definecolor{Spectral10}{RGB}{94,79,162}
\pgfplotsset{
  colormap={Spectral_r}{
    rgb255(0cm)=(94,79,162)         % 0.0
    rgb255(1cm)=(50,136,189)        % 0.1
    rgb255(2cm)=(102,204,204)       % 0.2
    rgb255(3cm)=(171,221,164)       % 0.3
    rgb255(4cm)=(230,245,152)       % 0.4
    rgb255(5cm)=(255,255,191)       % 0.5
    rgb255(6cm)=(254,224,139)       % 0.6
    rgb255(7cm)=(253,174,97)        % 0.7
    rgb255(8cm)=(244,109,67)        % 0.8
    rgb255(9cm)=(213,62,79)         % 0.9
    rgb255(10cm)=(158,1,66)         % 1.0
    }
}


% get width of given text
\usepackage{calc}

% define a horizontal spacer
\newcommand\horizontalspacer[0]{\vspace{5pt}\noindent\textcolor{lightgray}{\rule{\textwidth}{1mm}}
\vspace{5pt}}

% clickable links
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
\renewcommand{\itemautorefname}{Item}
% citations
\usepackage[
  backend=biber,
  sorting=none,
  style=ieee,
  doi=false,
  % isbn=false,
  url=false,
  eprint=false
]{biblatex}
\addbibresource{refs.bib}

% fancy boxes
\usepackage{fancybox}
\newcommand{\equationnamed}[2]{%
  \setlength{\fboxsep}{2pt} % padding inside box
  \setlength{\fboxrule}{0.01pt}
  \begin{center}
    \begin{minipage}{\textwidth}
      \begin{center}\textsc{#1}\end{center}
      #2
    \end{minipage}
  \end{center}
}
\newcommand{\equationnamedbox}[2]{%
  \setlength{\fboxsep}{5pt} % padding inside box
  \setlength{\fboxrule}{0.01pt}
  \begin{center}
    \fbox{
      \begin{minipage}{0.4\textwidth}
        \begin{center}\emphasis{#1}\end{center}
        #2
      \end{minipage}
    }
  \end{center}
}

% labels to enum items
\usepackage{enumitem}

% make empty lines skip
% \usepackage{parskip}

% ~~~~~~~~~ TYPESETTING AND OTHER MACROS ~~~~~~~~~~~~~~~~~~~~~

\newenvironment{absolutelynopagebreak}
  {\par\nobreak\vfil\penalty0\vfilneg
   \vtop\bgroup}
  {\par\xdef\tpd{\the\prevdepth}\egroup
   \prevdepth=\tpd}

% ~~~~~~~~~ AFANCY CHAPTER HEADINGS ~~~~~~~~~~~~~~~~~~~~~~~~~~~
\makeatletter
\def\thickhrulefill{\leavevmode \leaders \hrule height 1ex \hfill \kern \z@}
\def\@makechapterhead#1{%
  %\vspace*{50\p@}%
  \vspace*{10\p@}%
  {\parindent \z@ \centering \reset@font
        \thickhrulefill\quad
        \scshape \@chapapp{} \thechapter
        \quad \thickhrulefill
        \par\nobreak
        \vspace*{10\p@}%
        \interlinepenalty\@M
        \hrule
        \vspace*{10\p@}%
        \Huge {\textbf{#1}} \par\nobreak
        \par
        \vspace*{10\p@}%
        \hrule
    \vskip 40\p@
    % \vskip 100\p@
  }}
\def\@makeschapterhead#1{%
  %\vspace*{50\p@}%
  \vspace*{10\p@}%
  {\parindent \z@ \centering \reset@font
        \thickhrulefill
        \par\nobreak
        \vspace*{10\p@}%
        \interlinepenalty\@M
        \hrule
        \vspace*{10\p@}%
        \Huge \bfseries #1\par\nobreak
        \par
        \vspace*{10\p@}%
        \hrule
    \vskip 40\p@
    % \vskip 100\p@
  }}

  
\usepackage[pdftex,outline]{contour}

% ~~~~~~~~~ ALGORITHM MACROS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\newcommand{\var}[1]{\text{\texttt{#1}}}
\newcommand{\func}[1]{\text{\textsl{#1}}}

\makeatletter
\newcounter{phase}[algorithm]
\newlength{\phaserulewidth}
\newcommand{\setphaserulewidth}{\setlength{\phaserulewidth}}
\newcommand{\Phase}[1]{%
  \vspace{-1.25ex}
  % Top phase rule
  \Statex\leavevmode\llap{\rule{\dimexpr\labelwidth+\labelsep}{\phaserulewidth}}\rule{\linewidth}{\phaserulewidth}
  \Statex\strut\refstepcounter{phase}\textit{Step ~\thephase~--~#1}% Phase text
  % Bottom phase rule
  % \vspace{-1.25ex}\Statex\leavevmode\llap{\rule{\dimexpr\labelwidth+\labelsep}{\phaserulewidth}}\rule{\linewidth}{\phaserulewidth}
  }

\makeatother
\makeatother

\setphaserulewidth{.1pt}

% ~~~~~~~~~ MATH MACROS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

% abs value macro
% \DeclarePairedDelimiter\abs{\lvert}{\rvert}
\newcommand\abs[1]{\left|#1\right|}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand\angled[1]{\left\langle#1\right\rangle}
\newcommand\round[1]{\left\lfloor#1\right\rceil}

\newcommand\dist[1]{\left|\left|#1\right|\right|}
\newcommand\arr[1]{\left\langle#1\right\rangle}
\newcommand\pdpi[0]{\frac{\partial}{\partial p_i}}

% define the laplace operator 
\newcommand*\Laplace{\mathop{}\!\mathbin\nabla^2}
\newcommand\vek[1]{\vec{\bm{#1}}}
\newcommand\mat[1]{{\mathds{#1}}}
\newcommand\br[1]{\left(#1\right)}

\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\erf}{erf}

\DeclareMathOperator*{\argmax}{\arg\!\max}
\DeclareMathOperator*{\argmin}{\arg\!\min}

\newcommand\divergence{{\nabla\cdot}}


% ~~~~~~~~~ START ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\captionsetup[figure]{font=footnotesize,labelfont=footnotesize,justification=centering}
\begin{document}
\begin{titlepage}
  \pagestyle{empty}

  \begin{center}
    \Huge\textbf{Incompressible SPH Fluids with Two-Way Coupled Rigid Bodies}\\
    \vspace{0.5cm}
    \Large{Julian Karrer}\\
    \vfill
    \Large
    % \contour{black}{\textcolor{white}{Lab Course}}\\
    % \contour{black}{\textcolor{white}{Master of Science in Computer Science}}\\
    \begin{figure}[H]
        \centering
        \includegraphics[width=1\textwidth]{images/title2.png}
    \end{figure}
    \vfill
    % \vspace{5.2cm}
    % \vspace{0.5cm}
    \large
    Faculty of Engineering\\
    Department of Computer Science\\
    Supervised by Prof. Dr.-Ing. Matthias Teschner\\
    \vspace{0.5cm}
    \includegraphics*[width=5cm]{images/ufr-logo.png}
  \end{center}
\end{titlepage}
% \captionsetup[figure]{font=normalsize,labelfont=normalsize}

\tableofcontents
\newpage

\chapter{Introduction}
This report aims to outline a complete implementation of a robust, fully GPU-parallel particle based solver for incompressible fluid flow problems with weakly coupled rigid bodies that can two-way interact with the fluid. The solver is efficient enough to make simulations with high spatial resolution and corresponding particle counts well into the millions feasible on a single machine with consumer hardware. Diffuse spray, foam and air bubbles are generated in a physically motivated way to enhance visual realism for simulations of sufficiently large spatial scale for such effects to occur. \\

The governing equations to be solved, namely the incompressible Navier-Stokes equations in Lagrangian form are introduced and discretized using the Smoothed Particle Hydrodynamics scheme, or SPH for short, in \autoref{chp:sph}. 

This leads to a set of equations that can be worked into a fluid solver in \autoref{chp:fluid}. Most challengingly, this solver must uphold incompressibility, leading to a pressure solver that must solve the pressure Poisson equation, in this case using a relaxed Jacobi solver with different choices of source term that lead to different behaviour of the solver. 

Once the fluid solver is completed, including one-way coupling to boundaries as discussed in \autoref{chp:rigid}, the remainder of the chapter discusses a second, rigid body solver, that uses an SPH-based surface representation to interact with the former and enable two-way interactions of rigid bodies with fluids. Challenges include a robust sampling of surfaces, the accurate calculation of the required volume and moments of mass of triangular meshes and 


\chapter{Smoothed Particle Hydrodynamics}\label{chp:sph}
    \section{Navier-Stokes Equations}\label{sec:navier-stokes}
    In order to develop a numerical solver for fluid flow problems, the respective governing equations must first be discussed. In the case of linearly viscous, incompressible fluid flow, the framework of continuum mechanics yields a set of equations called the \emphasis{Navier-Stokes equations}. Since the focus on this report is on the implementation of the solver, these equations are only very concisely outlined in the following and the interested reader is referred to more extensive works such as by \cite[Anderson]{anderson} or a previous report authored by the present writer \autocite{labcourse} for a more thorough derivation.

    \subsubsection{Lagrangian Continuum Mechanics}
    Since the flow problems in question occur not a microscopic scale, such as concerning the interaction of individual molecules, but a macroscopic scale instead, continuum mechanics is a suitable framework for the formulation of the governing equations. It assumes that the fluid is a continuum that is under deformation, wherein field quantities such as density, velocity etc. are convected with continuum elements as they deform, and makes use of the \emphasis{Material derivative} to apply the rules of calculus to these quantities \autocite{anderson}:
      \begin{equation}
        \frac{D}{Dt} := \underbrace{\frac{\partial}{\partial t}}_{\textit{local derivative}} + \underbrace{\br{\vek{v}\cdot \nabla}}_{\textit{convective derivative}}
      \end{equation}
    The first term represents the local derivative, or instantaneous rate of change of the quantity with respect to the continuum element, while the second describes the rate of change of the quantity at a fixed position due to the deformation of the continuum and consequently the convection of the continuum element with the velocity field $\vek{v}$. The frame of reference for each continuum element that is convected along with the velocity field is precisely the one in which the second term becomes zero, yielding the Lagrangian or so-called non-conservation form of the equations instead of the Eulerian form of the problem in which a static frame of reference is chosen \autocite{anderson}. We choose the Lagrangian representation in the following since the continuum itself is later discretized into particles that are advected with the flow and only quantities at particle positions will be of interest. An Eulerian discretization of the space within which the continuum exists could be chosen instead, but may be less suitable in application to free-surface flows with complex and dynamic boundary geometry \autocite{tutorial2019}.

    \subsubsection{Governing Equations}
    The governing equations can be derived from laws of conservation applied to the continuum. The first such conservation law is the \emphasis{conservation of mass} $\frac{Dm}{Dt}=0$, which can be transformed using either the Reynolds Transport theorem or the divergence theorem \autocite{labcourse} into the continuity equation of the form:
    \equationnamedbox{Continuity Equation}{
      \begin{equation}\label{eq:continuity}
        \frac{D\rho}{Dt} +\rho\br{\divergence \vek{v}} = 0
      \end{equation}
    }
    for incompressible flows, which has to hold across the entire volume integral over the fluid domain \autocite{anderson}. Since the flow is incompressible, the density of any fluid element must remain constant across a wide range of pressures, as will be assumed is the case of water. This assumption leads to two equivalent constraints posed by the continuity equation \autocite{continuum-intro}: 
    \begin{enumerate}
      \item $\frac{D\rho}{Dt}=0$, the density of any fluid element cannot change \label{enum:continuity-equation-constraint-den}
      \item $\divergence\vek{v}=0$ the velocity field of the fluid must be divergence free \label{enum:continuity-equation-constraint-vel}
    \end{enumerate}
    These expressions can be used to derive alternative source terms for the pressure solver in \autoref{sec:alternative-source-terms}.


    The second conservation law to be considered is the \emphasis{conservation of momentum}. Since momentum is conserved, the material derivative of momentum across the fluid must be balanced by conservative forces acting on the fluid. These forces can be decomposed into stresses $\vek{t}$ acting in normal direction $\hat{\vek{n}}$ on the surface of each fluid element on one side, and external body forces per unit mass $\vek{b}^{ext}$ such as gravity acting on the entire volume. Inserting the definition of the \emphasis{Cauchy-Stress Tensor} $\mat{T}$ (sometimes referred to as $\bar\sigma$) where $\mat{T}\hat{\vek{n}}=\vek{t}$ and applying the divergence theorem as well as the continuity equation to the resulting integral equation yields that for every fluid element it must hold \autocite{continuum-intro}:
    \equationnamed{Cauchy momentum equation}{
      \begin{equation}
        \rho\frac{D\vek{v}}{Dt} = \divergence\mat{T} + \rho\vek{b}^{ext}
      \end{equation}
      }

    The stress tensor $\mat{T}$ can further be decomposed into the isotropic component $\mat{T} = -p\mat{1} + \mat{V}$ where $p$ is the hydrostatic pressure and $\mat{V}$ is the deviatoric stress or viscous stress tensor \autocite{incompressible-flow-volker}. This component can be modelled in terms of the symmetric rate of deformation tensor $\mat{D}$, which in sum with its antisymmetric counterpart, the spin tensor $\mat{W}$, makes up the velocity gradient \autocite{continuum-intro}. Assuming incompressible flow and a linearly viscous or Newtonian fluid, one can obtain \autocites{tutorial2019}{incompressible-flow-volker}:
    \equationnamed{Constitutive Relation}{
      \begin{equation}
        \mat{T} = -p\mat{1} + \mu \br{\nabla\vek{v} + \br{\nabla\vek{v}}^T}
      \end{equation}
    }
    where $\mu$ is the dynamic viscosity, which is related to the kinematic viscosity $\nu$ by $\nu=\sfrac{\mu}{\rho}$. For strongly enforced incompressibility, the pressure value $p$ can be interpreted as a Lagrange multiplier chosen such that the momentum equation satisfies the continuity equation \autocite{tutorial2019}, which will be the motivation for the iterative pressure solver discussed in \autoref{chp:fluid}. Inserting the constitutive relation into the Cauchy momentum equation, rearranging and applying the Theorem of Schwarz \autocite{incompressible-flow-volker} one can finally obtain the Navier-Stokes momentum equation for incompressible, Newtonian fluids in Lagrangian form:
    \equationnamedbox{Navier Stokes Momentum Equation}{
      \begin{equation}\label{eq:navier-stokes-momentum}
        \frac{D\vek{v}}{Dt}=-\frac{1}{\rho}\nabla p + \nu\Laplace\vek{v}+\vek{b}^{ext}
      \end{equation}
    }

    Apart from the momentum and continuity equations, the conservation of energy can be used to derive the energy equation which can be used to describe the thermal processes within such flows \autocite{anderson}, however this is not usually as relevant in the application to Computer Graphics and is neglected in the following. As such, the continuity and momentum equations are the main point of focus of this report, will be referred to as the Navier-Stokes equations and solved numerically.

    % Since the flows of interest in this application are of macroscopic scale, the framework of continuum mechanics can be used, which assumes the fluid domain to be a continuously differentiable field of quantities to which the rules of calculus apply, instead of, for example, a discrete set of molecules of microscopic scale in which such assumptions would not adequately hold. One can define the \emphasis{material derivative} $\frac{D}{Dt}$ of such a field as a derivative of a field quantity with respect to time within some continuum element \autocite{anderson}:
    % \begin{definition} The material derivative:
    %   \[\frac{D}{Dt} := \frac{\partial}{\partial t} + \br{\vek{v}\cdot \nabla}\]
    % \end{definition}
    
    % Since the continuum may, and in any dynamic case does deform, the continuum element under consideration is subject to convection by the velocity field of the fluid, yielding two views of the problem: the rate of change of the quantity can be described with respect to some static position, yielding an Eulerian view of the problem, or with respect to the position of the continuum element subject to convection, such that only the changes local to the element affect the material derivative and the convective term $\vek{v}\cdot \nabla$ is precisely zero, yielding a Lagrangian framework. Since a discretization of the continuum into particles is chosen here, the Lagrangian framework is especially suitable, leading to the so-called non-conservation form of the equations.

    \newpage\section{SPH Discretization}\label{sec:sph-discretization}
    In order to numerically solve the Navier-Stokes equations, it is necessary to discretize the equations in space and time to make them tractable for simulation. As previously discussed, a Lagrangian simulation will be conducted, meaning the continuum itself is discretized into fluid elements of some mass, at which field quantities such as density, velocity and pressure must be determined and which are advected with the velocity field according to Newton's laws of motion. 
    
    Smoothed Particle Hydrodynamics, or SPH for short, is an interpolation scheme which can be used to reconstruct a continuous and differentiable field from quantities sampled at individual points that each represent some fluid volume, which we refer to as particles. This representation and the SPH scheme allow field quantities to be computed at any point of interest and at particle locations in particular, making it suited for realizing a Lagrangian simulation.

    \subsubsection{Derivation}

    The SPH method can be derived from a relaxation of the unit impulse or Dirac $\delta$ distribution, which is defined as \autocite{signal-processing-falaschi}:
    \equationnamed{Dirac $\delta$-distribution}{
      \vspace{-0.5cm}
      \begin{align}
        \int_{-\infty}^\infty\delta(x)\,dx &=1 &\textit{normalization}\\
        x\neq0 \implies \delta(x)&=0 &
      \end{align}  
    }
    and analogously for higher dimensional arguments $\delta\br{\vek  x}$. The $\delta$-distribution describes point-like samples in space, as indicated by the sampling property $f(x)\delta(x-x_s) = f(x_s)\delta(x-x_s)$ for sample positions $x_s$ from which the sifting property can be obtained \autocite{signal-processing-falaschi}:
    \begin{equation}\label{eq:sifting-property}
      f(x) = \int_{-\infty}^\infty f(x_s)\delta(x-x_s) \,dx
    \end{equation}
    Which means that by the convolution theorem \autocite{tutorial2019}:
    \begin{equation}
      f(\vek{x})  = \int_\Omega f(\vek{x}')\delta(\vek{x}-\vek{x}')\,dV' = (f\star \delta) (\vek{x})
    \end{equation}
    where $\star$ is the convolution operator on functions.
    While this serves as a precise description of a perfect sampling of a field, it is unsuited for numerical simulation since important properties such as differentiability are not given for a finite number of samples. Instead, two approximations are made to arrive at the SPH scheme:
    \begin{enumerate}
      \item The $\delta$-distribution is approximated by a kernel function $W$ with desirable properties
      \item The integral in \autoref{eq:sifting-property} is approximated by a sum over the finite fluid samples.
    \end{enumerate}

    The SPH sum at a sample position $\vek{x}_i$ then reads:
    \begin{align}
      f(\vek{x}_i) &= \int_\Omega f(\vek{x}_j)\delta(\vek{x}_i-\vek{x}_j)\,dV_j &\\ 
      &\approx \int_\Omega f(\vek{x}_j) W(\vek{x}-\vek{x}_j, \hbar)\,dV_j &\text{$\delta\br{\vek{x}_{ij}}\approx W\br{\vek{x}_{ij}, \hbar}$}\\
      &\approx \sum_{j\in\mathcal{N}_i} f(\vek{x}_j) W(\vek{x}_{ij}, \hbar) V_j  &\textit{finite sample set}
    \end{align}
    where $\mathcal{N}_i = \{j : \abs{\vek{x}_{ij}}<\hbar\}$ is the set of neighbour samples of $i$ within some radius $\hbar$, which is finite for compactly supported $W$. Denoting quantities $f(\vek{x}_i)$ as $f_i$ etc., the volume $V_j$ associated with sample $j$ can be expressed as $\frac{m_j}{\rho_j}$ for short, resulting in the standard SPH sum:
    \equationnamedbox{Standard SPH Sum}{\begin{equation}\label{eq:sph-sum}
      \angled{f_i} = \sum_{j\in\mathcal{N}_i} \frac{m_j}{\rho_j} f_j W_{ij} 
    \end{equation}}

    \subsubsection{Consistency and Normalization}
    A Taylor series error analysis of the standard SPH discretization $\angled{f_i}$ reveals that there are two conditions that must be fulfilled for $0^{\text{th}}$- and $1^{\text{st}}$-order consistency respectively to be achieved, namely \autocite{price-2012}:
    \equationnamed{Consistency Criteria}{
      \vspace{-0.3cm}
      \begin{align}
        \sum_{j\in\mathcal{N}_i} \frac{m_j}{\rho_j}W_{ij} &\overset{!}{=} 1 
        &\textit{$0^{\text{th}}$ order consistency}
        \label{eq:normalization-0th-order}\\
        \sum_{j\in\mathcal{N}_i} \frac{m_j}{\rho_j}\br{\vek{x}_j - \vek{x}_i}W_{ij} &\overset{!}{=} \vek{0}
        &\textit{$1^{\text{st}}$ order consistency}
        \label{eq:normalization-1st-order}
      \end{align}
    }
    Note that the low asymptotic error order for arbitrary samplings appears to have questionable practical relevance \autocite{tutorial2019} - perhaps because there appears to be some trade-off in Lagrangian methods between the linear error order of a discretization and the quality of the physical conservation properties and sampling isotropy caused by discrete operators, as discussed for example by \autocite[Price]{price-2012}. If desired however, the conditions can be fulfilled for normalized, symmetric kernels by multiplying a scalar to ensure \autoref{eq:normalization-0th-order} holds and solving a $3\times 3$ matrix inversion per particle to ensure \autoref{eq:normalization-1st-order} \autocites{tutorial2019}{price-2012}. Similar consistency conditions exist for SPH approximations of differential operators \autocite{price-2012}.

    \subsubsection{Kernel Function}
    The kernel function is parametrized by $\vek{x}_{ij}=\vek{x}_i-\vek{x}_j$ for samples $i,j$ and a kernel support radius $\hbar$. We denote $W_{ij} := W(\vek{x}_{ij},\hbar) $ for short. For the approximation to be consistent, $W$ must converge to $\delta$ as the support radius goes to zero, meaning the kernel must fulfil \autocite{tutorial2019}:
    \begin{align}
      \int_\Omega W(\vek{x}_{ij},\hbar) &= 1 &\textit{normalization}\\
      \lim_{\hbar\rightarrow 0} W(\vek{x}_{ij},\hbar) &= \delta(\vek{x}_{ij}) &\textit{Dirac-$\delta$ condition}
    \end{align}
    Further useful properties include \autocite{tutorial2019}:
    \begin{align}
      \abs{\vek{x}_{ij}}>\hbar &\implies W(\vek{x}_{ij},\hbar) &\textit{compact support}\\
      W(\vek{x}_{ij},\hbar) &= W(\vek{x}_{ji}, \hbar) &\textit{spherical symmetry}\label{eq:sph-property-symmetry}\\
      W(\vek{x}_{ij},\hbar) &> 0 &\textit{positivity}\\
       W(\vek{x}_{ij},\hbar) &\in C^2 &\textit{twice continuously differentiable}\label{eq:sph-property-differentiable}
    \end{align}
    since compact support can decrease the number of non-zero contributions for non-degenerate cases from $\mathcal{O}\br{N^2}$ to $\mathcal{O}\br{N}$ in the number $N$ of samples and is vital for computational efficiency. Spherical symmetry and normalization together enable methods for ensuring first order consistency \autocite{tutorial2019}. Positivity is vital for approximations of quantities such as absolute pressure, mass density etc. that must not be negative and can be neglected otherwise, while $C^2$-continuity is convenient for the discretization of second-order partial differential equations \autocite{tutorial2019}.\\

    Due to many highly convenient signal-theoretical properties of the Gaussian kernel, such as not overshooting approximated step-functions, not creating new zero-crossings of second derivatives and having optimal spatial and frequency locality in the sense of the uncertainty inequality \autocite{gauss-optimal}, many commonly employed kernel functions mimic a truncated Gaussian distribution using polynomial splines that are efficient to evaluate. In $d$ dimensions, due to symmetry and compact support to $\hbar$, all kernel functions can be represented with respect to a one-dimensional shape function $w$ and its derivative $\partial_q w$ as \autocite{band-phd}:
    \begin{align}\label{eq:kernel-definition}
      W(\vek{x}_{ij}, \hbar) &= \frac{\alpha(d)}{\hbar^d}w\br{q}\\
      \nabla W(\vek{x}_{ij}, \hbar) &= \frac{\alpha(d)}{\hbar^{d+1}}\frac{\vek{x}_{ij}}{\abs{\vek{x}_{ij}}} \partial_q w\br{q}\\
    \end{align}
    where $q = \frac{\abs{\vek{x}}}{\hbar} \in \left[0,1\right]$ and $\alpha(d)$ depends only on dimensionality. As in much of the literature, the Cubic Spline kernel is employed in this report \autocite{band-phd}:
    \begin{align}\label{eq:cubic-spline-form-func}
      w(q) &= \br{1-q}^3_+ - 4\br{\sfrac{1}{2}-q}^3_+\\
      \partial_q w(q) &= -3\br{1-q}^2_+ - 12\br{\sfrac{1}{2}-q}^2_+\\
      \alpha(3) &= \sfrac{16}{\pi}
    \end{align}
    where $(x)_+ := \max\br{0, x}$. These functions are illustrated in \autoref{fig:cubic-spline}. As is often recommended \autocite{tutorial2019}, a kernel support $\hbar=2h$ of two times the initial spacing between particles $h$ is used for all calculations.

    \begin{figure}
      \centering
      \begin{tikzpicture}[domain=0:1]
        \begin{axis}[xmin=0, xmax=1,ymin=0, ymax=1.2, grid=both,
          width=0.8\textwidth, height=0.5\textwidth]
          \addplot[red, thick, samples=100, smooth, domain=0:1]  
            {(max(0,1-\x)^3-4*max(0,0.5-\x)^3)};
          \addplot[orange, thick, samples=100, smooth, domain=0:1]  
            {(3*max(0,1-\x)^2-12*max(0,0.5-\x)^2)};
            \legend{$w(q)$, $\partial_qw(q)$,}
            \legend{$w(q)$, $\partial_qw(q)$}
        \end{axis}
      \end{tikzpicture}
      \caption{The shape function $w(q)$ that defines the Cubic Spline kernel function and its first derivative are plotted for all values of $q\in[0,1]$ that yield non-zero $w(q)$. The function is compactly supported. Since the kernel function $W$ is spherically symmetric, this one-dimensional function of the scaled distance is sufficient to describe $W$ in any dimensionality, only the scaling factors $\alpha$ have to be adjusted.}\label{fig:cubic-spline}
    \end{figure}

    \subsubsection{Differential Operators}
    The gradient of the kernel function can be used to directly to approximate differential operators such as gradient, divergence and rotation on vector fields, the least straightforward example amongst them perhaps being:
    \begin{equation}\label{eq:dyadic-product-sph-sum}
      \angled{\nabla \vek{f}_i}_\otimes = \sum_{j\in\mathcal{N}_i} \frac{m_j}{\rho_j} \vek{f}_j \otimes \nabla W_{ij}
    \end{equation}
    where $\otimes$ is the dyadic product \autocite{tutorial2019}. Such direct discretization can lead to poor approximation quality in practice, especially for low support radii $\hbar$, which has lead to the development of alternative discretizations with useful properties.

    One such discretization is the \emphasis{difference formula}, which subtracts the first Taylor series term of the discretization error to restore 0th order consistency (see \autoref{eq:normalization-0th-order}) even for suboptimal samplings, yielding a more accurate approximation for the gradient of a scalar field \autocites{tutorial2019}{price-2012}:
    \equationnamed{Difference Formula}{\begin{equation}\label{eq:sph-difference}
      \angled{\nabla f_i}_- = \angled{\nabla f_i} - f_i\angled{\nabla 1} = \sum_{j\in\mathcal{N}_i} \frac{m_j}{\rho_j} \br{f_j - f_i} \nabla W_{ij}
    \end{equation}}

    Another useful alternative discretization for the gradient of a scalar field is the \emphasis{symmetric formula}, which can be derived from the discrete Lagrangian particularly such that when applied to the pressure field for example and a standard approximation of density is used, symmetric forces that conserve linear and angular momentum can be obtained \autocites{tutorial2019}{price-2012}:
    \equationnamed{Symmetric Formula}{
      \begin{equation}\label{eq:sph-symmetric}
        \angled{\nabla f_i}_\parallel = \rho_i\sum_{j\in\mathcal{N}_i} m_j \br{\frac{f_i}{\rho_i^2} + \frac{f_j}{\rho_j^2}} \nabla W_{ij}
      \end{equation}
    }
    While this formula is generally less accurate than the difference formula, the conservation of momenta can be a benefit that in practice outweighs lacking guarantees of first or even 0th order consistency for arbitrary samplings \autocite{tutorial2019}.

    Lastly, second derivatives must be discretized, particularly the Laplace operator necessary to describe dissipative terms such as viscosity \autocite{price-2012}. However, since the second derivative of $w(q)$ varies more intensely across the kernel support, a direct discretization would have to be exceptionally well sampled to yield accurate results - meaning the approximation quality for low support radii, sparse or high-discrepancy samplings would suffer\footnote{It might help this intuition to consider the sample positions to be quasi-randomly distributed values $X$. There exist numerous examples of upper bounds on the variance of a function of a random variable in terms of some measure of how intensely that function varies. As an example, consider random values $Z_i:=f(X_i), \mathds{V}[Z]<\infty$ in terms of some function $f:\chi\mapsto\mathds{R}$ for a measurable $\chi$ and independently distributed sample positions $X_i$ from any distribution. If $f$ has the \textit{bounded difference property} $\sup_{x'} \abs{w(x)-w(x')}\leq c$ for a non-negative bound $c$, the Efron-Stein inequality implies $\mathds{V}[Z] \leq \frac{c}{4}$ (see \cite[Boucheron et al.]{bounded-variance} for more such bounds). Since the first derivative of our shape function $\partial_qw(q)$ is more tightly bound than the second $\partial_q^2 w(q)$, achieving a lower $c$, the variance of its sampling has a tighter bound, where lower variance results in better estimator performance.}. Instead, discretizations that rely on the kernel gradient approximation are typically preferred \autocite{tutorial2019}. Amongst them, the following discretization of the Laplace operator is especially useful since for a divergence-free field $\vek{f}(\vek{x})$ it can be used to derive forces that obey momentum conservation: each summand is antisymmetric in $i,j$ and all vectorial terms are projected onto the line spanned by the positions of samples $i$ and $j$ \autocite{tutorial2019}:
    \equationnamed{Laplace Operator Discretization}{
      \begin{equation}\label{eq:sph-laplace}
        \angled{\Laplace\vek{f}_i}_\Delta = 2(d+2)\sum_{j\in\mathcal{N}_i}\frac{m_j}{\rho_j} \frac{\vek{f}_{ij}\cdot\vek{x}_{ij}}{\abs{\vek{x}_{ij}}^2}\nabla  W_{ij}
      \end{equation}
    }
    where again $d$ is the dimensionality of the problem. With this, all necessary SPH discretizations for the following chapters are available.

    \subsubsection{Particle Deficiency and Free Surfaces}\label{subsec:particle-deficiency}
    One peculiarity of the SPH discretization scheme is the behaviour at free surfaces, where the fluid phase discretized into particles meets the non-discretized phase representing for example air in a liquid simulation or vacuum in simulations of gases. Since sums over fluid particles are used to interpolate the density field $\rho$, a lack of samples in the unrepresented phase can be thought of as a perfectly dense sampling with particles that have zero density. Similarly, other field quantities such as pressure, velocity etc. are also implicitly assumed to be zero in the absence of fluid particles, which is relevant for the clamping of pressure values to positive values in \autoref{subsec:jacobi-solver}. 
    
    This leads to a phenomenon called \emphasis{particle deficiency} at the free surface, where only a section of each kernel support radius is filled with neighbouring fluid particles, leading to an underestimation of the density $\rho$ at the surface. While the implicit assumption of $\rho=0$ in the non-represented phase is not a bad assumption where $\rho_{air}\ll\rho_{H_2O}$ etc., it leads to the average density across particles $\sfrac{1}{N}\sum_i^N\rho_i$, where $\rho_i$ is SPH-discretized, depending on the geometry of the fluid, as well as always being an underestimation of the 'true' average density in the fluid. This should be accounted for when average field quantities are used, such as in the convergence criteria of pressure solvers in \autoref{subsec:iisph-convergence}. An illustration of this phenomenon is shown in \autoref{fig:particle-deficiency}.


  \begin{figure}
    \centering
    \begin{asy}[width=\textwidth]

import contour;
settings.render = 0;
size(12cm, 6cm);

// Function to draw a particle with a white fill and lightblue outline
void particle(pair pos) {
  draw(circle(pos, 0.5), lightblue+opacity(0.3));
  filldraw(circle(pos, 0.005), black,black);
}

int h = 2;
pair ok = (-3, -0.5);
pair nok = (3, 2);
srand(501769823); 
real minDist = 0.9;

pair view_min = (-6,-2);
pair view_max = (6, 4);

//calculate Euclidean distance between two points
real distance(pair a, pair b) {
  return sqrt((a.x - b.x)^2 + (a.y - b.y)^2);
}
// generate Poisson-like distribution
pair[] generateParticles(real xMin, real xMax, real yMin, real yMax, real minDist, int maxPoints) {
  pair[] points;
  // Start with center point
  points.push(ok);
  points.push(nok);

  int attempts = 0;
  while (points.length < maxPoints && attempts < 10000) {
    // Generate random point in area
    real x = xMin + (xMax - xMin)*unitrand();
    real y = yMin + (yMax - yMin)*unitrand();
    pair candidate = (x, y);

    // Check distance to all existing points
    bool valid = true;
    for (pair p : points) {
      if (distance(p, candidate) < minDist) {
        valid = false;
        break;
      }
    }

    if (valid) {
      points.push(candidate);
    }
    ++attempts;
  }
  return points;
}
real yUpperBound = 2.0; 

pair[] points = generateParticles(view_min.x-3, view_max.x+3, view_min.y-1, view_max.y, minDist, 9999999);

// filter out points above yUpperBound
pair[] filteredPoints;
for (pair p : points) {
    if (p.y <= yUpperBound) {
        filteredPoints.push(p);
    }
}

// draw all filtered particles in viewport
for (pair p : filteredPoints) {
  if (
    view_min.x<=p.x && p.x <= view_max.x// && 
    //view_min.y<=p.y && p.y <= view_max.y 
  ) { 
    particle(p);
  }
}


// ISOSURFACE -------------------------------------------------

void drawSPHDensityContour(pair[] points, real threshold, real xMin, real xMax, real yMin, real yMax, real h, int nx=200, int ny=100) {
    // Create density grid (ny rows, nx columns)
    real[][] density = new real[nx][ny];
    real dx = (xMax - xMin)/nx;
    real dy = (yMax - yMin)/ny;
    real xWidth = xMax - xMin;
    real kernelScale = 2/(pi*h^2);

    for (int j = 0; j < ny; ++j) {
        real y = yMin + j*dy;
        for (int i = 0; i < nx; ++i) {
            real x = xMin + i*dx;
            density[i][j] = 0.0;
            
            for (pair p : points) {
                real deltaX = x - p.x;
                //deltaX -= xWidth * round(deltaX/xWidth);
                real deltaY = y - p.y;
                real r = sqrt(deltaX^2 + deltaY^2);
                
                if (r <= h) {
                    density[i][j] += kernelScale * (1 - (r/h)^2);
                }
            }
        }
    }

    guide[][] contours = contour(density, (xMin, yMin), (xMax, yMax), new real[] {threshold});
    
    for (guide[] contourSet : contours) {
        for (guide g : contourSet) {
            draw(g, lightblue + linewidth(1pt) + opacity(0.7));
        }
    }
}

drawSPHDensityContour(filteredPoints,  0.4, -6, 6, -2, 4, 1*h);

// ------------------------------------------------------------


// draw ok particle
filldraw(circle(ok, 0.5), lightgreen+opacity(0.5), black);
draw(circle(ok, h), dashed + black);

// labeling hbar
real phi_hbar = 0.6;
pair hlabel = h*(cos(phi_hbar), sin(phi_hbar));
draw(ok -- hlabel+ok, dashed);
label("$\hbar$", hlabel*0.5+ok, NW, fontsize(8pt));

// labeling h
real phi_h = 2.5;
pair h_vec = 0.5*(cos(phi_h), sin(phi_h));
draw(-h_vec+ok -- h_vec+ok, opacity(0.3));
label("$h$", h_vec*0.6+ok, S, fontsize(8pt));

// draw nok particle
filldraw(circle(nok, 0.5), red+opacity(0.5), black);
draw(circle(nok, h), dashed + black);
label("$\hbar$", hlabel*0.5+nok, NW, fontsize(8pt));
draw(nok -- hlabel+nok, dashed);
label("Density Isosurface", (-5,2.6), S, fontsize(7pt)+lightblue+opacity(0.7));

    \end{asy} 
    \caption{The particle deficiency phenomenon is illustrated in two dimensions, where particles positions are shown as dots with a surrounding circle of a diameter equal to the expected particle spacing $h$ (Note that particles are \textit{not} modelled as spheres but some volume of unspecified shape - only the isosurfaces of the spherically symmetric kernel function justifies this artistic choice). The particle highlighted in green lies within the fluid and has a well-sampled neighbourhood, allowing for relatively accurate estimation of the local density field through the SPH interpolation scheme. This very scheme however results in an underestimation at the free surface as highlighted for the particle shown in red which is barely inside the fluid volume, the SPH-estimated density of which however is below rest density since the lack of samples in the upper region of the kernel support is implicitly assumed to represent a density of zero, which is smoothed out across the free surface. While this effect diminishes with increasing spatial resolution and accordingly decreasing kernel support radius, leading to better resolution of the sharp discontinuity in the density field at the ideal fluid surface, the phenomenon can still lead to undesired clustering at the free surface and incorrect average densities across particles if not handled carefully, especially at lower resolutions.}
    \label{fig:particle-deficiency}
  \end{figure}
    


\chapter{Incompressible Fluid Solver}\label{chp:fluid}
  With the SPH method from \autoref{chp:sph}, the tools to discretize and numerically solve the Navier-Stokes equations outlined in \autoref{sec:navier-stokes} are available. This chapter focuses more concretely on implementing such a fluid solver and facing the challenge of ensuring the continuity equation is upheld by an incompressible fluid. This leads to the pressure Poisson equation or PPE for short, which is iteratively solved by the Implicit Incompressible SPH solver of \autoref{sec:iisph}. Multiple source terms and variations of the solver are available, which are discussed in \autoref{sec:alternative-source-terms}.
    \section{Discretization of the Navier-Stokes Equations}\label{sec:sph-navier-stokes}
    The Navier-Stokes equations for incompressible Newtonian fluids in Lagrangian form that we consider are the continuity equation \autoref{eq:continuity} and momentum equation \autoref{eq:navier-stokes-momentum}. These form a system of equations where the momentum equation provides means of calculating the acceleration $\vek{a}_i = \frac{D\vek{v}_i}{Dt}$ necessary to compute particle trajectories using Newton's second law, while the continuity equation can be seen as a constraint on the former \autocite{tutorial2019}, ensuring incompressibility. Firstly, recall the momentum equation for some particle $i$:
    \begin{equation}\label{eq:navier-stokes-annotated}
      \underbrace{\vek{a}_i  \vphantom{\frac{1}{\rho_i}\nabla}}
        _{\text{total acceleration }}=
      \underbrace{-\frac{1}{\rho_i}\nabla p_i}
        _{\text{pressure acceleration }\vek{a}_i^p} + 
      \underbrace{\nu\Laplace\vek{v}_i  \vphantom{\frac{1}{\rho_i}\nabla}}
        _{\text{viscosity acceleration }\vek{a}_i^\nu}+
      \underbrace{\vek{b}^{ext} \vphantom{\frac{1}{\rho_i}\nabla}}
        _{\text{external accelerations eg. }\vek{g}}
    \end{equation}
    Each of these terms can now be discretized using the SPH formulas from \autoref{chp:sph}. For previously discussed reasons, the viscous term may be approximated in a symmetric and accurate fashion using the discrete Laplace operator as defined in \autoref{eq:sph-laplace}:
    \begin{equation}\label{eq:discrete-viscous}
      \angled{\nu\Laplace\vek{v}_i}_\Delta = \nu\angled{\Laplace\vek{v}_i}_\Delta = 2\nu(d+2)\sum_{j\in\mathcal{N}_i}\frac{m_j}{\rho_j} \frac{\vek{v}_{ij}\cdot\vek{x}_{ij}}{\abs{\vek{x}_{ij}}^2+\epsilon}\nabla W_{ij}
    \end{equation}
    Note the additional small term $\epsilon$ in the denominator that has been added to prevent numerical instabilities and divisions by zero if the distance between two particles was close to zero - in this implementation $\epsilon=0.01h^2$ in the expected particle spacing $h$ was chosen.
    Since viscous forces tend to be dominated by pressure forces for large-scale, low viscosity or high-Reynolds simulations as this report is subject to, the pressure acceleration is the major contributor to the particles' momenta, making the symmetric formula in \autoref{eq:sph-symmetric} a robust choice to obtain physically accurate results \autocite{tutorial2019}:
    \begin{equation}\label{eq:discrete-pressure}
      \angled{-\frac{1}{\rho_i}\nabla p_i}_\parallel 
      = -\frac{1}{\rho_i}\angled{\nabla p_i}_\parallel 
      = -\sum_{j\in\mathcal{N}_i} m_j \br{\frac{p_i}{\rho_i^2} + \frac{p_j}{\rho_j^2}} \nabla W_{ij}
    \end{equation}
    The external accelerations $\vek{b}^{ext}$ in this case are simply the gravitational acceleration $\vek{g}$ with $\abs{\vek{g}}=9.81\sfrac{\si{\meter}}{\si{\second}^2}$. 

    The density $\rho_i$ in the above approximations is a scalar quantity and can be discretized using the standard SPH sum from \autoref{eq:sph-sum} \autocite{tutorial2019}:
    \begin{align}\label{eq:discrete-density}
      \angled{\rho_i}  
      = \sum_{j\in\mathcal{N}_i} \frac{m_j}{\rho_j} \rho_j W_{ij} 
      = \sum_{j\in\mathcal{N}_i} m_j W_{ij} 
    \end{align}

    Since the mass $m_i$ is set at the start of the simulation, $\nu,\vek{g}$ are given and initial $\vek{x}_i, \vek{v}_i$ are known, the only quantity in the momentum equation yet to be accounted for is the pressure field $p$. The system could be closed by employing a state equation relating the pressure directly to the density field using some stiffness parameter $k$ through the ideal gas equation $p=k\rho$ \autocite{müller-2003}, the Murnaghan equation of state \autocite{murnaghan-eos} as used in Weakly Compressible SPH \autocite{wcsph} $p=k\br{\br{\sfrac{\rho}{\rho_0}}^\gamma-1}$ or similar equations of state. Since for the incompressible fluids simulated in this report the constraints imposed by the continuity equation must be strongly enforced, pressure is instead computed iteratively by solving a pressure Poisson equation or PPE as outlined in \autoref{sec:iisph}.

    In the following, quantities $f_i$ will generally refer to the SPH-discretized fields for brevity.

    \subsubsection{Time Discretization}
    After spatial discretization using the SPH scheme, the second order partial differential equation \autoref{eq:navier-stokes-momentum} is turned into an ordinary differential equation \autocite{tutorial2019} that must be discretized in time. 
    % Equipped with an initial state $\{\forall i:\br{\vek{x}_i(t_0),\vek{v}_i(t_0),m_i(t_0)}\}$ that is assumed to be a valid state of the fluid, as well as the discrete momentum equation to calculate accelerations $\vek{a}_i(t)$ with, numerical time integration schemes can be used to solve the equation of motion and propagate the solution forwards in time.

    For this purpose, time is discretized into time steps $\Delta t$ and a numerical integration scheme is applied to obtain particle trajectories by updating particle positions and velocities. In Computer Graphics literature, the most prevalent scheme \autocites{tutorial2019}{wcsph}{iisph}{iisph-flip}{dfsph} to this end is symplectic or semi-implicit Euler time integration:
    \begin{align}\label{eq:numerical-time-integration}
      \vek{v}(t+\Delta t) &= \vek{v}(t) + \Delta t \vek{a}(t)\\
      \vek{x}(t+\Delta t) &= \vek{x}(t) + \Delta t \vek{v}(t+\Delta t)
    \end{align}

    There is a trade-off in choosing a time step size: large time steps mean more progress per solver step and can lead to greater efficiency, while time steps that are too large might lead to instability and can actually cause lower overall performance especially for iterative solvers, as will be discussed later. A common upper bound on $\Delta t$ is the Courant-Friedrichs-Lewy or CFL condition, which states that particles should not move further than some fraction $\lambda\in\left]0,1\right]$ of their size or spacing $h$ within one time step, which for a global adaptive time step size implies \autocite{tutorial2019}:
    \begin{equation}\label{eq:cfl-condition}
      \Delta t \leq \lambda \frac{h}{\max_i \abs{\vek{v}_i}}
    \end{equation}
    with a common choice \autocites{dfsph}{monaghan92} being $\lambda=0.4$. Alternative formulations based on the maximum accelerating forces \autocites{monaghan92}{time-adaptive-sph} or more elaborately derived formulas that differ per kernel function and pressure solver \autocite{optimal-timestep} exist.


    \subsubsection{Initial conditions}\label{subsec:initial-conditions}
    The numerical time integration scheme propagates a solution to the governing equations forwards in time, calculating a new valid state of the fluid at $t+\Delta t$ from a current one at time $t$. To seed this recursion, an initial state at $t_0$ must be provided. Perhaps the simplest and most common method of creating such an initial state is to define a volume filled with liquid in the simulation domain and sample it with fluid particles on a regular, square grid with spacing $h$, initial zero velocities, and a uniform rest volume $V_0=\sfrac{1}{h^d}$, or equivalently mass $m_0 = \sfrac{\rho_0 }{h^d}$ in $d$ dimensions. This method poses two challenges in particular: one is sampling an arbitrary fluid volume, the other is preventing artefacts due to the anisotropic, regular sampling and ensuring that the initial stages of the simulation run smoothly. 

    \begin{itemize}
      \item  The first problem of sampling some closed volume with fluid particles is trivial when that volume is a box. More generally, a watertight mesh that bounds the fluid volume can be provided, where candidate fluid particle positions are sampled on a regular grid of spacing $h$ within the axis-aligned bounding box of said mesh. For each such candidate position, since the mesh is watertight, a single ray cast operation is sufficient to determine whether the position is within the fluid volume or not: according to the Jordan Curve theorem, if any ray originating at the candidate position intersects the bounding mesh an odd number of times, the point lies within the volume \autocite{point-in-polygon} and a fluid particle is instantiated, otherwise the candidate is discarded\footnote{The implementation of this report uses the \texttt{trimesh.ray.ray\_pyembree.RayMeshIntersector} implementation of a ray cast operator \autocite{trimesh}}. Additionally, all candidate positions closer than $h$ to a boundary particle as described in \autoref{sec:boundary} can be discarded to prevent boundary penetration and unnaturally high pressure values at $t_0$.
      \item The second problem may be alleviated by applying a jitter on initial positions in conjunction with non-uniform particle masses \autocite{labcourse}. Sampling particles of uniform mass causes a density gradient due to particle deficiency towards the boundary of the fluid volume, which can in turn result in an erroneous pressure gradient that causes the fluid to expand towards the boundary, causing an 'explosion'. The mass of each particle can instead be chosen such that uniform rest density $\rho_0$ is achieved everywhere at $t_0$ by assigning a higher rest volume to particles at boundaries, ensuring $\forall i: \rho_i(t_0)=\rho_0\Longrightarrow p_i(t_0)=0$. It is easy to see that this can be implemented in a manner consistent with the employed SPH density approximation by iterating:
      \item \begin{align}
        m_i^0 &= \frac{1}{h^d}\\
        \angled{\rho_i}^l &\gets \sum_{j\in\mathcal{N}_i} m_j^l W_{ij} \\
        m_i^{l+1} &\gets m_i^l \cdot \frac{\rho_0}{\angled{\rho_i}^l}\label{eq:initializing-fluid-rest-density}
      \end{align}
      until convergence, which in this implementation was defined as when the average density error $\frac{1}{N}\sum_{i}\rho_i - \rho_0$ changes by less than $10^{-10}\frac{\si{\kg}}{\si{\meter}^3}$ compared to the previous iteration. Stricter bounds are unproblematic since the cost is a one-off preprocessing computation. Slow convergence can be seen as an indicator for an ill-defined scene specification. 

      This initialization to rest density enables another possible improvement in the form of jittered initial positions. A regular grid in conjunction with a kernel with support radius $\hbar$ that is an integer multiple of the grid spacing $h$ was found to be exceptionally prone to aliasing artefacts in the initial stages of simulation. Even just a jitter of $\overrightarrow{\Delta x} \sim 0.01h\cdot\mathcal{N}\br{\vek{\mu}=\vek{0}, \vek{\sigma}=\vek{1}}$ was found in this implementation to relieve numerical issues and anisotropic behaviour, while the uniform density initialization prevents the jitter from resulting in erroneous initial pressure forces. Slightly non-uniform particle masses are also suspected to persistently improve the amorphousness of the particle sampling, reducing numerical viscosity \autocite{labcourse}.
    \end{itemize}
   

    \section{The Pressure Poisson equation}
    With the discretization of the Navier-Stokes equations from \autoref{sec:sph-navier-stokes}, a simple equation-of-state based solver such as Weakly Compressible SPH or WCSPH for short, could be implemented directly. However, such a solver that computes pressure accelerations at particles only locally, from immediately neighbouring particles at each time step, can exhibit noticeable oscillations as the complexity of the scene, namely the maximum height of a column of fluid supported by the pressure, increases.

    As such a column of particles is uniformly accelerated by gravity but faces an impenetrable boundary at its bottom, a counteracting pressure gradient has to build up. Since pressure is only computed locally and the numerical speed of sound is low, particles will continue to fall, increasing measured density and thereby pressure beyond the linear gradient necessary to counteract gravity, until this superlinear gradient pushes the top of the column up beyond its hydrostatic resting position and repeating, resulting in unrealistic oscillations. In other words, it is hard to enforce incompressibility with such a solver: even if a large stiffness coefficient $k$ is chosen to counteract this effect and increase the numerical speed of sound, a correspondingly small time step size $\Delta t$ would be required to keep the system numerically stable, decreasing efficiency \autocite{iisph}. 
    
    Alternatively, the differential density deviation $\frac{D}{Dt}\rho$ could be penalized, however while no oscillations would occur, numerical errors would instead accumulate into volume drift \autocite{tutorial2019}, leading to arguably worse results where the fluid volume would irrecoverably decrease over time and accurate boundary conditions are challenging to implement. The misconception that volume oscillations are inherent to Lagrangian methods such as SPH fluid simulation can instead be interpreted as a deliberate choice enabling accurate simulation of free-surface flows where volume drift would lead to noticeable errors.
  
    To tackle this issue, the solution of the Navier-Stokes equation can be decomposed into sequential sub-problems, namely into pressure accelerations and non-pressure accelerations such as caused by gravity and viscosity \autocite{tutorial2019}. The idea is to integrate the less problematic sub-problem explicitly, while the dominating pressure accelerations can subsequently be implicitly determined to leave the fluid in a divergence-free or uncompressed predicted state at the next time step \autocite{tutorial2019}. This is also referred to as \emphasis{operator splitting} \autocite{tutorial2019} and implemented here as computing the predicted velocity $\vek{v}_i^*$ due to non-pressure accelerations:
    \begin{align}
      \vek{v}_i^*\br{t} = \vek{v}\br{t} + \Delta t\br{\vek{g} + \nu\Laplace\vek{v}_i\br{t}}\label{eq:operator-splitting}
    \end{align}
    before implicitly integrating the pressure acceleration $\vek{a}_i^p$ as described in \autoref{sec:iisph}. This equates to solving the \emphasis{Pressure Poisson equation}, or PPE for short, which will be derived and discretized in the following and is used to project the particle velocities onto an uncompressed or divergence-free state.

    \subsubsection{Derivation of the Pressure Poisson Equation}

    Pressure computation is, in the case of inviscid flows, the stiff sub-problem in the partial differential equations to solve \autocite{tutorial2019}. Pressure values that strongly enforce incompressibility, or in other words act as Lagrange multipliers to the continuity equation \autoref{eq:continuity} \autocite{tutorial2019}, should be computed efficiently. This can be done using iterative, global solvers that solve the Pressure Poisson equation, which asserts that the pressure forces do in fact enforce the continuity equation. 
    
    One way to derive the PPE is to focus on the density invariance constraint imposed by the continuity equation (see constraint \ref{enum:continuity-equation-constraint-den}). A predicted velocity $\vek{v}_i^*$ would result in positions $\vek{x}_i^*$ that can be used to compute a predicted density $\rho_i^*$ using, for example, \autoref{eq:discrete-density-boundary}. The incompressibility constraint then means that:
    \begin{equation}\label{eq:ppe-incompressibility-constraint}
      \forall i: \rho_i\br{t+\Delta t}=\rho_0
    \end{equation} 
    must hold, or that the pressure acceleration must cause a change to the predicted density that keeps the measured density $\rho_i\br{t+\Delta t}$ at the next time step at the rest density everywhere.
    
    Updating particle positions to $\vek{x}_i^*$ in a sub-step, recomputing the sets of neighbours that depend on those positions and using \autoref{eq:discrete-density-boundary} to compute $\rho_i^*$ is not done in practice, since the recalculation of neighbours is computationally expensive. Instead, the predicted density can be obtained by numerically integrating the change in density $\br{\sfrac{D\rho}{Dt}}^*$ due to $\vek{v}_i^*$ to estimate the density at the next time step \autocite{tutorial2019}:
    \begin{align}
      \rho^* &= \rho + \Delta t \br{\frac{D\rho}{D t}}^* &\textit{approximate density at next time step}\\
      \rho^*&=\rho + \Delta t \br{-\rho\divergence\vek{v}^*} &\textit{insert continuity equation \ref{eq:continuity}}\\
      \rho^* &= \rho - \Delta t \rho \divergence \vek{v}^* \label{eq:ppe-rho-star}
    \end{align}

    \autoref{eq:ppe-rho-star} encodes the fact that a velocity field with negative divergence at a point will see an increase in density, while positive divergence of the velocity field means a decrease in mass density. 

    This same formula can be used to predict a density change $\rho_p^*$ due to the pressure acceleration:
    \begin{align}
      \rho_p^* &= \rho - \Delta t \rho \divergence \br{\Delta t\vek{a}_p} &\textit{\autoref{eq:ppe-rho-star} using $\vek{v}^*_p = \Delta t \vek{a}_p$ instead of $\vek{v}^*$}\\
      &= \rho - \Delta t \rho \divergence \br{\Delta t\br{-\frac{1}{\rho}\nabla p}} &\textit{Definition of $\vek{a}_p$ from \autoref{eq:navier-stokes-annotated}}\\
      &= \rho + \Delta t^2 \divergence \br{\nabla p} &\textit{simplify}\\
      \rho_p^* &= \rho + \Delta t^2 \Laplace p &\textit{$\divergence \br{\nabla p} = \Laplace p$}\label{eq:ppe-rho-star-prs}
    \end{align}
    Finally, one can assert that adding the predicted change in density due to the pressure acceleration $\rho_p^* - \rho$ to the predicted density due to non-pressure accelerations $\rho^*$ should result in rest density $\rho_0$ at the next time step:
    \begin{align}
      \rho\br{t+\Delta t} &\overset{!}{=} \rho_0 &\textit{incompressibility constraint \autoref{enum:continuity-equation-constraint-den}, \autoref{eq:ppe-incompressibility-constraint}}\\
      \rho^* + \br{\rho_p^* - \rho} &\overset{!}{=} \rho_0 &\\
      \rho^* \cancel{+ \rho} + \Delta t^2 \Laplace p \cancel{- \rho} &\overset{!}{=} \rho_0 &\textit{insert \autoref{eq:ppe-rho-star-prs}}\\
      \Delta t \Laplace p &\overset{!}{=}\frac{\rho_0-\rho^*}{\Delta t} &\textit{rearrange}\label{eq:iisph-density-source-ppe}
    \end{align}
    which is one form of the pressure Poisson equation \autocite{tutorial2019}. 
    Basically the exact same derivation can be used with constraint \ref{enum:continuity-equation-constraint-vel} derived from the continuity equation, namely a divergence free velocity field at the next time step. Note the similarity to the derivation above:
    \begin{align}
      \divergence\br{\vek{v}\br{t+\Delta t}} &\overset{!}{=} 0&\textit{divergence free velocity constraint \ref{enum:continuity-equation-constraint-vel}}\\
      \divergence\br{\vek{v}^* - \Delta t \frac{1}{\rho}\nabla p} &\overset{!}{=} 0 &\textit{insert $\vek{v}_p^*=\Delta t \br{- \frac{1}{\rho}\nabla p}$}\\
      \divergence\vek{v}^* - \Delta t \frac{1}{\rho}\Laplace p &\overset{!}{=} 0 & \textit{linearity of divergence}\\\label{eq:velocity-divergence-source-ppe}
      \Delta t \Laplace p&\overset{!}{=} \rho \divergence\vek{v}^* &\textit{rearrange}
    \end{align}

    \section{Implicit Incompressible SPH}
    The iterative solution of the Pressure Poisson Equation in the context of SPH-based fluid simulations leads to a family of methods commonly named \emphasis{Incompressible SPH} or ISPH for short. A few choices can be made regarding the implementation of such a fluid solver, such as:
    \begin{enumerate}
      \item how to discretize the Laplace operator in the PPE \autocite{fuerstenau-laplace-discretization-comparison}
      \item whether to use density invariance (\autoref{eq:iisph-density-source-ppe})\autocite{iisph}, velocity divergence (\autoref{eq:velocity-divergence-source-ppe}) or some combination of both terms \autocites{dfsph}{optimized-source-term} as source terms for the solver
      \item which numerical solver to use
    \end{enumerate}

    This report takes the \emphasis{Implicit Incompressible SPH} or IISPH method by \autocite[Ihmsen et al.]{iisph} as its basis and answers these questions by discretizing the Laplacian using the discretized divergence of the pressure gradient, using the density invariant source term and solving the resulting system using a Relaxed Jacobi solver. Each of these components of the final pressure solver used in Algorithm \ref{alg:fluid-sim} are outlined in the following.
    
    \subsubsection{Discretizing the Pressure Poisson Equation}\label{subsec:discretizing-the-ppe}
    The focus in the following lies on the density invariance source term, or in other words discretizing the PPE:
    \begin{equation}
      \Delta t^2 \Laplace p = \rho_0-\rho^*
    \end{equation}
    using the SPH scheme so that it can be solved numerically at each discrete particle location.

    For this purpose, the following discretization of the divergence of a vector field, derived from the standard SPH sum (\autoref{eq:sph-sum}) by pulling the density into the differential operator, is commonly employed \autocites{monaghan92}{2014-survey-state-of-the-art-sph}: 
    \begin{align}
      \divergence \vek{f}&= \frac{1}{\rho}\br{\divergence \br{\vek{f}\rho} - \vek{f}\cdot\nabla\rho}
      &\textit{after \autocite[Monaghan 1992]{monaghan92}}\\
      \angled{\divergence \vek{f}} &= \frac{1}{\rho_i} \underbrace{\sum_{j\in\mathcal{N}_i}  \frac{m_j}{\rho_j} \br{\rho_j\vek{f}_j} \cdot \nabla W_{ij}}_{\angled{\divergence \br{\vek{f}\rho}}}
        - \frac{1}{\rho_i} \underbrace{\vek{f}_i \cdot \sum_{j\in\mathcal{N}_i}  m_j \nabla W_{ij}}_{\vek{f}_i\cdot \angled{\nabla\rho}}&\textit{SPH sum (\autoref{eq:sph-sum})}
    \end{align}
    which simplifies to:
    \equationnamed{SPH divergence discretization}{
      \begin{equation}
        \angled{\divergence \vek{f}}_{\divergence} = 
        -\frac{1}{\rho_i} \sum_{j\in\mathcal{N}_i}  
          m_j\vek{f}_{ij}\cdot\nabla W_{ij}
        \label{eq:sph-divergence}
      \end{equation}  
    }

    Using this, the source term $s_i=\rho_0-\rho^*_i$ of the pressure Poisson equation can be discretized as:
    \begin{align}
      \angled{\rho_0-\rho^*} &= \rho_0 - \br{\rho_i - \Delta t \rho_i \angled{\divergence \vek{v}^*_i}_\divergence} &\textit{Definition of $\rho^*$ (\autoref{eq:ppe-rho-star})}\\
      &= \rho_0 - \br{\rho_i + \Delta t \cancel{\rho_i} \br{\cancel{\frac{1}{\rho_i}} \sum_{j\in\mathcal{N}_i} m_j\vek{v}^*_{ij}\cdot\nabla W_{ij}}} &\textit{apply $\divergence$ discretization (\autoref{eq:sph-divergence})}\\
      &= \rho_0 - \rho_i - \Delta t \sum_{j\in\mathcal{N}_i}  m_j\vek{v}^*_{ij}\cdot\nabla W_{ij} &\textit{simplify}
    \end{align}
    which can be extended using the boundary handling of \autoref{chp:rigid} like in \autoref{subsub:bdy-extend-governing} to:
    \begin{equation}
      s_i = \angled{\rho_0-\rho^*} =
      \rho_0 - \rho_i 
      - \Delta t \sum_{j\in\mathcal{N}_i}  m_j\vek{v}^*_{ij}\cdot\nabla W_{ij} 
      - \Delta t \sum_{k\in\mathcal{B}_i}  m_k\vek{v}^*_{ik}\cdot\nabla W_{ik}
      \label{eq:iisph-source-term}
    \end{equation}
    where for static boundaries one can assume $\vek{v}^*_k=\vek{0}$ while for weakly coupled dynamic rigid bodies as describe in \autoref{sec:rigid-bodies} $\vek{v}^*_k \approx \vek{v}_k\br{t}$ can serve as a best guess approximation. 
    
    \horizontalspacer

    Secondly, the term $\Delta t^2\Laplace p$ needs to be discretized. To this end, a discretization consistent with the computation of the pressure acceleration in \autoref{eq:discrete-pressure-bdy} should be chosen, such that the solver computes pressures that actually translate to the predicted, uncompressed state after $\vek{a}_i^p$ has been computed from each $p_i$ \autocite{tutorial2019}:
    \begin{align}
      \angled{\Delta t^2\divergence\br{\nabla p}} &= \Delta t^2\angled{\divergence\br{-\rho_i \vek{a}_i^p}}_\divergence &\textit{since $ \vek{a}_i^p=-\frac{1}{\rho_i}\nabla p$ (\autoref{eq:navier-stokes-annotated})}\\
      &= - \rho_i \Delta t^2\angled{\divergence\br{\vek{a}_i^p}}_\divergence&\textit{use linearity}\\
      &= 
      \Delta t^2 \sum_{j\in\mathcal{N}_i} m_j \vek{a}^p_{ij} \cdot \nabla W_{ij}
      &\textit{apply $\divergence$ discretization (\autoref{eq:sph-divergence})}
    \end{align}
    which extends when considering boundary particles to a term:
    \begin{equation}
      \angled{\Delta t^2\divergence\br{\nabla p}} =
      \Delta t^2 \sum_{j\in\mathcal{N}_i} m_j \vek{a}^p_{ij} \cdot \nabla W_{ij}
      +\Delta t^2 \sum_{k\in\mathcal{B}_i} m_k \vek{a}^p_{i} \cdot \nabla W_{ik}\label{eq:iisph-ap_i}
    \end{equation}
    with the simplifying assumption that pressure accelerations $\vek{a}^p_k$ at boundary particles are zero. 
    
    Note that the discretization of the Laplace operator $\Laplace p$ using the discretization of the divergence in $\angled{\divergence\br{\nabla p}}$ is not the only possibility, and other discretizations such as the discrete Laplace operator in \autoref{eq:sph-laplace} commonly used to compute viscosities, is also thinkable. For one comparison of the discretization used in \autocite[Implicit Incompressible SPH]{iisph}, which is presented here, to \autoref{eq:sph-laplace} we refer to \autocite[Fürstenau et al.]{fuerstenau-laplace-discretization-comparison}.
    
    \subsubsection{Solving the Discretized PPE}\label{sec:iisph}
    With the discretized pressure Poisson equation obtained, one can go about solving the resulting system of $N$ equations at $N$ particles for the unknown pressure values $p_i$. Many possible solvers for this problem exist, such as Conjugate-Gradient \autocite[as alternatively proposed by Ihmsen et al.]{iisph}, and more recently Nonsmooth Nonlinear Conjugate Gradient methods as suggested by \autocite[Probst and Teschner]{monolithic-rigids-timo}.

    A relatively straightforward implementation that is robust, matrix-free, i.e. does not rely on explicitly constructing system matrices of prohibitive $\mathcal{O}\br{N^2}$ proportions, and which can be fully parallelized is achieved by a \emphasis{Relaxed Jacobi Solver} as suggested in Implicit Incompressible SPH \autocite[by Ihmsen et al.]{iisph}. 

    Remember that the pressure Poisson equation for each of the $N$ particles using the density invariance constraint as in \autoref{eq:iisph-density-source-ppe} is:
    \begin{equation}
      \angled{\Delta t^2 \Laplace p_i^2} = \rho_i-\rho_i^*
    \end{equation}
    where $\angled{\Delta t^2 \Laplace p_i^2}$ can be discretized as seen in \autoref{eq:iisph-ap_i}. This can be interpreted as a set of $N$ equations that can be written in matrix form as:
    \begin{equation}
      \mathds{A}\vek{p} = \vek{s}
    \end{equation}
    where $\vek{s}$ is the vector of all source terms $s_i= \rho_i-\rho_i^*$ which are known from \autoref{eq:iisph-source-term} and $\vek{p}$ is the vector of all unknown pressure values $p_i$. The system matrix $\mathds{A}$ then represents the discrete Laplace operator (with a factor of $\Delta t^2$) and is defined by \autoref{eq:iisph-ap_i}. '
    
    In order to solve this system of equations the Jacobi Solver in \autoref{subsec:jacobi-solver} requires:
    \begin{enumerate}
      \item each of the products $\br{\mathds{A}\vek{p}}_i$. This is precisely the same as $\angled{\Delta t^2\Laplace p_i}$ and is computed as stated in \autoref{eq:iisph-ap_i} by choosing an SPH discretization of the Laplace operator.
      \item the source terms $s_i$. This is also known, see \autoref{eq:iisph-source-term} for density invariance. Alternative source terms, such as the velocity divergence, could be used instead.
      \item the diagonal element $\mathds{A}_{ii}$. This is not yet known.
    \end{enumerate}
    The next step is therefore finding the diagonal element of the discrete Laplacian so that one can solve for the unknown pressures.

    \subsubsection{Diagonal Element $\mathds{A}_{ii}$}\label{subsec:diagonal-element}
    The diagonal element $\mathds{A}_{ii}$ of the discretized PPE is less obvious to compute, but thankfully there is a conceptually easy way to derive it from now known quantities: simply take the left-hand side of the $i$-th Pressure Poisson equation, i.e. the value $\br{\mathds{A}\vek{p}}_i$ that is known from \autoref{eq:iisph-ap_i}, and apply the partial derivative with respect to the $i$-th pressure value:
    \begin{equation}
      \mathds{A}_{ii} = \pdpi \br{\mathds{A}\vek{p}}_i
    \end{equation}
    Intuitively, the result can be thought of as asking \textit{how much does changing $p_i$ affect the predicted error at particle $i$}? Unfortunately, since the SPH-discretized expression $\br{\mathds{A}\vek{p}}_i$ is quite large, consisting of multiple SPH sums over particle neighbours, this partial derivative can be quite cumbersome to derive, so the pressure acceleration term $\vek{a}_i^p$ within it is first prepared by factoring out all occurrences of $p_i$:
    \begin{align}
      \vek{a}_i^p &= -
      \sum_{j\in\mathcal{N}_i} 
        m_j \br{\frac{p_i}{\rho_i^2} + \frac{p_j}{\rho_j^2}} \nabla W_{ij} 
      -\sum_{k\in\mathcal{B}_i} 
        m_k\frac{p_i}{\rho_i^2} \nabla W_{ik} 
      &\textit{\autoref{eq:discrete-pressure-bdy}}\\
      &= -p_i \frac{1}{\rho_i^2}\br{
        +\sum_{j\in\mathcal{N}_i} 
        m_j \nabla W_{ij} 
        +\sum_{k\in\mathcal{B}_i} 
        m_k \nabla W_{ik} 
      }
      - \sum_{j\in\mathcal{N}_i} 
        m_j\frac{p_j}{\rho_j^2} \nabla W_{ij} 
      &\textit{factor out $p_i, \sfrac{1}{\rho_i^2}$}\\
      \label{eq:iisph-a-i}
      &= -p_i \frac{1}{\rho_i^2}\br{
        \vek{g}_{ij}
        +\vek{g}_{ik}
      }
      - \sum_{j\in\mathcal{N}_i\setminus \{i\}} 
        m_j\frac{p_j}{\rho_j^2} \nabla W_{ij} 
      &\textit{use definitions \autoref{eq:iisph-g-ij-def}}
    \end{align}
    where for brevity, the following definitions were used:
    \begin{align}
      \vek{g}_{ij} &= \sum_{j\in\mathcal{N}_i\setminus \{i\}}m_j \nabla W_{ij}\label{eq:iisph-g-ij-def} \\
      \vek{g}_{ik} &= \sum_{k\in\mathcal{B}_i} m_k \nabla W_{ik}
    \end{align}
    Note that since the kernel function $W$ is differentiable and spherically symmetric (\autoref{eq:sph-property-symmetry}, \autoref{eq:sph-property-differentiable}), it follows that $\nabla W_{ii}=\vek{0}$, so the $i$-th term of the sum can be excluded and a sum over $\mathcal{N}_i\setminus \{i\}$ be used instead, which is convenient in the derivation since it implies $\pdpi \vek{g}_{ij} = \pdpi \vek{g}_{ik} = 0$.

    With this, the larger expression $\br{\mathds{A}\vek{p}}_i$ can be stated as:
    \begin{align}
      \br{\mathds{A}\vek{p}}_{i}
      ={}&\Delta t^2 \sum_{j\in\mathcal{N}_i} m_j \vek{a}^p_{ij} \cdot \nabla W_{ij}
      +\Delta t^2 \sum_{k\in\mathcal{B}_i} m_k \vek{a}^p_{i} \cdot \nabla W_{ik} 
      &\textit{\autoref{eq:iisph-ap_i}}\\
      \begin{split}
        ={}&\Delta t^2 \sum_{j\in\mathcal{N}_i} m_j \vek{a}^p_{i} \cdot \nabla W_{ij}
        -\Delta t^2 \sum_{j\in\mathcal{N}_i} m_j \vek{a}^p_{j} \cdot \nabla W_{ij} +\Delta t^2 \sum_{k\in\mathcal{B}_i} m_k \vek{a}^p_{i} \cdot \nabla W_{ik}
      \end{split}
      &\textit{split $\vek{a}^p_{ij}$}\\
      \begin{split}
        ={}&\Delta t^2 
        \br{
          \underbrace{\sum_{j\in\mathcal{N}_i\setminus \{i\}} m_j \vek{a}^p_{i} \cdot \nabla W_{ij}}_{a_i:=}
          \underbrace{-\sum_{j\in\mathcal{N}_i\setminus \{i\}} m_j \vek{a}^p_{j} \cdot \nabla W_{ij} }_{b_i:=}
          +\underbrace{\sum_{k\in\mathcal{B}_i} m_k \vek{a}^p_{i} \cdot \nabla W_{ik}}_{c_i:=}
        }
      \end{split}
      &\textit{exclude $\nabla W_{ii}=\vek{0}$}\label{eq:iisph-abc-def}
    \end{align}
    where each of the terms $a_i,b_i,c_i$ need to be partially differentiated with respect to $p_i$ to find the diagonal element:
    \begin{equation}
      \mathds{A}_{ii} = \pdpi \br{\mathds{A}\vek{p}}_{i} = \Delta t^2\br{\pdpi a_i + \pdpi b_i + \pdpi c_i}\label{eq:iisph-ai-bi-ci}
    \end{equation}
    First, the partial derivative of the term $a_i$ can be computed as:
    \begin{align*}
      \pdpi a_i &=
      \pdpi \sum_{j\in\mathcal{N}_i\setminus \{i\}} m_j \vek{a}^p_{i} \cdot \nabla W_{ij} &\textit{\autoref{eq:iisph-abc-def}}\\
      &= \pdpi \sum_{j\in\mathcal{N}_i\setminus \{i\}} m_j \br{
        -p_i \frac{1}{\rho_i^2}\br{
        \vek{g}_{ij}
        +\vek{g}_{ik}
      }
      \underbrace{
        \cancel{- \sum_{j\in\mathcal{N}_i\setminus \{i\}} 
        m_j\frac{p_j}{\rho_j^2} \nabla W_{ij} }
      }_{\textit{no $p_i$, no contribution}}
      } \cdot \nabla W_{ij} &\textit{insert \autoref{eq:iisph-a-i}}\\
      &= -\frac{1}{\rho_i^2} \sum_{j\in\mathcal{N}_i\setminus \{i\}} m_j \br{\vek{g}_{ij} +\vek{g}_{ik}
      } \cdot \nabla W_{ij} &\textit{apply partial derivative}
    \end{align*}
    Since the dot product is linear and both $\vek{g}_{ij}$ and $\vek{g}_{ik}$ are the same in every summand, they can be factored out, simplifying to:
    \begin{align}
      \pdpi a_i &= -\frac{1}{\rho_i^2} \br{\vek{g}_{ij} +\vek{g}_{ik}} \cdot \vek{g}_{ij} &\textit{apply $\vek{g}_{ij}$ definition}\label{eq:iisph-dai-dpi}
    \end{align}

    Secondly, the derivation for $\pdpi c_i$ is exactly analogous to $\pdpi  a_i$, only that indices $j\in\mathcal{N}_i\setminus \{i\}$ of fluid neighbours are replaced by indices $k\in\mathcal{B}_i$ for boundary neighbours, yielding:
    \begin{align}
      \pdpi c_i &= -\frac{1}{\rho_i^2} \br{\vek{g}_{ij} +\vek{g}_{ik}} \cdot \vek{g}_{ik} &\textit{analogous to as \autoref{eq:iisph-dai-dpi}}\label{eq:iisph-dci-dpi}
    \end{align}

    Lastly, only the partial derivative $\pdpi b_i$ remains to be computed, where $\vek{a}_j^p$ is used:
    \begin{align*}
      \pdpi b_i &=
      \pdpi - \sum_{j\in\mathcal{N}_i\setminus \{i\}} m_j \vek{a}^p_{j} \cdot \nabla W_{ij} &\textit{\autoref{eq:iisph-abc-def}}\\
      &= -\pdpi \sum_{j\in\mathcal{N}_i\setminus \{i\}} m_j 
      \underbrace{\br{
        \underbrace{\cancel{-p_j \frac{1}{\rho_j^2}\br{
          \vek{g}_{jl}
          +\vek{g}_{jk}
        }}}_{\textit{no $p_i$, no contribution}}
        - \sum_{l\in\mathcal{N}_j\setminus \{j\}} 
        m_l\frac{p_l}{\rho_l^2} \nabla W_{jl} 
      }}_{=\vek{a}_j^p} \cdot \nabla W_{ij} &\textit{insert \autoref{eq:iisph-a-i}}
    \end{align*}
    Since the acceleration $\vek{a}_j^p$ at particle $j$ is used here, the indices $l$ are introduced to refer to neighbours $l$ of each neighbour $j$ of particle $i$. Note that these second-degree neighbours of particle $i$ include the particle $i$ itself:
    \begin{equation}
      i \in \bigcup_{j\in\mathcal{N}_i\setminus\{i\}}\br{\mathcal{N}_j \setminus \{j\}  }
    \end{equation} 
    In other words, there is only one term in the nested sum that includes the pressure $p_i$ and therefore contributes to the partial derivative, namely the term where $l=i$. This means that applying the partial derivative leaves only a single term in the inner sum and the expression simplifies to:
    \begin{align}
      \pdpi b_i &= -\pdpi \sum_{j\in\mathcal{N}_i\setminus \{i\}} m_j \underbrace{\br{
        - \sum_{l\in\mathcal{N}_j\setminus \{j\}} 
        m_l\frac{p_l}{\rho_l^2} \nabla W_{jl} 
      }}_{\textit{only contributes for $l=i$}} \cdot \nabla W_{ij} &\textit{see above}\\
      &= -\pdpi \sum_{j\in\mathcal{N}_i\setminus \{i\}} m_j \br{
        - m_i\frac{p_i}{\rho_i^2} \nabla W_{ji} 
      } \cdot \nabla W_{ij} 
      &\textit{summand $l=i$ remains}\\
      &=- \pdpi \sum_{j\in\mathcal{N}_i\setminus \{i\}} m_j m_i 
        \frac{p_i}{\rho_i^2} 
        \underbrace{\nabla W_{ij} \cdot \nabla W_{ij}}_{=\abs{\nabla W_{ij}}^2}
         &\textit{$-\nabla W_{ji}=\nabla W_{ij}$}\\
      &= -\frac{m_i}{\rho_i^2} \sum_{j\in\mathcal{N}_i\setminus \{i\}} m_j  \abs{\nabla W_{ij}}^2 &\textit{apply partial derivative}
    \end{align}

    Finally, the partial derivatives $\pdpi a_i, \pdpi b_i, \pdpi c_i$ can be reinserted into \autoref{eq:iisph-ai-bi-ci} to obtain the diagonal element:
    \begin{align*}
      \mathds{A}_{ii} &= \Delta t^2\br{\pdpi a_i + \pdpi b_i + \pdpi c_i}
      &\textit{\autoref{eq:iisph-ai-bi-ci}}\\
      &= \Delta t^2\br{
        \underbrace{
          -\frac{1}{\rho_i^2} \br{\vek{g}_{ij} +\vek{g}_{ik}} \cdot \vek{g}_{ij}
        }_{\pdpi a_i}
        \underbrace{
          -\frac{1}{\rho_i^2} \br{\vek{g}_{ij} +\vek{g}_{ik}} \cdot \vek{g}_{ik}
        }_{\pdpi c_i}
        \underbrace{
          - \frac{m_i}{\rho_i^2} \sum_{j\in\mathcal{N}_i\setminus \{i\}} m_j  \abs{\nabla W_{ij}}^2
        }_{\pdpi b_i}
      }
      &\textit{reinsert}\\
      &= -\frac{\Delta t^2}{\rho_i^2} \br{
        \underbrace{
          \br{\vek{g}_{ij} +\vek{g}_{ik}} \cdot \vek{g}_{ij}
          +
          \br{\vek{g}_{ij} +\vek{g}_{ik}} \cdot \vek{g}_{ik}
        }_{\textit{binomial}}
        + \sum_{j\in\mathcal{N}_i\setminus \{i\}} m_i m_j  \abs{\nabla W_{ij}}^2
      }&\textit{factor out $-\frac{1}{\rho_i^2}$}
    \end{align*}
    Since the dot product is a linear operator, the binomial formula and the identity $\vek{x} \cdot \vek{x} = \abs{\vek{x}}^2$ can be applied to simplify:
    \begin{align}
      \br{\vek{g}_{ij} +\vek{g}_{ik}} \cdot \vek{g}_{ij}
        +
      \br{\vek{g}_{ij} +\vek{g}_{ik}} \cdot \vek{g}_{ik}
      % \\
      % &= \br{\vek{g}_{ij} +\vek{g}_{ik}} \cdot \br{\vek{g}_{ij} +\vek{g}_{ik}}\\
      &= \abs{\vek{g}_{ij} +\vek{g}_{ik}}^2
    \end{align}
    
    Reinserting the definitions of $\vek{g}_{ij},\vek{g}_{ik}$ from \autoref{eq:iisph-g-ij-def} results in:

    \equationnamed{Diagonal Element $\mathds{A}_{ii}$}{
      \vspace{-0.5cm}
      \begin{align}
        \mathds{A}_{ii} &= -\frac{\Delta t^2}{\rho_i^2} \br{
          \abs{
            \sum_{j\in\mathcal{N}_i\setminus \{i\}}m_j \nabla W_{ij} + \sum_{k\in\mathcal{B}_i} m_k \nabla W_{ik}
          }^2 
          + \sum_{j\in\mathcal{N}_i\setminus \{i\}} m_i m_j  \abs{\nabla W_{ij}}^2
        }
        \label{eq:iisph-diagonal-element}
      \end{align}
    }


    % When using relaxed Jacobi iterations, the system of $N$ equations:
    % \begin{equation}
    %   \mathds{A}\vek{p} = \vek{s}
    % \end{equation}
    % can be solved using only the product $\br{\mathds{A}\vek{p}}_i$ on the left-hand side for each particle, which is exactly \autoref{eq:iisph-ap_i}, and the diagonal element $\mathds{A}_{ii}$ of the $N\times N$ matrix $\mathds{A}$ which encodes the Laplace operator and the double integration with respect to time. The diagonal elements are can be thought of as the coefficients of $p_i$ in the $i$-th equation of $\br{\mathds{A}\vek{p}}_i$ (which is \autoref{eq:iisph-ap_i}) - the plan in the following is to write out that $i$-th expression explicitly and factor out all the coefficients of $p_i$ within it, starting with the pressure acceleration:
    % \begin{align}
    %   \vek{a}_i^p &= -
    %   \sum_{j\in\mathcal{N}_i} 
    %     m_j \br{\frac{p_i}{\rho_i^2} + \frac{p_j}{\rho_j^2}} \nabla W_{ij} 
    %   -\sum_{k\in\mathcal{B}_i} 
    %     m_k\frac{p_i}{\rho_i^2} \nabla W_{ik} 
    %   &\textit{\autoref{eq:discrete-pressure-bdy}}\\
    %   &= p_i \underbrace{\frac{1}{\rho_i^2}\br{
    %     -\sum_{j\in\mathcal{N}_i} 
    %     m_j \nabla W_{ij} 
    %     -\sum_{k\in\mathcal{B}_i} 
    %     m_k \nabla W_{ik} 
    %   }}_{=: \vek{c}_i}
    %   - \sum_{j\in\mathcal{N}_i} 
    %     m_j\frac{p_j}{\rho_j^2} \nabla W_{ij} 
    %   &\textit{factor out $p_i, \sfrac{1}{\rho_i^2}$}
    % \end{align}
    % where since the kernel is spherically symmetric (\autoref{eq:sph-property-symmetry}) and differentiable and therefore $\nabla W_{ii}=0$ holds, one can exclude the index $i$ from the set of neighbours and write:
    % \begin{align}
    %   \vek{a}_i^p &= p_i \vek{c}_i - 
    %   \sum_{j\in\mathcal{N}_i\setminus\{i\}} 
    %     m_j\frac{p_j}{\rho_j^2} \nabla W_{ij} \label{eq:iisph-a-p-c_i}
    % \end{align}
    % which makes it easier to extract the $p_i$ in the larger expression $\br{\mathds{A}\vek{p}}_{i}$ as follows:
    % \begin{align}
    %   \br{\mathds{A}\vek{p}}_{i}
    %   ={}&\Delta t^2 \sum_{j\in\mathcal{N}_i} m_j \vek{a}^p_{ij} \cdot \nabla W_{ij}
    %   +\Delta t^2 \sum_{k\in\mathcal{B}_i} m_k \vek{a}^p_{i} \cdot \nabla W_{ik} 
    %   &\textit{\autoref{eq:iisph-ap_i}}\\
    %   \begin{split}
    %     ={}&\Delta t^2 \sum_{j\in\mathcal{N}_i} m_j \vek{a}^p_{i} \cdot \nabla W_{ij}
    %     -\Delta t^2 \sum_{j\in\mathcal{N}_i} m_j \vek{a}^p_{j} \cdot \nabla W_{ij}\\
    %     &+\Delta t^2 \sum_{k\in\mathcal{B}_i} m_k \vek{a}^p_{i} \cdot \nabla W_{ik}
    %   \end{split}
    %   &\textit{split $\vek{a}^p_{ij}$}
    % \end{align}
    % Inserting \autoref{eq:iisph-a-p-c_i} for $\vek{a}_i^p, \vek{a}_j^p$ then yields:
    % \begin{align*}
    %   \br{\mathds{A}\vek{p}}_{i}={}
    %     &\Delta t^2 \sum_{j\in\mathcal{N}_i} m_j 
    %      \br{p_i \vek{c}_i 
    %      \underbrace{- \sum_{j\in\mathcal{N}_i\setminus\{i\}} m_j\frac{p_j}{\rho_j^2} \nabla W_{ij}}_{\textit{does not contribute, $j\neq i$}}
    %      } 
    %      \cdot \nabla W_{ij}
    %      &\textit{insert \autoref{eq:iisph-a-p-c_i}}\\
    %    &-\Delta t^2 \sum_{j\in\mathcal{N}_i} m_j 
    %      \br{
    %        \underbrace{p_j \vek{c}_j}_{\textit{no contribution for $i=j$ since $\nabla W_{ii}=0$}}
    %        \underbrace{- \sum_{l\in\mathcal{N}_j\setminus\{j\}} m_l\frac{p_l}{\rho_l^2} \nabla W_{jl}}_{\textit{only contributes for $l=i$}}
    %      }
    %      \cdot \nabla W_{ij}
    %      &{}\\
    %    &+\Delta t^2 \sum_{k\in\mathcal{B}_i} m_k 
    %      \br{p_i \vek{c}_i 
    %        \underbrace{- \sum_{j\in\mathcal{N}_i\setminus\{i\}} m_j\frac{p_j}{\rho_j^2} \nabla W_{ij}}_{\textit{does not contribute, $j\neq i$}}
    %      } 
    %      \cdot \nabla W_{ik}
    %      &{}
    % \end{align*}
    % Note here that the index $l$ refers to the neighbours of the neighbours of particle $i$. Exactly one of these second-degree neighbours $l$ refers back to $i=l$ and therefore contains a $p_i$ that contributes to $\mathds{A}_{ii}$, all other terms in the sum are irrelevant. Also note that sums over $\mathcal{N}_i\setminus\{i\}$ explicitly do \textit{not} contain the term $p_i$ and can therefore be completely ignored. With $\br{\mathds{A}\vek{p}}_i$ in a form that allows all coefficients of $p_i$ in the $i$-th equation of the system to be extracted, the diagonal element $\mathds{A}_{ii}$ can be obtained by taking the partial derivative with respect to $p_i$, resulting in:
    % \equationnamed{Diagonal Element $\mathds{A}_{ii}$}{
    %   \begin{align}
    %     \begin{split}
    %       \mathds{A}_{ii} = \frac{\partial \br{\mathds{A}\vek{p}}_i}{\partial p_i} &=\Delta t^2\sum_{j\in\mathcal{N}_i} m_j \vek{c}_i \cdot \nabla W_{ij}\\
    %       &+ \Delta t^2 \sum_{j\in\mathcal{N}_i} m_j \br{
    %          \frac{m_i}{\rho_i^2}\nabla W_{ji}
    %       } \cdot \nabla W_{ij}\\
    %       &+\Delta t^2 \sum_{k\in\mathcal{B}_i} m_k 
    %         \vek{c}_i
    %       \cdot \nabla W_{ik}
    %     \end{split}\label{eq:iisph-diagonal-element}\\
    %     \textit{where: \quad}\vek{c}_i &:= \frac{1}{\rho_i^2}\br{-\sum_{j\in\mathcal{N}_i} 
    %     m_j \nabla W_{ij} 
    %    -\sum_{k\in\mathcal{B}_i} 
    %     m_k \nabla W_{ik} }
    %   \end{align}
    % }
    
    The diagonal element depends only on the current positions, neighbour sets, masses and densities of the particles, all of which do not change within the simulation step, meaning the diagonal element can be computed once per time step after the density has been computed and then reused in each iteration of the iterative solver described in the following. 

    The diagonal element derived here is notably similar to the optimal stiffness coefficient $\kappa$ derived in \autocite[Divergence-Free SPH]{dfsph} for use in an iterative solver that is otherwise similar in structure to a regular state equation solver. The optimal $\kappa$ can in fact be phrased in terms of the diagonal element above as \autocite{dfsph}:
    \begin{equation}\label{eq:dfsph-kappa-di}
      \kappa_i = \frac{\rho_0 - \rho_i^*}{\mathds{A}_{ii}\rho_i}
    \end{equation}
    and another \autocite[survey paper]{2022-survey-equographics-star} shows that the density solve of DFSPH is essentially equivalent to the IISPH solver presented in this report.
    
    It is also worth noting that both the specific boundary handling method described in \autoref{chp:rigid}, namely \autocite[consistent boundaries]{consistent-boundaries} with density mirroring, as well as the semi-implicit time integration scheme used in \autoref{eq:numerical-time-integration} and in the operator splitting in \autoref{eq:operator-splitting} are used in the derivation of the expression for $\mathds{A}_{ii}$ and the diagonal element needs to be adjusted if any of these components differ.

    Lastly, a slight variation of the algorithm was implemented based on the assumption that for strongly enforced incompressibility $\rho_i=\rho_j=\rho_0$ holds, namely all densities in the pressure solver and the pressure acceleration calculation were replaced with $\rho_0$, which was hoped to result in a more robust solver. 

    \subsubsection{Relaxed Jacobi Solver}\label{subsec:jacobi-solver}
    With the source term $s_i$ shown in \autoref{eq:iisph-source-term}, the diagonal element $\mathds{A}_{ii}$ from \autoref{eq:iisph-diagonal-element}, both of which can be pre-computed, as well as the pressure acceleration $\vek{a}_i^p$ in \autoref{eq:discrete-pressure-bdy} and the right-hand side of the PPE $\br{\mathds{A}\vek{p}}_i$ in \autoref{eq:iisph-ap_i}, all the values required to implement a relaxed Jacobi solver and solve the system of equations for the unknown pressures $p_i$ are available. The core idea here is to iterate:
    \begin{equation}
      p_i^{l+1} = \br{p_i^{l} + \omega \frac{s_i-\br{\mathds{A}\vek{p}_i}}{\mathds{A}_{ii}}\cdot\mathds{1}\br{\mathds{A}_{ii}>\epsilon}}_+\label{eq:iisph-relaxed-jacobi-iteration}
    \end{equation}
    until the predicted density error per particle $\br{\rho^{err}_i}^l = \br{\mathds{A}\vek{p}}_i-s_i$ fulfils some convergence criterion. The indicator function $\mathds{1}\br{P}$ for a predicate $P$ is used to only update the pressure when the diagonal element, which the predicted density error is divided by, is greater than some positive, small threshold like $\epsilon=10^{-6}$ in order to avoid numerical instabilities and divisions by zero. Note also the use of the clamping operator $\br{x}_+ = \max\br{0, x}$ that can be viewed as a proximal operator \autocite{monolithic-rigids-timo} and ensures that pressure values are always positive. This prevents undesirable clumping artefacts that could otherwise occur when using an SPH discretization \autocite{iisph} and are speculated to result from the assumption that regions with incompletely sampled regions at free surfaces are implicitly assumed to have a pressure value of zero where there are no fluid particles (as illustrated in \autoref{fig:particle-deficiency}) \autocite{tutorial2019}. Since the solver is run until convergence, this clamping is unproblematic because it effectively shifts all pressure values up to be positive, while not changing the pressure \textit{gradient} that is actually used to compute pressure forces and accelerations \autocite{tutorial2019}.

    The choice of a relaxation parameter $\omega\in\left]0,1\right[$ is not just the namesake of the relaxed Jacobi solver but determines its convergence properties: like with any relaxation scheme, a higher value of $\omega$ leads to faster convergence, while a lower value of $\omega$ might prevent numerical instabilities. In practice, the critical value $\omega=0.5$ is used, which is the highest value that has been shown to lead to robust convergence \autocite{iisph}.

    For the initial iteration $l=0$, the authors of the \autocite[original paper]{iisph} recommend warm-starting the solver by initializing all pressure values $\vek{p}$ with half the pressure computed in the previous time step, i.e.:
    \begin{equation}
      p^0_i\br{t} = \omega_{ws}p^0_i\br{t-\Delta t_{prev}}
      \label{eq:iisph-warm-start}
    \end{equation}
    with $\omega_{ws}=0.5$. Since the initial time step $t=0$ ought to be an uncompressed state anyways, the base case of this backwards recursion is unproblematic. However, a more conservative initialization which realizes a cold start of the solver can be used instead to trade off some performance for more robust behaviour. In this case, the pressure can be more efficiently reset by inserting $p_i=0$, $\br{\mat{A}\vek{p}_i} = 0$ into \autoref{eq:iisph-relaxed-jacobi-iteration} and obtaining:
    \begin{equation}\label{eq:cold-start-pressure-init}
      p_i^0 = \frac{\omega s_i}{\mat{A}_{ii}}
    \end{equation}
    effectively skipping the first solver iteration by setting the pressure to the result thereof.
    
    The relaxed Jacobi solver as presented in \autoref{eq:iisph-relaxed-jacobi-iteration} can suffer in practice from divergence and in consequence exploding pressure values when the initial guess for the pressures $p_i^0$ is too high, so for especially complex scenes with high fluid columns and where pressures might change abruptly, cold-starting the solver might be preferable when a slightly higher iteration count per time step can be tolerated.


  \subsubsection{Convergence Criteria}\label{subsec:iisph-convergence}
  The Jacobi iteration in \autoref{eq:iisph-relaxed-jacobi-iteration} is repeated until the pressure values $p_i$ have converged to a solution that ensures incompressibility at the next time step, but the exact stopping criterion or convergence criterion has yet to be discussed. 

  In \autoref{subsec:jacobi-solver} the predicted density error $\br{\rho^{err}_i}^l = \br{\mathds{A}\vek{p}}_i-s_i$ that appears in the Jacobi update has already been mentioned. This predicted error is not exactly equal to the density error at the next time step since some approximations were made, for example when a repeated neighbourhood search was foregone in favour of calculating predicted density $\rho^*$ using the velocity divergence in \autoref{eq:ppe-rho-star}. However, the predicted density error is, by design, as close as possible to the actual density error at the next time step as can be, given the information available, and can be reused from \autoref{eq:iisph-relaxed-jacobi-iteration} at no additional computational cost, so it is often used \autocites{tutorial2019}{iisph}. 

  Two predicted error metrics are commonly employed \autocite{iisph}:
  \begin{align}
    \rho^{l}_{max} &= \frac{1}{\rho_0}\max\br{\{\forall i\in\mathcal{N}_1^N: \br{\rho^{err}_i}^l\}} 
    &\textit{Maximum predicted density error}\\
    \rho^{l}_{avg} &= \frac{1}{\rho_0}\frac{1}{N}\sum_{i=1}^N \br{\rho^{err}_i}^l
    &\textit{Average predicted density error}
  \end{align}

  The \autocite[authors of IISPH]{iisph} recommend iterating until the average predicted density error is below a threshold of $0.1\%$, such that no oscillations in the fluid are perceptible. The ratio of $\rho^{l}_{max}$ and $\rho^{l}_{avg}$ varies, so the authors argue that the keeping the maximum error fixed does not prevent visual artefacts that stem from varying average density, especially for tall fluid columns in complex scenes \autocite{iisph}. 
  
  On the other hand, recalling the particle deficiency phenomenon discussed in \autoref{subsec:particle-deficiency}, the average density can inadvertently be influenced by the geometry of the fluid, where a fluid with a large surface area and therefore many particles at the free surface can have its 'true' average density underestimated by the SPH discretization. This was found in this implementation to negatively impact simulations of, for example, shallow waters, where the solver might stop iterating prematurely because of this underestimation of average density. The maximum density metric does not suffer from this issue, so a conjunction of both convergence criteria was used in the implementation presented here to achieve more robust behaviour for arbitrary scenes. 

  Note that the maximum predicted density error is a strict upper bound of the average predicted error, so the threshold for $\rho^{l}_{max}$ poses a stricter constraint and can be relaxed to $0.5\%$ for example.

  The final convergence criterion is a boolean predicate on the predicted density errors $\br{\rho_i^{err}}^l$ in the $l$-th iteration and can be stated as:
  \begin{equation}
    \br{\rho^l_{avg} \leq \rho^{thresh}_{avg}}
    \land \br{\rho^l_{max} \leq \rho^{thresh}_{max}}
    \land \br{l_{min} \leq l \leq l_{max}}
    \label{eq:convergence-criterion}
  \end{equation}
  where minimum and maximum solver iteration counts such as $l_{min}=3, l_{max}=\infty$ can be added to increase robustness and ensure reasonably converged pressure values to use when warm-starting the next iteration. Recommended choices for the threshold values are $\rho^{thresh}_{avg}=0.1\%$ \autocite{iisph} and $\rho^{thresh}_{max}=0.5\%$.

  Using only upper bounds as thresholds, i.e. $\leq$ in \autoref{eq:convergence-criterion}, ensures that only compressions are penalized by the solver whereas a predicted density below rest density is allowed, since the particle deficiency phenomenon (see \autoref{subsec:particle-deficiency}) could otherwise cause the solver to never converge when the ratio of surface area to volume of the fluid in its current configuration is large and for example $\abs{\rho^l_{avg}} \leq \rho^{thresh}_{avg}$ could never be true. 

  % This means that technically, the lowest pressure values that satisfy the convergence predicate should be obtained by the solver, but arbitrarily larger pressure values that cause an explosion of the fluid could also satisfy the convergence criterion \autoref{eq:convergence-criterion}. This could be the reason that warm-starting the solver can lead to decreased robustness, as an initial guess of pressure values that is too high could invite a valid but unphysically large solution to the PPE. In practice however, the Jacobi solver tends to be well-behaved when used as described in this report.

  \subsubsection{IISPH Solver}\label{subsec:iisph-solver}
  Putting all the equations in \autoref{sec:iisph} that form the implicit pressure solver together with the discretized Navier-Stokes equations in \autoref{sec:sph-navier-stokes} and the boundary handling from \autoref{sec:boundary} results in Algorithm \ref{alg:fluid-sim}. 
  The rigid body solver from \autoref{sec:rigid-bodies}, which enables the two-way interaction of the fluid with dynamic rigid bodies, has also been included in the overview.

  Before the algorithm is invoked, the initial conditions of the fluid can be prepared as described in \autoref{subsec:initial-conditions} while the initialization of the boundary is discussed in \autoref{subsec:sampling-the-boundary}.

  \begin{algorithm}[H]
    \caption{IISPH Fluid Solver and Two-way Coupled Rigid Solver Step}
    \label{alg:fluid-sim}
    \begin{algorithmic}[1]
      \Phase{Preliminary Computations}
      \State Find neighbour sets $\mathcal{N}_i, \mathcal{B}_i$\Comment{Fixed Radius Neighbour Search (\autoref{appendix:fixed-radius-neighbour-search})}
      % \For{$i \in \mathds{N}_0^{N-1}$ in parallel}
        \State Compute densities $\rho_i$\Comment{\autoref{eq:discrete-density-boundary}}
      % \EndFor
      % \For{$i \in \mathds{N}_0^{N-1}$ in parallel}
        \State Compute non-pressure accelerations $\vek{a}_i^\nu,\vek{g}$\Comment{\autoref{eq:discrete-viscous}}
      % \EndFor
      \State Determine time step size $\Delta t$\Comment{\autoref{eq:cfl-condition} or \autoref{sec:optimal-time-step-controller}}
      % \For{$i \in \mathds{N}_0^{N-1}$ in parallel}
        \State Compute predicted velocities $\vek{v}_i^*$\Comment{Operator splitting, \autoref{eq:operator-splitting}}
      % \EndFor

      \Phase{IISPH - Precomputations}
      \State Compute diagonal element $\mathds{A}_{ii}$\Comment{\autoref{eq:iisph-diagonal-element}}
      \State Compute source term $s_{i}$\Comment{\autoref{eq:iisph-source-term}}
      \State Reset pressure values or warm-start $p_i$\Comment{\autoref{eq:iisph-warm-start} or \autoref{eq:cold-start-pressure-init}}

      \Phase{IISPH Solver Iteration}
      \For{iteration $l=0$ until convergence}\Comment{convergence critertion \autoref{eq:convergence-criterion}}
        \State Compute pressure acceleration $\vek{a}_i^{p,l}$ \Comment{\autoref{eq:discrete-pressure-bdy}}
        \State Compute predicted density error due to pressure accelerations $\br{\mathds{A}\vek{p}}_i^l$ \Comment{\autoref{eq:iisph-ap_i}}
        \State Update pressure using relaxed Jacobi iteration \Comment{\autoref{eq:iisph-relaxed-jacobi-iteration}}
      \EndFor

      \Phase{Numerical Time Integration}
      \State $\vek{v}\br{t+\Delta t} \gets \vek{v}_i^* +\Delta t \vek{a}_i^p$ 
      \State $\vek{x}\br{t+\Delta t} \gets \vek{x}_i +\Delta t \vek{v}\br{t+\Delta t}$ 
      \Comment{like \autoref{eq:numerical-time-integration}, respecting operator splitting}

      \Phase{Update Rigid Bodies and Dynamic Boundary Particles}
      \State Use final $\vek{a}_i^{p}$ to compute $\vek{f}_i, \vek{\tau}_i$ on rigid bodies
      \State Update rigid body state and corresponding boundary particles \Comment{Algorithm \ref{alg:rigid-sim}}

    \end{algorithmic}
  \end{algorithm}



    \section{Solver Variations and Degrees of Freedom}\label{sec:alternative-source-terms}
    There exist numerous variations of the presented IISPH solver that aim to improve solver performance or some measure of simulation accuracy or quality. Some degrees of freedom in the solver implementation that have been mentioned in this report include: 
    \begin{enumerate}
      \item the time step size to use, as limited by the CFL condition in \autoref{eq:cfl-condition} and further discussed in \autoref{sec:optimal-time-step-controller}
      \item whether to warm-start the solver using pressure values of the previous iteration, see $\omega_{ws}$ in \autoref{eq:iisph-warm-start}. Generally, cold starts tend to be more stable, while warm starts tend to increase performance
      \item using SPH-estimated densities $\rho_i, \rho_j$ or the rest density $\rho_0$ in pressure acceleration computations and the derivation of the diagonal element as mentioned in \autoref{subsec:diagonal-element}
      \item minimum and maximum iteration counts as well as error thresholds in the convergence criterion as seen in \autoref{eq:convergence-criterion}
      \item The choice of discretization of the Laplacian in the pressure Poisson equation as discussed in \autoref{subsec:discretizing-the-ppe}
    \end{enumerate}

    \horizontalspacer

    A particularly important and oft-discussed degree of freedom is the choice of \emphasis{source term} for the pressure Poisson equation. Recall that the defining constraint of the pressure solver is that pressure accelerations computed by the solver should result in a fluid state that satisfies the continuity equation (\autoref{eq:continuity}). For incompressible fluids, this equation results in two equivalent constraints: the density in the fluid should remain at rest density at all times ($\frac{D\rho}{D t}=0$) and the velocity field should remain divergence-free ($\divergence \vek{v}=0$). The IISPH solver presented in Algorithm \ref{alg:fluid-sim} solves a PPE that uses a discretized density invariance source term as stated in \autoref{eq:iisph-source-term}. 
    
    Both source term formulations have benefits and drawbacks. 
    
    Minimizing the velocity divergence can result in a more accurate and smoother velocity field with fewer oscillations and improved stability, and is often the method of choice in Eulerian approaches \autocite{dfsph}. On the other hand, errors accumulate as drift since the minimized quantity is a differential in time, meaning that only minimizing velocity divergence can lead to volume loss (as is often the case in Eulerian approaches \autocite{dfsph}) and the quality of the particle sampling can degrade over time, rendering the SPH approximations increasingly inaccurate \autocite{optimized-source-term}. Particle-based boundary representations that are often desired for their flexibility and robustness (as outlined in \autoref{sec:boundary}) can be challenging to realize in this setting due to drift causing leakage.

    Only enforcing density invariance on the other hand can result in a less accurate and temporally consistent velocity field where errors manifest as unwanted oscillations and can cause significant artificial viscosity, but preserves the quality and regularity of the particle sampling and does not suffer from volume loss \autocite{optimized-source-term}.
    
    After the IISPH method had been established, authors have sought to find pressure solvers that can leverage the strengths of each of these source term formulations without suffering from its drawbacks, resulting in \autocite[more optimized source term formulations]{optimized-source-term} and, very similarly, the \autocite[Divergence-Free SPH]{dfsph} or DFSPH method. The idea is to solve the pressure Poisson equation twice, using each source term, and combining the results. 
    The former paper achieves this by computing a velocity field that minimizes divergence, estimating an updated velocity gradient $\nabla\vek{v}$ from it by employing \autoref{eq:dyadic-product-sph-sum} and using this velocity gradient to interpolate the divergence-free velocity field computed by the first solver to the updated particle positions calculated by a second solver, which uses the density invariance source term. 

    DFSPH on the other hand was originally proposed to appear similar to an iterative Equation-of-State or EOS solver, but use a state equation $\nabla p_i = \kappa_i\nabla \rho_i$ \autocite{dfsph} with stiffness coefficients $\kappa_i$ alluded to in \autoref{eq:dfsph-kappa-di}, which are specifically chosen per-particle to realize a solution to the pressure Poisson equation in a way that is \autocite[essentially equivalent]{2022-survey-equographics-star} to the relaxed Jacobi iteration used by IISPH. Crucially, two such coefficients are calculated:
    \begin{align}
      \kappa_i &= \frac{1}{\Delta t^2}\br{\rho_i^* - \rho_0}\alpha_i \\
      \kappa_i^v &= \frac{1}{\Delta t}\frac{D \rho_i}{D t}\alpha_i
    \end{align}
    where the computationally expensive but common factor $\alpha_i = -\frac{\Delta t^2}{\rho_i\mat{A}_{ii}}$ can be reused. Each coefficient is used in a subsequent, iterative solve of the PPE using the respective source term formulation, each updating predicted velocities that are finally integrated to update particle positions. Note that the terms given in the \autocite[DFSPH paper]{dfsph} do not include boundary handling, proper clamping of values and other variations that can be critical for stability and are present in the \autocite[authors' own implementation]{SPlisHSPlasH_Library}. Since the density invariance solve of DFSPH is not just equivalent to IISPH but the main author of the DFSPH paper\autocite{dfsph} \autocite[in a later paper]{consistent-boundaries} \textit{defined} DFSPH to be a relaxed Jacobi solver as outlined in this report using the velocity divergence source term, followed by the exact density invariant solver seen in Algorithm \ref{alg:fluid-sim}, this is the form of the DFSPH solver stated in the following, rather than an equivalent form using $\kappa_i, \kappa_i^v$. The discretized velocity divergence source term was already mentioned in \autoref{eq:iisph-source-term} and is simply:
    \begin{align}\label{eq:velocity-divergence-source-term}
      s_i^v &= - \Delta t \br{\frac{D\rho}{D t}}^* =- \Delta t  \rho_0 \angled{\divergence\vek{v}^*_i}_{\divergence} \\
      &= -\Delta t \rho_0  \br{
        \sum_{j\in\mathcal{N}_i} m_j\vek{v}_{ij}^*\cdot \nabla W_{ij}
        + \sum_{k\in\mathcal{B}_i} m_j\vek{v}_{ik}^*\cdot \nabla W_{ik}
      }
    \end{align}
    This source term results in the DFSPH fluid solver outlined in Algorithm \ref{alg:dfsph}. This algorithm necessitates a second convergence criterion for the velocity divergence - in the most simple case, a fixed and low iteration count such as $l_{max} = 2$ is set and the divergence solve is simply seen as a preconditioner and pressure initializer for the later, density invariant solve, applying the concept of operator splitting once more than in IISPH. Care needs to be taken to now consider both resulting pressure accelerations from the two solvers in the computation of total forces $\vek{f}_i$ exerted by the fluid on dynamic rigid bodies.


  \begin{algorithm}[H]
    \caption{DFSPH Fluid Solver and Two-way Coupled Rigid Solver Step}
    \label{alg:dfsph}
    \begin{algorithmic}[1]
      \Phase{Preliminary Computations}
      \State Find neighbour sets $\mathcal{N}_i, \mathcal{B}_i$\Comment{Fixed Radius Neighbour Search (\autoref{appendix:fixed-radius-neighbour-search})}
      % \For{$i \in \mathds{N}_0^{N-1}$ in parallel}
        \State Compute densities $\rho_i$\Comment{\autoref{eq:discrete-density-boundary}}
      % \EndFor
      % \For{$i \in \mathds{N}_0^{N-1}$ in parallel}
        \State Compute non-pressure accelerations $\vek{a}_i^\nu,\vek{g}$\Comment{\autoref{eq:discrete-viscous}}
      % \EndFor
      \State Determine time step size $\Delta t$\Comment{\autoref{eq:cfl-condition} or \autoref{sec:optimal-time-step-controller}}
      % \For{$i \in \mathds{N}_0^{N-1}$ in parallel}
        \State Compute predicted velocities $\vek{v}_i^*$\Comment{Operator splitting, \autoref{eq:operator-splitting}}
      % \EndFor

      \Phase{Precomputations}
      \State Compute diagonal element $\mathds{A}_{ii}$\Comment{\autoref{eq:iisph-diagonal-element}}
      \State Compute density invariance source term $s_{i}$\Comment{\autoref{eq:iisph-source-term}}
      \State Reset pressure values or warm-start $p_i$\Comment{\autoref{eq:iisph-warm-start} or \autoref{eq:cold-start-pressure-init}}


      \Phase{Velocity Divergence Solver}
      \For{iteration $l=0$ until convergence}
        \State Compute pressure acceleration $\vek{a}_i^{p,l}$ \Comment{\autoref{eq:discrete-pressure-bdy}}
        \State Compute predicted density error due to pressure accelerations $\br{\mathds{A}\vek{p}}_i^l$ \Comment{\autoref{eq:iisph-ap_i}}
        \State Update pressure using relaxed Jacobi iteration \Comment{\autoref{eq:iisph-relaxed-jacobi-iteration} with $s_i^v$ from \autoref{eq:velocity-divergence-source-term}}
      \EndFor
      \State $\vek{v}_i^* \gets\vek{v}_i^* + \Delta t \vek{a}_i^{p}$ \Comment{Update predicted velocities}
      \State Accumulate the used $\vek{a}_i^{p}$ to compute $\vek{f}_i$ on rigid bodies

      \Phase{Density Invariance Solver}
      \For{iteration $l=0$ until convergence}\Comment{convergence critertion \autoref{eq:convergence-criterion}}
        \State Compute pressure acceleration $\vek{a}_i^{p,l}$ \Comment{\autoref{eq:discrete-pressure-bdy}}
        \State Compute predicted density error due to pressure accelerations $\br{\mathds{A}\vek{p}}_i^l$ \Comment{\autoref{eq:iisph-ap_i}}
        \State Update pressure using relaxed Jacobi iteration \Comment{\autoref{eq:iisph-relaxed-jacobi-iteration} with $s_i$ from \autoref{eq:iisph-source-term}}
      \EndFor

      \Phase{Numerical Time Integration}
      \State $\vek{v}\br{t+\Delta t} \gets \vek{v}_i^* +\Delta t \vek{a}_i^p$ 
      \State $\vek{x}\br{t+\Delta t} \gets \vek{x}_i +\Delta t \vek{v}\br{t+\Delta t}$ 
      \Comment{like \autoref{eq:numerical-time-integration}, respecting operator splitting}

      \Phase{Update Rigid Bodies and Dynamic Boundary Particles}
      \State Use further accumulated $\vek{a}_i^{p}$ to compute $\vek{f}_i, \vek{\tau}_i$ on rigid bodies
      \State Update rigid body state and corresponding boundary particles \Comment{Algorithm \ref{alg:rigid-sim}}

    \end{algorithmic}
  \end{algorithm}



    \subsection{Performance Optimizing Metropolis Time Step Selection}\label{sec:optimal-time-step-controller}
    Given the amount of degrees of freedom in the implementation of an incompressible SPH solver, the question of how to robustly measure the relative performances of solver variations naturally arises. In the literature there appears to be no clear consensus on how to measure such differences in performance and stability, or the comparisons that are conducted can be called into question due to the amount of free parameters: \textit{how can one be sure that suboptimal relative performance is due to an algorithmic disadvantage and not suboptimal selection of free parameters?}

    One such parameter in particular is critical for the algorithm's performance, namely the time step size $\Delta t$. While the Courant-Friedrichs-Lewy condition in \autoref{eq:cfl-condition} does put an upper limit on the time step size, a couple of issues arise:
    \begin{enumerate}
      \item For scenes such as a resting column of water under hydrostatic pressure, the maximum velocity $\max_i\abs{\vek{v}_i}$ is close to zero, enabling close to arbitrary time step sizes. Commonly, a maximum time step size $\Delta t_{max}$ is introduced to circumvent this.
      \item The CFL condition only provides a safe upper bound on the time step size but says nothing about the optimality of any given $\Delta t$
    \end{enumerate}

    It is clear that too small a time step size results in suboptimal performance, since an error within the bounds of the convergence criterion could also have been achieved using a larger time step at a given iteration count. Similarly, a time step size that is too large must be suboptimal, since even assuming it is still in the bounds enforced by the CFL condition that guarantee stability, the required number of solver iterations for larger time steps sizes will grow disproportionally after some point. Assuming the solver converges not linearly, but makes most progress in its first few iterations and then eases towards convergence, a $\Delta t$ requiring many solver iterations must at some point become less efficient than using fewer solver iterations at the cost of making smaller steps. If the constant computational cost per time step is large and the pressure solve is cheap, a larger time step size tends to be more efficient, but if the cost of the pressure solve is significant compared to constant costs per step, as can be the case for incompressible SPH in particular, a lower iteration count might be desirable.

    \subsubsection{Defining Performance from Noisy Measurements}

    To further formalize the problem of performance-optimal time step selection, the notion of solver performance can be defined as:
    \begin{equation}\label{eq:solver-performance}
      P = \frac{T_{sim}}{T_{comp}}
    \end{equation}
    where $T_{sim}$ is the simulated time and can be exactly computed by summing the $\Delta t$ of subsequent time steps under consideration, while $T_{comp}$ is some measurement of the wall-clock-time or process time taken by the solver to compute the same steps. This means that for a given hardware setup, scene complexity and particle count, $P\geq 1$ means 'real-time' performance, while for challenging scenes a $P$ much less than one can be expected.

    Unfortunately, simply measuring the execution time of a single time step has limitations: due to inherent imprecisions of the timing method itself, differing resource allocations over time and other factors outside the control of the fluid solver, performance measurement of single steps can be noisy and yield high variance. Instead, a model for the estimated performance of a time step is suggested here, which takes into account the structure of the fluid solver in Algorithm \ref{alg:fluid-sim} to filter some of this noise:
    \begin{equation}\label{eq:perf-cost-model}
      \hat{T}_{comp} \approx \bar{T}_{const} + l \cdot \bar{T}_{iter} + w
    \end{equation}
    Here, the estimated computation time $\hat{T}_{comp}$ is decomposed into a constant component per time step $T_{const}$, which comprises calculations such as neighbourhood search, computation of densities $\rho_i$, non-pressure accelerations, pre-computed values such as the source terms $s_i$ and diagonal elements $\mathds{A}_{ii}$ and so on, which can be expected to take very similar computational effort at each step. 
    Similarly, the computation time per iteration of the pressure solver $T_{iter}$ can be assumed to be near-constant, approximated by its long-term average $\bar{T}_{iter}$ and multiplied by the iteration count $l$ in the current step, leaving only a noise term $w$, which can reasonably be modelled as a Gaussian, additive noise accounting for any variation in the measurement that does appear.

    
    The assumption that $T_{const}$ and $T_{iter}$ are near-constant across time steps is supported by a couple of observations. Note that the sorting-based neighbourhood search outlined in \autoref{appendix:fixed-radius-neighbour-search} comprises a very predictable and near-constant number of operations in $\mathcal{O}\br{N}$ in the number $N$ of particles. The SPH sums can also be assumed to take near-identical computational effort in each step since the maximum number of neighbours of each particle $M$ is near-constant for many scenes when incompressibility is strongly enforced, and neighbourhoods of particles tend to be filled as resolution increases, making the SPH sums which are in $\mathcal{O}\br{MN}$ very predictable in runtime. This is further supported by the frequent resorting of particle attribute buffers enabled by the counting sort in \autoref{appendix:fixed-radius-neighbour-search}, which enhances memory coherence and makes performance degradation due to scattered read and write operations much less significant, thus keeping the computational lower and more constant. 
    
    This means that assuming noisy observations in the $k$-th time step $T_{const,k}$ and $T_{iter,k}$ that are symmetrically spread about some constant, expected value, an unbiased Monte-Carlo estimate of the expected values $\bar{T}_{const}, \bar{T}_{iter}$ that converges to the true constant can be obtained by the long-term average values:
    \begin{align}
      \bar{T}_{const} &= \frac{1}{K}\sum_{k=0}^K T_{const,k}\\
      \bar{T}_{iter} &= \frac{1}{\sum_{k=0}^K l_k}\sum_{k=0}^K  l_k \cdot T_{iter,k}
    \end{align}
    where $K$ is the total number of time steps observed so far and $l_k$ is the solver iteration count of the $k$-th time step. This leads to an estimated performance metric of the $k$-th time step:
    \equationnamed{Estimated Performance Score}{
      \begin{equation}\label{eq:perf-hat-defintion}
        \hat{P}_k = \frac{\Delta t_k}{\bar{T}_{const} + l_k \cdot \bar{T}_{iter}}
      \end{equation}
    }
    
    \subsubsection{Problem Statement and Outlook}

    If there was an algorithm that chose time step sizes $\Delta t_k$ automatically as to optimize $\hat{P}_k$, the relative performance of algorithms could be compared robustly and empirically without the choice of the upwards-bound but otherwise free parameter $\Delta t$, which directly affects the simulation performance as defined in \autoref{eq:perf-hat-defintion}. Furthermore, this optimization task has practical relevance as it would increase performance within the bounds on stability and simulation quality imposed by the CFL-condition and convergence criterion respectively and at the same time eliminate hand-tuning of parameters of the fluid solver related to time step size. When appropriate, the solver would simply choose to use a lower-than-possible time step size to achieve maximal performance at no cost to quality. 

    Unfortunately, this optimization problem appears to have some challenging features. It can generally be understood as a constrained, non-linear, possibly non-convex, black-box optimization of a time-dependent and noisy function in a delayed feedback loop, placing the problem at a particularly challenging intersection of Artificial Intelligence (and reinforcement learning in particular), control theory, the study of time-inhomogeneous Markov decision problems, numerical optimization and related fields. Despite the arguably great difficulty of this general formulation of the problem, many simplifying assumptions can reasonably be taken that result in a feasible algorithm for automatic time step selection as described in the following, which already shows promising results. Avenues of future improvement to this first approximation might include:
    \begin{enumerate}
      \item Considering the problem as an instance of \autocite[online learning for black box optimization]{bbo-bayesian-phd} or controller tuning and applying a form of \autocite[Time-Varying Bayesian Optimization]{time-varying-bo} or similar \autocite[time-varying optimization]{tvopt-library}
      \item Considering the time step controller an agent in a changing environment and applying Reinforcement Learning approaches to it 
      \item Lifting the veil of black-box optimization by modelling the performance function (as already partly done in \autoref{eq:perf-cost-model}), taking a closer analysis of the convergence behaviour of the solver into account or calculating a target iteration count (which can be controlled using a simple feedback loop) based on the ratio of constant to per-iteration costs.
    \end{enumerate} 


    \subsubsection{Greedy Time Step Size Control}
    The performance function changes with time. Consider for example a scene with a flat, water-filled basin flowing into a tall container: at first, the maximum height of any water column in the scene is low, but it later increases, changing the performance-optimal time step size. However, it can be reasonably assumed that such changes to $\hat{P}_{k,t}$ occur on a timescale much larger than that of individual time steps $\Delta t$, for example on the scale of seconds in scenes where $\Delta t$ is on the scale of milliseconds. For now, it can be assumed that $\hat{P}_k$ is approximately constant on the scale of more time steps than necessary to find an optimum of $\hat{P}$. Perhaps the simplest method for finding such an optimum would then be a greedy \emphasis{Monte Carlo Local Search}. This method explores values in the neighbourhood of the current time step size $\Delta t_k$ using random sampling from some proposal distribution $\mathcal{P}$, which is in this case simply taken to be the normal distribution with some variance such as $\sigma^2_\mathcal{P}=10^{-6}\sec$ that is orders of magnitude smaller than the the expected time step size:
    \equationnamed{Proposal Distribution}{
    \begin{equation}
      \mathcal{P} = \mathcal{N}\br{\mu = \Delta t_k, \sigma^2_\mathcal{P}}
    \end{equation}
    }

    Values that yield higher performance are then greedily accepted, all other proposals are rejected. The algorithm is outlined in Algorithm \ref{alg:local-search}.
    Note that the variance of the proposal distribution is an important hyperparameter: too high a value will mean sudden jumps in time step size, which can greatly impact the stability of the solver in this application. Variance that is too small will lead to slow convergence to optimal time step sizes, which is not ideal but more tolerable than instability, so a conservatively low choice of $\sigma^2_\mathcal{P}$ is recommended.

    \begin{algorithm}
      \caption{Monte Carlo Local Search}
      \label{alg:local-search}
      \begin{algorithmic}[1]
        \For{every time step $k$}
          \State Draw a proposed time step size $\Delta t_{k+1}' \sim \mathcal{P}$ from proposal distribution $\mathcal{P}$ 
          \State Ensure $\Delta t_{k+1}' \leq \Delta t_{CFL}$ by clamping \Comment\autoref{eq:cfl-condition}
          \State Run simulation step and measure performance $\hat{P}\br{\Delta t_{k+1}'}$ of the proposal
          \If{$\hat{P}\br{\Delta t_{k+1}'} > \hat{P}\br{\Delta t_k}$} 
            \State $\Delta t_{k+1} \gets \Delta t_{k+1}'$ \Comment accept proposal
          \Else
            \State $\Delta t_{k+1} \gets \Delta t_{k}$ \Comment reject proposal
          \EndIf 
        \EndFor
      
      \end{algorithmic}
    \end{algorithm}

    % \begin{enumerate}
    %   \item A proposed time step size $\Delta t_{k+1}' \sim \mathcal{P}\br{\Delta t_k, \sigma}$ is drawn from a proposal distribution $\mathcal{P}$ with some spread $\sigma$ about the current time step size $\Delta t_k$
    %   \item The new performance $\hat{P}_{k+1}'$ for the proposed step size is measured
    %   \item If the performance is an improvement upon $\hat{P}_k$, the time step size is accepted, otherwise it is rejected and $\Delta t_{k+1} = \Delta t_k$
    %   \item Repeat.
    % \end{enumerate}

    This greedy scheme unfortunately becomes inadequate in the context of performance that varies with time: an optimum with a performance score could be found which can no longer be matched by any proposal because the time-dependent problem has gotten more difficult in the meantime, leading to every proposal being rejected and the controller being stuck in a past optimum. To circumvent this, an \emphasis{exploration-exploitation trade-off} must famously be struck \autocite{bbo-bayesian-phd}, balancing the greedy and exploitative behaviour of the local search with exploration of the search space which increases entropy and enables the escape from convergence to local extrema \autocite{simulated-annealing}.
    
    \subsubsection{Random-Walk-Metropolis with Automatic Temperature Selection}
    The \emphasis{Random Walk Metropolis algorithm} or RWM solves this problem by accepting even non-improving proposals by using an acceptance probability of \autocite{simulated-annealing}:
    \begin{equation}\label{eq:acceptance-prob}
      \Pr\{\text{accept }\Delta t_{k+1}'\} = \begin{cases}
        \exp\br{\frac{\hat{P}_{k+1}'- \hat{P}_k}{T_k}} 
        &\hat{P}_k>\hat{P}_{k+1}'\\
        1 
        &\hat{P}_k\leq\hat{P}_{k+1}'
      \end{cases}
    \end{equation}
    where $T_k$ is a control parameter, also called 'temperature' in the context of simulated annealing \autocite{simulated-annealing} due to the physical analogy of annealing metal to find a crystalline minimum in the energy landscape of atom configurations when temperature is slowly brought down \autocite{simulated-annealing}. Note that the intended behaviour is $\lim_{T_k\searrow 0} \Pr\{\text{accept }\Delta t_{k+1}'\}=0$ to avoid division by zero. Usually, a temperature schedule for $c_k$ such as geometric cooling \autocite{simulated-annealing}, which eases the temperature towards zero, is used. The same can not be applied in this context, since $\hat{P}$ is actually time-dependent and potential changes to the function and its maximum should be explored by increasing the temperature even at a later time in the simulation.
    
    Temperature is instead controlled using the estimation for the rate of acceptance of proposals $\chi$ \autocite{simulated-annealing}:
    \begin{equation}
      \chi\br{T} \approx \frac{a+r \exp\br{-\frac{\Delta\hat{P}^-}{T}}}{a+r}
    \end{equation}
    where $a$ and $r$ are the number of improving (i.e. $\hat{P}_k\leq\hat{P}_{k+1}'$) and non-improving proposals (i.e. $\hat{P}_k>\hat{P}_{k+1}'$) respectively and $\Delta \hat{P}^- = \frac{1}{r}\sum_{k=0}^K \br{\hat{P}_k - \hat{P}_{k+1}'}_+$ is the average decrease in performance score of non-improving transitions. This can be rearranged to yield a formula for the temperature $c$ in terms of a target acceptance rate $\chi$ \autocite{simulated-annealing}:
    \equationnamed{Temperature Function}{
      \begin{equation}\label{eq:c-from-chi}
        T\br{\chi,z} 
        % = \frac{\Delta \hat{P}^-}{\ln\br{\frac{r}{\br{a+r}\chi - a}}} 
        = \frac{\Delta \hat{P}^-}{\ln\br{\frac{1-z}{\chi-z}}} \cdot \Theta\br{\chi-z}
      \end{equation}
    }
    
    where $z=\frac{a}{a+r}$ is the observed rate of improving proposals. When $z\geq\chi$ the term is undefined due to a negative argument to the logarithm and the temperature is set to zero instead by the Heaviside step function $\Theta$. This results in a temperature $T\br{\chi}$ as shown in \autoref{fig:temperature}, which differs from zero only if the rate of improvement $z$ drops below the target acceptance rate $\chi$.


    \begin{figure}
      \centering
      \begin{tikzpicture}
        \begin{axis}[
            colormap name=Spectral_r,
            % colormap/viridis, 
            xmin=0,   xmax=1,
	          ymin=0,   ymax=0.3,
            view={0}{90},
            title={Normalized Temperature  $\frac{T\br{\chi=0.234}}{T_{max}\br{\chi=0.234}}$}, 
            xlabel={Average Loss of Non-Improving Proposals $\Delta \hat{P}^-$},
            ylabel={Rate of Improving Proposals $\frac{a}{a+r}$},
            width=0.6\textwidth,
            height=0.6\textwidth,
          ]
          % add plot
          \addplot3 [
            surf, 
            samples=50,
            shader=interp, 
            domain=0:1, 
            y domain=0:0.3, 
          ] {0.234-y>0?x/ln((1-y)/(0.234-y)) * ln(1/0.234):0}; 
          % add chi line
          \addplot [
            thick, dashed, black, % Line style
            domain=0:1,           % x-domain of the line
          ] coordinates {(0, 0.234) (1, 0.234)};
          % add chi label
          \node at (axis cs: 0.1, 0.234) [anchor=south, black] {\(\chi\)};
          \addplot3 [
          contour gnuplot={
              contour dir=z,
              % levels={0.1,0.2,0.5,0.8},
              number = 4,
              labels over line,
              draw color = black,
            },
            samples=50,
            domain=0:1, 
            y domain=0:0.3, 
          % thick,
          ]{0.234-y>0?x/ln((1-y)/(0.234-y)) * ln(1/0.234):0};
          % \addplot3 [
          %  contour filled 
          % ] {0.234-y>0?x/ln((1-y)/(0.234-y)) * ln(1/0.234):0};
        \end{axis}
      \end{tikzpicture}
      \caption{The temperature $T\br{\chi=0.234}$, normalized from $[0;T_{max}]$ to $[0;1]$ is plotted for differing rates of improving proposals $z$ and average loss of non-improving proposals $\Delta \hat{P}^-$. Note that the function is scale-invariant in $\Delta\hat{P}^-$, i.e. if the temperature is normalized between its minimum and maximum values to values in $[0;1]$ then the shape of the graph does not change if $\Delta\hat{P}^-$ is plotted from zero to one or from zero to any other positive number. This scale-invariance shows that the unit of measurement of performance has no impact on the behaviour of the algorithm, which is intended. 
      The plot can be interpreted by assuming lower temperature values mean more greedy behaviour, while higher temperatures result in more exploration. When the rate of improving proposals $z$ is higher than the target acceptance rate $\chi$, the algorithm is strictly greedy. As the rate of improving proposals drops below the threshold, and particularly as $\Delta \hat{P}^-$ increases and the proposals get worse, the temperature increases so that the acceptance rate can be pushed up to the target level of $\chi$. This means that when improvement becomes stale or the current optimum is no longer valid and proposals get worse, the temperature increases to enable more exploration in hopes of discovering a better time step size $\Delta t$.}
      \label{fig:temperature}
    \end{figure}

    While this might seem like exchanging one arbitrary parameter choice for another, there is actually a reasonable default for the acceptance rate $\chi$ that can be used: surprisingly, for a wide variety of proposal and target distributions it can be shown that as the dimensionality of the problem increases, the most efficient choice of the target acceptance rate $\chi$ converges to $\chi=0.234$ \autocites{moving-beyond-0234}{exploring-0234-arxiv}. Although the applicability of this \textit{optimal scaling} to the specific problem at hand has admittedly not been thoroughly theoretically investigated, as this would exceed the scope of this report, it serves here as a reasonable default to eliminate hyperparameter-tuning. 
    
    While \autoref{eq:c-from-chi} is usually employed to set the initial temperature $T_0$ and then apply a cooling schedule \autocite{simulated-annealing}, it is here applied throughout the simulation for the reasons mentioned: temperature should increase again when the acceptance rate drops below the threshold of $\chi=0.234$, either because an optimum was found or because the underlying function $\hat{P}$ has changed and what currently appears to be an 'optimal' performance score is in fact no longer attainable. 
    To provide this flexibility at the cost of possibly oscillating about an optimal time step size, the changes of $\hat{P}$ can be estimated to occur on an order of $1\sec \gg \Delta t$ and a time horizon of e.g. $t_{hrz}=1\sec$ is introduced, beyond which observations are forgotten. Instead of calculating $T\br{\chi}$ from \autoref{eq:c-from-chi} using all observed proposals, only proposals made in the last second are considered and proposals beyond this time horizon do not contribute to the values $a$, $r$ and $\Delta \hat{P}^-$. This results in scheme that flexibly increases the temperature as to keep the rate of improving proposals in the last second of simulated time above the threshold $\chi=0.234$. 


    Lastly, the fact that the time controller is in a \textit{delayed} feedback loop and changes to the time step size might only see effects to performance after some iterations is taken into account. This delay can be accounted for and the performance function $\hat{P}$ can be made yet less noisy by dividing the simulation so-called episodes $E=\{k, k+1, \dots, k+K_{E}-1\}$, in this case of  $K_{E}=5$ time steps each. This makes rewards for a good proposal of $\Delta t'$ more likely to be captured by the performance score of the corresponding episode, instead of rewarding some later, possibly unrelated proposal.
    
    The total performance score in \autoref{eq:perf-hat-defintion} applied to one episode then becomes:
    \begin{equation}\label{eq:episode-performance}
      \hat{P}\br{E} = \frac{
        \sum_{k\in E} \Delta t_k
      }{
        K_{E}\cdot \bar{T}_{const} + \br{\sum_{k\in E} l_k} \cdot \bar{T}_{iter}
      }
    \end{equation}
    while variance $\sigma^2_\mathcal{P}$ of the proposal distribution is simply linearly scaled by the steps per episode $K_E$ because the proposed changes to $\Delta t$ are uncorrelated:
    \begin{equation}\label{eq:episode-proposal}
      \mathcal{P} = \mathcal{N}\br{\mu = \Delta t_k, K_{E}\cdot\sigma^2_\mathcal{P}}
    \end{equation}
    This results in the time step controller shown in Algorithm \ref{alg:time-step-controller}. This controller was tested on both dam-break and static water column scenarios and found to converge on time step sizes close to the optimum obtained by hand-tuning parameters for performance.

    
    \begin{algorithm}
      \caption{Random Walk Metropolis Controller for $\Delta t$}
      \label{alg:time-step-controller}
      \begin{algorithmic}[1]
        \For{every episode $E$}
          \State Draw a proposed time step size $\Delta t' \sim \mathcal{P}$ from proposal distribution $\mathcal{P}$ \Comment\autoref{eq:episode-proposal}
          \State Run simulation for $K_E$ steps using $\Delta t_k=\min\br{\Delta t', \Delta t_{CFL}}$\Comment{$\Delta t_{CFL}$ from \autoref{eq:cfl-condition}}
          \State Estimate performance $\hat{P}\br{E}$ \Comment\autoref{eq:episode-performance}
          \State Update $a,r,z$ and calculate temperature $T\br{\chi,z}$ \Comment\autoref{eq:c-from-chi}
          \State Calculate acceptance probability $\Pr\{\text{accept }\Delta t'\}$\Comment\autoref{eq:acceptance-prob}
          \State Draw uniform random variable between 0 and 1: $u\sim\mathcal{U}\br{[0;1]}$
          \If{$u < \Pr\{\text{accept }\Delta t'\}$} 
            \State $\Delta t \gets \Delta t'$ \Comment accept proposal
          \EndIf 
        \EndFor
      
      \end{algorithmic}
    \end{algorithm}



    The result of this automatic time step selection for an exemplary static water column is shown in \autoref{fig:metropolis-dt-plots}, highlighting how performance is approximately maximized through a convergence towards - as well oscillation about - an optimal time step size. Note that for both the recommended value of $\chi=0.234$ and the greedy local search with $\chi=0$ the pressure solver iteration count for the performance-optimal time step size tends towards low values, between the minimum of $3$ iterations and $10$ iterations, which indicates that the neighbourhood search as presented in \autoref{appendix:fixed-radius-neighbour-search} is fast enough to the point where pressure solver iterations are a bottleneck of the fluid solver implementation and are minimized by the algorithm.

    \begin{figure}
      \centering
      \begin{tikzpicture}
          \begin{groupplot}[
              group style={
                  group size=1 by 2,
                  vertical sep=0.2cm
              },
              width=0.8\textwidth,
              height=0.4\textwidth,
              scale only axis,
              grid=both,
              major grid style={line width=0.5pt, opacity=0.6}, 
              minor grid style={line width=0.2pt, opacity=0.3},
              legend pos=south east,
              legend cell align=left
          ]
          
          \nextgroupplot[
              ylabel={Performance $P$ [$\frac{\sec}{\sec}$]},
              xticklabels=\empty 
          ]
          \addplot[color=Spectral4, thin, opacity=1.0, mark=none] 
              table [x=0.234m_perf_x, y=0.234m_perf_y, col sep=comma]  
              {data/2025-02-09T19:34:23.820291.csv-downsampled.csv };
          \addlegendentry{current $\hat{P}\br{t}$, $\chi=0.234$}
          \addplot[color=Spectral6, thin, opacity=1.0, mark=none] 
              table [x=0m_perf_x, y=0m_perf_y, col sep=comma]  
              {data/2025-02-09T19:34:23.820291.csv-downsampled.csv };
          \addlegendentry{current $\hat{P}\br{t}$, $\chi=0.0$}

          \addplot[color=Spectral0, thick, mark=none] 
              table [x=0.234m_perf_x, y=0.234m_perf_z1, col sep=comma]  
              {data/2025-02-09T19:34:23.820291.csv-downsampled.csv };
          \addlegendentry{average $\bar{P}$, $\chi=0.234$}
          \addplot[color=Spectral2, thick, mark=none] 
              table [x=0m_perf_x, y=0m_perf_z1, col sep=comma]  
              {data/2025-02-09T19:34:23.820291.csv-downsampled.csv };
          \addlegendentry{average $\bar{P}$, $\chi=0.0$}
  
          \nextgroupplot[
              ylabel={$\Delta t$ [$\sec$]},
              xlabel={$t$ [$\sec$]},
              scaled y ticks=false,
              y label style={at={(axis description cs:-0.15,.5)},anchor=south} 
          ]
          \addplot[color=Spectral8, thick, mark=none] 
              table [x=0.234m_dt_x, y=0.234m_dt_y, col sep=comma]  
              {data/2025-02-09T19:34:23.820291.csv-downsampled.csv };
          \addlegendentry{$\chi=0.234$}
          \addplot[color=Spectral10, thick, mark=none] 
          table [x=0m_dt_x, y=0m_dt_y, col sep=comma]  
          {data/2025-02-09T19:34:23.820291.csv-downsampled.csv };
        \addlegendentry{$\chi=0$}
  
          \end{groupplot}
      \end{tikzpicture}
      \caption{A $2m\times 2m\times 5m$ static water column of $302K$ fluid and $217K$ boundary particles is simulated for $10\sec$ of simulated time using automatic time step selection with $\sigma^2 = 5\cdot 10^{-6}\sec, K_E=5$ as described in Algorithm \ref{alg:time-step-controller} using the IISPH fluid solver (\autoref{sec:iisph}). The time horizon describes the expected timescale of changes to the problem difficulty and is therefore disabled using $t_{hrz}=\infty$ for this static scene. It can be observed that the initial, conservatively small time step size is quickly and automatically increased, increasing performance up to about $5\%$ of real-time performance and subsequently oscillating about that optimum value, as the time step size oscillates about its respective optimum. The long-term average performance plateaus, its maximum yielding a robust performance metric to compare other algorithms against. A relatively high target acceptance rate of $\chi=0.234$ causes the time step size to vary throughout the entire simulation, exploring metastable local maxima, which would be desirable for dynamic scenes. A value of $\chi=0$ instead equates to a purely greedy Monte-Carlo local search as described in Algorithm \ref{alg:local-search}. This can be observed to lead to slightly faster convergence towards the optimum and less oscillation thereabout in the case of this static scene, but could result in the algorithm becoming stuck in a past optimum due to lack of exploration for a dynamic scene.}
      \label{fig:metropolis-dt-plots}
  \end{figure}
    

    
    % The timescale of such changes to $\hat{P}$ can be estimated to be on the order of seconds and a time horizon is introduced beyond which observations are forgotten. Instead of calculating $c\br{\chi}$ from \autoref{eq:c-from-chi} using all observed proposals, only proposals made in the last second are considered and proposals beyond this time horizon do not contribute to the values $a$, $r$ and $\Delta \hat{P}^-$.

    % Furthermore, reparametrizing $c\br{\chi}$ in terms of the rate of improving proposals $z := \frac{a}{a+r}$ yields:
    % \begin{equation}
    %   c\br{\chi} = \frac{\Delta \hat{P}^-}{\ln\br{\frac{1-z}{\chi-z}}}
    % \end{equation}
    % which means that when the rate of improving proposals $z$ is higher than the target acceptance rate, the argument to the logarithm is negative, and the expression is undefined. In this case, the logarithm can intuitively be thought to approach infinity, making the temperature approach zero, resulting in a greedy search as in Algorithm \ref{alg:local-search}. Since, as previously argued, such a greedy search is undesired and undefined values must be avoided in implementation, a practical way to circumvent this is to introduce a lower limit $\Pr_{min}>0$ to the acceptance probability and set $c=0$ when an undefined value would arise. In this report, $\Pr_{min}=1\%$ was chosen. The temperature as a function of $z$ and $\Delta \hat{P}^-$ is shown in \autoref{fig:temperature}.



    % It might seem as though the parameter $c$ was simply exchanged for $\chi$, but this value can surprisingly be found in a wide variety of applications, proposal and target distributions to be optimally set by the value $\chi=0.234$ 

    
    % Instead of just measuring these values directly, the computation time $T_{comp}$ can be further decomposed using assumptions derived from the structure of the simulation (Algorithm \autoref{alg:fluid-sim}) to arrive at a model such as:
    % \begin{equation}
    %   T_{comp} = T_{const} + l T_{iter} + w
    % \end{equation}
    % where $T_{const}$ is some constant cost per time step, $T_{iter}$ is some cost per iteration $l$. 
    % The algorithms chosen in this report not only have a runtime that is linear in the number of particles ($\mathcal{O}\br{N}$) but can also be expected to have relatively low variance about their expected runtime: counting sort is used for neighbourhood search, most of the SPH calculations in $\mathcal{O}\br{MN}$ in the number $M$ of neighbours per particle can be assumed to have close to the same computational cost on every invocation since incompressibility of the fluid is enforced and since memory coherency is enforced by cheap and correspondingly frequent resorting as outlined in \autoref{appendix:fixed-radius-neighbour-search}, low variations in $T_{const}$ and $T_{iter}$ can be expected. This makes it viable to approximate these terms as constants and introduce the error term $w$, which models deviations from this linear behaviour in $l$ as an additive noise, for example due to differences in average neighbour count, hardware resource allocation, clock frequency and other factors that might vary across runtime. This model has the benefit of smoothing the performance estimate, since long-term averages:
    % \begin{align}
    %   \overline{T}_{const} &= \frac{1}{K}\sum_{k=0}^K T_{const,k}\\
    %   \overline{T}_{iter} &= \frac{1}{\sum_{k=0}^K l_k}\sum_{k=0}^K  l_k \cdot T_{iter,k}
    % \end{align}
    



\chapter{Weakly Coupled Rigid Bodies}\label{chp:rigid}
    The fluid solver from \autoref{chp:fluid} is not quite complete without discussing the boundary conditions within which the mixed initial-boundary value problem is solved \autocite{tutorial2019}. A versatile method for discretizing boundary objects in a manner consistent with the SPH discretization of the fluid is discussed in \autoref{sec:boundary} \autocite{versatile-boundary-akinci}. While this already realizes one-way coupling of the fluid to a static boundary, a rigid body solver has to be developed and two-way coupled to the fluid solver to achieve interacting rigids and fluids. The prerequisite mass moments are discussed in \autoref{sec:mass-moments}, which are then used to implement a solver for rigid body kinematics in \autoref{sec:rigid-bodies}. Note that while fluids and rigids then interact, more challenging interactions such as rigid-rigid contacts with accurate friction that are an active area of research at time of writing \autocite{monolithic-rigids-timo} are not handled in this report.

    \section{Uniform and Robust Boundary Representation}\label{sec:boundary}
    %Various forces act upon a fluid when it is in contact with the surface of a boundary, from pressure forces that enforce non-penetration of the boundary over viscous forces depending on whether slip-, no-slip or other conditions are modelled and other forces such as the adhesion that results in capillary action, which are neglected in this instance. 

    A common idea in modelling boundary interactions in SPH is to discretize the boundary into particles and extend the SPH sums from \autoref{chp:sph} to include such boundary particles, which are treated in much the same way as fluid particles. This keeps the discretization of the governing equations consistent between fluid and boundary particles, using pressure forces of the incompressible fluid to guarantee non-penetration of the boundary while viscous forces or the absence thereof can realize slip or no-slip boundary conditions. 

    In order to extend the SPH sums over boundary particles, the quantities $m_k, \vek{x}_k $ and $ \vek{v}_k$ must be known for all boundary particles $k$. While $\vek{v}_k$ is zero for static boundaries and can be computed as outlined in \autoref{sec:rigid-bodies} otherwise, the remaining two quantities highlight two challenges in implementing such a boundary: the size of the boundary particles, which equates to their mass $m_k$, and the sampling of the boundary domain, which yields the $\vek{x}_k$.

    \subsection{Sampling and Initialization}\label{subsec:sampling-the-boundary}
    As will be outlined in the following, a single layer of non-uniformly distributed boundary particles on the surface of the boundary are sufficient to ensure accurate fluid-boundary interactions. This is helpful, since thin shells and complex triangular meshes should be able to be represented, which is generally not trivial or even possible when multiple layers of particles or particles of uniform size must be used. Instead, a non-uniform but low-discrepancy sampling of the boundary's surface is sought in this instance.

    Firstly, an input triangular mesh representing an arbitrary boundary geometry is approximated by a \emphasis{watertight manifold} with vertices that are roughly uniformly distributed across the surface using the method outlined by \autocite[Huang et al.]{mesh-watertight-manifold}. This method represents the geometry using an octree and reconstructs it by isosurface extraction before projecting the newly created vertices onto the original mesh to improve accuracy \autocite{mesh-watertight-manifold}.

    The resulting watertight manifold, while retaining the shape of the geometry as well as possible, amongst other desirable properties enables the notion of distance of points across the mesh to be sensible. Since the mesh is now a manifold, the distance between two points that are not on the same triangle can be calculated by taking the mesh topology into account, since only exactly two triangles in the mesh share a single edge across which the shortest path between two points can be unambiguously defined. This enables sampling techniques on the manifold that guarantee minimum or maximum distances between points, such that low-discrepancy or blue noise samplings can be found that ensure that boundary particles are evenly spread across the mesh but do not leave holes in the mesh for the fluid to penetrate.

    In this instance, \emphasis{Poisson disk sampling} was chosen as such a sampling technique. It guarantees that points are at least some radius $r$ according to some distance metric apart by, for example \autocite{fast-poisson-disk-sampling}, repeatedly sampling the spherical annulus between $r$ and $2r$ about some point in the sample set and adding points to the set that are no less than $r$ away from any other point in the set, until the entire surface is sampled. This can be done efficiently in $\mathcal{O}(N)$ in the number $N$ of points in the final sampling using auxiliary data structures such as a uniform grid \autocite{fast-poisson-disk-sampling}. An example of the blue noise distribution of boundary particles generated on a mesh is shown in \autoref{fig:boundary-sampling}

    \begin{figure}
      \centering
      \begin{subfigure}[t][0.5\textwidth]{0.5\textwidth}
        \includegraphics[width=\textwidth]{images/ship.png}
        \caption{Rendering of the mesh}
        \label{fig:ship-sample-a}
      \end{subfigure}%
      \begin{subfigure}[t][0.5\textwidth]{0.5\textwidth}
        \includegraphics[width=\textwidth]{images/ship_sample_inv.png}
        \caption{Sampling of the mesh}
        \label{fig:ship-sample-b}
      \end{subfigure}\vspace{1cm}



      \begin{subfigure}[t][0.4\textwidth]{0.4\textwidth}
        \includegraphics[width=\textwidth]{images/ship_close.png}
        \caption{Close-up view of sails and ropes}
        \label{fig:ship-sample-c}
      \end{subfigure}
      \hfill
      \begin{subfigure}[t][0.4\textwidth]{0.4\textwidth}
        \includegraphics[width=\textwidth]{images/ship_sample_close_inv.png}
        \caption{Close-up view of sampled sails and ropes}
        \label{fig:ship-sample-d}
      \end{subfigure}
      \vspace{0.5cm}
      \caption{A rendering of a triangular mesh of a ship is shown in \autoref{fig:ship-sample-a}, with a close-up view of ropes and sails that are particularly challenging to sample in \autoref{fig:ship-sample-c}. The corresponding, low-discrepancy or blue noise samplings of boundary particles obtained by Poisson Disk sampling \autocite{pcu} are shown in \autoref{fig:ship-sample-b} and \autoref{fig:ship-sample-d} respectively, where particle positions are represented by small spheres. This highlights that the algorithm samples the mesh robustly even for complex and problematic geometries such as long, acute triangles in the ropes or non-manifold input geometry, at a resolution independent of that of the input mesh.}
      \label{fig:boundary-sampling}
    \end{figure}

    Both the Poisson disk sampling and resampling of the mesh as a watertight manifold are implemented in the \textit{Point Cloud Utils} library that was employed in the implementation discussed in this report \autocite{pcu}.

    \subsubsection{Boundary Particle Effective Mass}
    With the boundary particles' positions sampled on the boundary surface according to a blue noise distribution, a method for determining the effective mass of each boundary particle as it exerts forces upon a fluid particle is required to integrate the boundary formulation into the SPH sums in \autoref{sec:sph-navier-stokes}. As seen in \autoref{chp:sph}, the SPH sum approximates an integral over field quantities, which can be partitioned into an integral over the fluid domain $\Omega_{fl}$ and the boundary $\Omega_{bdy}$, which for the purpose of handling contact forces as pressure forces using the fluid solver can be assumed to be of the rest density $\rho_0$ of the fluid under consideration. Note that in this instance $\rho_0$ is fixed, for multiphase flows it can have differing values and in any case it is distinct from the actual mass density of the material, such as wood, stone or steel, being modelled by the boundary $\Omega_{bdy}$, which instead might influence the rigid body kinematics described in \autoref{sec:rigid-bodies}.

    For the density of a fluid particle near the boundary, one can write \autocite{density-maps}:
    \begin{align}
      \rho_i &= \int_{\Omega_{fl}} W_{ij} \,dm_j+  \int_{\Omega_{bdy}} W_{ik} \,dm_j\\
      \angled{\rho_i} &= \sum_{j\in\mathcal{N}_i} m_jW_{ij} + \sum_{k\in\mathcal{B}_i} m_k W_{ik}
    \end{align}
    where $\mathcal{B}$ is defined analogously to $\mathcal{N}$ but for boundary samples that neighbour particle $i$ and $m_k$ is the effective mass of a boundary particle assuming a boundary of density $\rho_0$ in $\Omega_{bdy}$.

    Since the entire boundary volume is represented by only a single layer of particles at the interface to the fluid, missing contributions and the variation in effective mass due to the non-uniform sampling must be accounted for. It is apparent that accurate values of $m_k$ would have to be at least a function of the distance to the interface in order to capture the fact that there are more missing boundary contributions in the kernel support of some fluid particle $i$ if it is close to the boundary. This could be approximated by linearly extending the boundary density field via $m_k = \rho_0 - \frac{\rho_0\cdot \abs{\vek{x}_{ik}}}{\hbar}$ using for example signed distance fields, storing the results on a grid and interpolating them as proposed by \autocite[Koshier and Bender]{density-maps}. Instead, for the small kernel support radius $\hbar=2h$ in the expected fluid particle spacing $h$, a constant approximation that is not distance-dependent can be deemed sufficiently accurate and more efficient, yielding the method of \autocite[Akinci et al.]{versatile-boundary-akinci}.

    With this piece-wise constant model, the effective mass that achieves a boundary domain with density $\rho_0$ is $m_k = \rho_0V_k$ where $V_k$ is the effective volume of the boundary domain represented by particle $k$. This in turn can be computed using the SPH normalization criterion \autoref{eq:normalization-0th-order} implying that $\sum_{l\in\mathcal{B}_k} V_l W_{kl}\overset{!}{=} 1$ ought to hold at any boundary particle $k$ \autocite{versatile-boundary-akinci}. This means that the kernel sum effectively measures the inverse of the volume of a particle given a sampling: if not $1$ but a higher value is obtained, all $V_l$ ought to be correspondingly lower and vice versa, which means \autocite{versatile-boundary-akinci}: 
    \begin{align}
      V_k = \frac{m_k}{\rho_0} &\overset{!}{=} \frac{m_k}{\sum_{l\in\mathcal{B}_k} m_l W_{kl}} \approx \gamma_1 \frac{ m_k}{m_k \sum_{l\in\mathcal{B}_k} W_{kl}}\label{eq:boundary-mass-approximation}\\
      \Longrightarrow m_k &= \gamma_1 \frac{\rho_0}{\sum_{l\in\mathcal{B}_k} W_{kl}}\label{eq:boundary-mass-calculation}
    \end{align}
    where the parameter $\gamma_1$ in the approximation can be used to adjust the density contribution per boundary particle for differing kernel support sizes, choosing for example a value that ensures rest density for a regular sampling of a plane, however $\gamma_1=1$ is chosen in this instance. 
    Since the effective mass is assumed to be independent of distance to the boundary, $m_k$ can be precomputed once and stored for each boundary particle.
    
    Instead of using the approximation of assuming roughly equal boundary masses in \autoref{eq:boundary-mass-approximation}, the exact equation can be seen as a system of equations and solved for $m_k$ in the same fashion as discussed for initializing fluid particles with rest density in \autoref{eq:initializing-fluid-rest-density}:
    \begin{align}
      m_k^0 &= \frac{\rho_0}{\sum_{l\in\mathcal{B}_k} W_{kl}}\\
      m_k^{l+1} &\gets m_k^l \cdot \frac{\rho_0}{\sum_{l\in\mathcal{B}_k} W_{kl}}\label{eq:boundary-mass-calculation-iterative}
    \end{align}

    In the implementation, the exact system is solved once initially, using \autoref{eq:boundary-mass-calculation-iterative}, since the one-off cost of the iterative solver is acceptable, but subsequent recalculations of masses due to dynamic boundaries are handled with a single application of the approximate formula \autoref{eq:boundary-mass-calculation} by \autocite[Akinci et al.]{versatile-boundary-akinci} to avoid additional runtime cost.
    
    \subsection{Dynamic and Procedural Boundaries}\label{sec:dynamic-and-procedural-bdy}
    Not all boundaries are static: fluids might also interact with boundary particles representing dynamic rigid bodies simulated as described in \autoref{sec:rigid-bodies} or procedural boundaries, which behave according to some analytically specified equation of motion. In this implementation, procedural boundaries that implement rigid body translations were included, while rotations where neglected, since this limitation greatly simplifies the setting: given two equations $\vek{x}_{cm}(t), \vek{v}_{cm}(t)$, all boundary particles belonging to the same scripted object are shifted from their initial position by exactly $\vek{x}_{cm}(t)$ and all have the same velocity $\vek{v}_{cm}(t)$ at any point $t$ in time. 

    The previously discussed boundary handling can thankfully naturally handle intersecting boundaries. For example, a scripted boundary plane moving sinusoidally to produce waves may intersect a container of water. However, the masses of involved boundary particles might need to be recalculated so that the pressure forces exerted on the fluid by the boundary does not change where boundaries intersect. This is done according to two rules, which in conjunction conservatively estimate the set of boundary particles that need their masses recalculated according to \autoref{eq:boundary-mass-calculation} in each time step:
    \begin{enumerate}
      \item All masses of boundary particles belonging to scripted boundary objects are recalculated at every step. While performing the SPH kernel sum in \autoref{eq:boundary-mass-calculation} necessary to do so, all neighbouring particles $l$ have a counter set to 2 if they are static boundary samples.
      \item All static boundary samples with such a counter value greater than zero have their masses recalculated and the counter decremented.
    \end{enumerate}
    This means that not only are the masses of static boundary samples re-evaluated when scripted boundaries intersect them to ensure all boundary particles correctly exert forces on fluids as if they were the same fluid at rest density at all times, but the masses are also re-evaluated once more when a scripted boundary particle has just left the kernel support radius, ensuring correct masses as before. 
    
    Note that in the Taichi language used for this implementation, a quantized data type can be used to represent the counter since only 3 bits per boundary particle are required, which can be densely packed to reduce memory usage and possibly improve performance, see \autocite[Hu et al.]{quantized-taichi}

    \subsubsection{Extending the Governing Equations}\label{subsub:bdy-extend-governing}
    With the surfaces of boundary domains sampled using a blue-noise distribution and the effective boundary particle masses $m_k$ calculated such that they can be treated by the pressure solver like a fluid at rest with density $\rho_0$, the SPH discretized governing equations from \autoref{sec:sph-navier-stokes} can be extended to include the boundary domain.


    The density computation in \autoref{eq:discrete-density} is straightforward:
    \begin{equation}\label{eq:discrete-density-boundary}
      \angled{\rho_i} = \sum_{j\in\mathcal{N}_i} m_j W_{ij} + \sum_{k\in\mathcal{B}_i} m_k W_{ik}
    \end{equation}
    The viscosity computation in \autoref{eq:discrete-viscous} remains the same as before. While viscous forces at the boundary can be implemented and the symmetric forces acting on boundary particles can be integrated into the rigid body solver (see \autocite[Akinci et al.]{versatile-boundary-akinci}), slip-conditions were chosen instead in this implementation, since rather inviscid flows at larger scales are investigated.

    Lastly, the pressure computation must be adjusted. \autoref{eq:discrete-pressure} requires a density and pressure value per boundary particle $k$, which are unknown. Since the effective mass $m_k$ was chosen to represent $\rho_0$ in the boundary domain, the rest density $\rho_k\approx\rho_0$ is a reasonable approximation. For choosing a pressure value of a boundary particle, there exist many methods, such as simply mirroring the pressure of the fluid particle under consideration $p_i=p_k$ \autocite{versatile-boundary-akinci}, solving for pressures at boundary particles in the pressure solver explicitly to avoid inconsistent values of $p_k$ for different fluid particles $i$ \autocite{pressure-boundaries}{}, extrapolating the pressure field into the boundary domain using some scheme like \textit{Moving Least Squares} \autocite{mls-pressure-extrapolation}, or simply setting the pressure at boundary particles to zero \autocite{consistent-boundaries}. This choice can have a profound impact on pressure solver convergence and the quality of the resulting pressure field \autocite{consistent-boundaries}. In accordance with recent literature by \autocite[Bender et al.]{consistent-boundaries}, a so-called consistent boundary handling which imposes $p_k=0$ is chosen in this instance, which is exceptionally easy to implement, was found to improve solver stability and yield a reasonably smooth pressure field. It is equivalent to the approach of mirroring pressure values with a weighting of boundary pressure forces by a coefficient $\gamma_2=0.5$ \autocite{tutorial2019}. Using $p_k=0$, some terms in \autoref{eq:discrete-pressure} drop out, leaving the expression:
    \begin{equation}\label{eq:discrete-pressure-bdy}
      \vek{a}_i^p = \angled{-\frac{1}{\rho_i}\nabla p_i}_\parallel 
      = -\sum_{j\in\mathcal{N}_i} m_j \br{\frac{p_i}{\rho_i^2} + \frac{p_j}{\rho_j^2}} \nabla W_{ij} -\sum_{k\in\mathcal{B}_i} m_k\frac{p_i}{\rho_i^2} \nabla W_{ik}
    \end{equation}
    This expression is used to derive the diagonal element in the IISPH solver of \autoref{sec:iisph} using the additional assertion that $\rho_i\overset{!}{=}\rho_j\overset{!}{=}\rho_0$. Since this discretization of $\vek{a}_i^p$ makes use of the symmetric SPH approximation $\angled{\cdot}_\parallel$, one can easily derive the symmetric forces that the fluid particle $i$ exerts on each boundary particle $k$ using Newton's second law as \autocite{versatile-boundary-akinci}:
    \begin{equation}\label{force-on-bdy}
      f_{k\leftarrow i} = - m_i \vek{a}_{i\leftarrow k}^p = m_i m_k\frac{p_i}{\rho_i^2} \nabla W_{ik}
    \end{equation}
    which is used to exert forces and torques upon dynamic rigid bodies in \autoref{sec:rigid-bodies}.


    \section{Two-Way Coupled Rigid Body Solver}\label{sec:rigid-bodies}
    Assuming the mass properties of rigid bodies outlined in \autoref{sec:mass-moments} are known, the dynamics of unconstrained rigid bodies can be discussed in order to arrive at a solver which can be weakly coupled to the fluid solver from \autoref{chp:fluid}. Two-way coupling is achieved through two interfaces between the fluid and rigid solvers:
    \begin{enumerate}
      \item By sampling the boundary of the rigid body as explained in \autoref{sec:boundary} and moving the boundary particles as the rigid body moves, contact with the fluid is resolved in the same way as for static boundaries which exhibit one-way coupling: pressure forces computed by the fluid solver in \autoref{sec:iisph} resolve penetrations and keep the fluid from leaking into the rigid body.
      \item On the other hand, the fluid must also exert forces and torques on the rigid body, influencing its movement and creating two-way coupling. This was alluded to in \autoref{force-on-bdy}, where the pressure force exerted on a boundary particle by a fluid particle was formulated.
    \end{enumerate}

    These interfaces define the inputs and outputs of the rigid body solver. It takes a state of a rigid body and all the forces and torques that act on it, then computes an updated state of the body and finally moves all particles that sample the rigid body boundary to reflect the changes in the rigid body's state.
    These three steps are outlined in the following.

    \subsubsection{Forces and Torques}
    As per Chasles' theorem, rigid body motion can be described purely by the position and rotation of the body \autocite{classical-mechanics}, since ideal rigidity means that the relative positions of each two points in the body do not change and no other deformations such as observed in elastic bodies or fluids can occur. This means that the dynamics can be split into the two separate problems of computing the linear and angular movement of the body \autocite{classical-mechanics}. This is conveniently done by using the centre of mass $\vek{x}_{cm}$ as the origin which defines \textit{body space}. In this coordinate system, any point $\vek{x}'(t) := \vek{x}(t) - \vek{x}_{cm} $ belonging to the rigid body can be described using the equation of motion \autocite{physically-based-rigids}:
    \begin{equation}\label{eq:rigid-body-equation-of-motion}
      \vek{x}(t) = \mathds{O}(t)\vek{x}'(t=0) + \vek{x}_{cm}(t)
    \end{equation}
    where $\mathds{O}$ encodes the current rotation about the centre of mass in a rotation matrix and $\vek{x}_{cm}(t)$ is the position of the centre of mass in world space. This means that all rotational aspects of the motion are encoded in the orientation $\mathds{O}$, while the translational movement only affects the position and velocity of the centre of mass. 
    
    Accordingly, for the change in linear momentum $\vek{f}_{rigid} = \frac{d}{dt} M\vek{v}_{cm}$ and constant mass $M$ it holds that all forces $\vek{f}_i$ affecting the rigid body (in this instance the symmetric pressure forces in \autoref{force-on-bdy}) can simply be summed, including the external gravitational force acting on the body \autocite{physically-based-rigids}:
    \begin{equation}\label{eq:f-rigid}
      \vek{f}_{rigid} = \vek{f}_g + \sum_i \vek{f}_i
    \end{equation}
    irrespective of the position at which each force acts on the rigid body. This results in a single net translational force, which can be interpreted as an acceleration $\vek{a}_{cm} = \frac{1}{M}\vek{f}_{rigid}$ of the centre of mass $\vek{x}_{cm}$ and integrated with respect to time as in \autoref{eq:numerical-time-integration}.

    Meanwhile, the rate of change of the angular momentum  $\frac{d}{dt}\vek{L}=\vek{\tau}_{rigid}$ defines the net torque $\vek{\tau}_{rigid}$, which can be computed as \autocite{physically-based-rigids}:
    \begin{equation}\label{eq:rigid-net-torque}
      \vek{\tau}_{rigid} = \sum_i \vek{\tau}_i = \sum_i (\vek{x}_i-\vek{x}_{cm}) \times \vek{f}_i
    \end{equation}
    where the position $\vek{x}_i$ in world space at which the force $\vek{f}_i$ acts upon the rigid body is relevant. Intuitively, the vector $\vek{\tau}_i$ can be understood as pointing in the axis of the rotation that a force $\vek{f}_i$ would cause if the rigid body was fixed at its centre of mass and left to spin \autocite{physically-based-rigids}.
    
    \subsubsection{Updating Rigid Body State}
    Using the total input force $\vek{f}_{rigid}$ and torque $\vek{\tau}_{rigid}$, the state of the rigid body at the next time step $t+\Delta t$ must be computed from its current state. To obtain the updated position of the centre of mass $\vek{x}_{cm}\br{t+\Delta t}$ and the orientation $\mathds{O}\br{t+\Delta t}$ required to solve the equation of motion (\autoref{eq:rigid-body-equation-of-motion}), the corresponding differential quantities, namely the linear velocity $\vek{v}_{cm}$ and angular velocity $\vek{\omega}$ are required and are integrated with respect to time. 

    The linear velocity and updated centre of mass are straightforward and can be obtained using for example semi-implicit Euler time integration (\autoref{eq:numerical-time-integration}) from the linear acceleration described above as:
    \begin{align}
      \vek{v}_{cm}(t+\Delta t) &= \vek{v}_{cm}(t) + \Delta t \vek{a}_{cm}\br{t} = \vek{v}_{cm}(t) + \frac{\Delta t}{M} \vek{f}_{rigid}\br{t}\label{eq:rigid-vcm-update}\\
      \vek{x}_{cm}\br{t+\Delta t} &= \vek{x}_{cm}\br{t} + \Delta t \vek{v}_{cm}(t+\Delta t)\label{eq:rigid-xcm-update}
    \end{align}

    The angular velocity $\vek{\omega}$ points in the axis of rotation with a magnitude that encodes the speed of the revolution in radians per second \autocite{physically-based-rigids}. Analogous to how the linear velocity is related through mass to the linear momentum, the angular velocity is related through the inertia tensor to the angular momentum \autocite{classical-mechanics}:
    \begin{align}
      \vek{L}\br{t} &= \mathds{I}\br{t} \vek{\omega}\br{t}\\
      \mathds{I}\br{t}^{-1} \vek{L}\br{t} &=\vek{\omega}\br{t}\label{eq:rigid-update-omega-deriv}
    \end{align}
    where the angular momentum can be found by numerically integrating the torque with respect to time, since $\vek{\tau}=\frac{d}{dt}\vek{L}$ \autocite{physically-based-rigids}:
    \begin{equation}
      \vek{L}\br{t+\Delta t} = \vek{L}\br{t} + \Delta t \vek{\tau}_{rigid}\br{t}
    \end{equation}
    The inertia tensor (or its inverse) at some point in time $t$ can be determined as \autocite{physically-based-rigids}:
    \begin{align}
      \mathds{I}\br{t} &= \mathds{O}\br{t} \mathds{I}\br{t=0} \mathds{O}\br{t}^T\\
      \mathds{I}^{-1}\br{t} &= \mathds{O}\br{t} \mathds{I}^{-1}\br{t=0} \mathds{O}\br{t}^T
    \end{align}
    in terms of the initial inertia tensor $\mathds{I}\br{t=0}$ that is computed in \autoref{subsubsec:inertia-tensor-computation}. Note that since $\mathds{O}$ is a rotation matrix, the transpose of the matrix is its inverse. 

    The orientation $\mathds{O}$ can be updated using:
    \begin{equation}\label{eq:rigid-orientation-update}
      \mathds{O}\br{t+\Delta t} = \mathds{O}\br{t} + \Delta t \dot{\mathds{O}}\br{t}
    \end{equation}
    where $\dot{\mathds{O}}\br{t}$ or the rate of change of the orientation matrix is the skew-symmetrix matrix that encodes the angular velocity $\vek{\omega}\br{t} = \br{\omega_x, \omega_y, \omega_z}^T$, namely \autocite{physically-based-rigids}:
    \begin{equation}
      \dot{\mathds{O}}\br{t} = \begin{bmatrix}
        0&-\omega_z&\omega_y\\
        \omega_z&0&-\omega_x\\
        -\omega_y&\omega_x&0
      \end{bmatrix}
    \end{equation}
    With this, the orientation can be updated and the angular velocity at the next time step can be estimated using \autoref{eq:rigid-update-omega-deriv} as:
    \begin{align}\label{eq:rigid-update-omega}
      \vek{\omega}\br{t+\Delta t} &= \mathds{I}^{-1}\br{t+\Delta t} \vek{L}\br{t+\Delta t}\\
      &= \mathds{O}\br{t+\Delta t} \mathds{I}^{-1}\br{t=0} \mathds{O}\br{t+\Delta t}^T \vek{L}\br{t+\Delta t}
    \end{align}

    \subsubsection{Orthonormalization of $\mathds{O}$ and Quaternions}
    In \autoref{eq:rigid-orientation-update}, the orientation is updated by explicitly numerically integrating a differential change in orientation. Unfortunately, after this process, the orientation $\mathds{O}$ is not guaranteed to be a rotation matrix any more. Numerical drift can lead to accumulating errors, potentially leading to an unphysical scaling or skew of the rigid body \autocite{physically-based-rigids}, indicated by the determinant $\det\br{\mathds{O}}$ becoming different from one. To prevent this, $\mathds{O}$ could be re-orthonormalized after every so often, using for example the Gram-Schmidt process. However, this would cause the error to be projected onto arbitrarily chosen axes that depend on the order of the basis vectors considered in the process. A more elegant approach is choosing a different representation of orientations that can alleviate this issue.

    One such representation are quaternions, which extend the concept of complex numbers to four instead of two dimensions. A quaternion $s + x i + y j + z k$ has, in addition to the imaginary unit $i$, the basis vectors $j$ and $k$, and can be represented by a single vector $\vek{q} = \br{s,x,y,z}^T$ in four dimensions or a pair $\left[s,\vek{v}\right]$ of a scalar $s$ and a vector $\vek{v}=\br{x,y,z}^T$ holding its coefficients \autocite{physically-based-rigids}. Using the second definition, quaternion multiplication can be written as \autocite{physically-based-rigids}:
    \begin{align}
      \vek{q}_1 \vek{q}_2 
      &= \left[s_1,\vek{v}_1\right] \left[s_2,\vek{v}_2\right] \\
      &= \left[s_1 s_2 - \vek{v}_1 \cdot \vek{v_2}, s_1\vek{v}_2 + s_2\vek{v}_1 + \vek{v}_1 \times \vek{v}_2 \right]
    \end{align}
    which is not commutative. If a quaternion has unit length, it represents a rotation and the product of two such quaternions represents the composite of each rotation in sequence \autocite{physically-based-rigids}, making them especially convenient in this context. It turns out that the rate of change of the orientation $\dot{\vek{q}}$ can be expressed in terms of the angular velocity $\vek{\omega}$ using quaternion multiplication as:
    \begin{equation}\label{eq:rigid-quaternion-qdot}
      \dot{\vek{q}} = \frac{1}{2} \left[0,\vek{\omega}\right]\left[s,\vek{v}\right] 
    \end{equation}
   This can be explicitly numerically integrated:
    \begin{equation}\label{eq:rigid-qdot-integration}
      \vek{q}\br{t+\Delta t} = \vek{q}\br{t} + \Delta t\dot{\vek{q}}\br{t}
    \end{equation}
    and then normalized like any four-dimensional vector, so it remains a representation of a rotation. Finally, the resulting unit quaternion $\hat{\vek{q}}\br{t+\Delta t}=\br{s,x,y,z}^T$ can be converted to a rotation matrix $\mathds{O}\br{t+\Delta t}$  using the identity \autocite{physically-based-rigids}:
    \begin{equation}\label{eq:rigid-quaternion-to-matrix}
      \mathds{O} = \begin{bmatrix}
        1-2y^2-2z^2 & 2xy-2sz & 2xz+2sy\\
        2xy+2sz & 1-2x^2-2z^2 & 2yz-2sx\\
        2xz-2sy & 2yz+2sx & 1-2x^2-2y^2\\
      \end{bmatrix}
    \end{equation}
    which can be used as an auxiliary variable to compute $\mathds{I}^{-1}\br{t+\Delta t}$ and $\vek{\omega}\br{t+\Delta t}$ as outlined above, but does not need to be stored any more since $\vek{q}$ is now used to represent the current orientation of the rigid body. Note that the initial orientation can be encoded by $\hat{\vek{q}}\br{t=0} = \br{1,0,0,0}^T$.

    \subsubsection{Updating the Dynamic Boundary Particles}
    Once a new orientation $\mathds{O}\br{t+\Delta t}$ and position of the centre of mass $\vek{x}_{cm}\br{t+\Delta t}$ has been obtained, all boundary particles sampled on the surface of the rigid body can be moved according to \autocite{physically-based-rigids}:
    \begin{align}
      \vek{r}_i\br{t+\Delta t} &= \mathds{O}\br{t+\Delta t} \br{\vek{x}_i\br{t=0} - \vek{x}_{cm}\br{t=0}}\\
      \vek{x}_i\br{t+\Delta t} &= \vek{r}_i\br{t+\Delta t} + \vek{x}_{cm}\br{t+\Delta t}\label{eq:rigid-update-particle-x}\\
      \vek{v}_i\br{t+\Delta t} &= \vek{v}_{cm} + \vek{\omega}\br{t+\Delta t} \times \vek{r}_i\br{t+\Delta t}\label{eq:rigid-update-particle-v}
    \end{align}
    using the initial positions at $t=0$, where $\vek{r}_i$ was introduced for readability and need not be stored. 

    All the steps necessary to perform a single update of a rigid body are summarized in Algorithm \ref{alg:rigid-sim}.

    
    \begin{algorithm}
      \caption{Rigid Body Solver Step}
      \label{alg:rigid-sim}
      \begin{algorithmic}[1]
        \Phase{Compute Linear Movement}

        \State $\vek{v}_{cm} \gets \vek{v}_{cm} + \Delta t \br{\vek{g} + \frac{1}{M}\sum_i\vek{f}_i}$ \Comment{\autoref{eq:rigid-vcm-update}}
        \State $\vek{x}_{cm} \gets \vek{x}_{cm} + \Delta t \vek{v}_{cm}$ \Comment{\autoref{eq:rigid-xcm-update}}
        
        \Phase{Update Orientation}
        \State $\dot{\vek{q}} \gets \frac{1}{2} \left[0,\vek{\omega}\right]\vek{q}$ \Comment{\autoref{eq:rigid-quaternion-qdot} using quaternion multiplication}
        \State $\vek{q} \gets \frac{\vek{q} + \Delta t \dot{\vek{q}}}{\abs{\vek{q} + \Delta t \dot{\vek{q}}}}$ \Comment{\autoref{eq:rigid-qdot-integration} and normalize quaternion}
        \State Calculate $\mathds{O}$ from $\vek{q}$ \Comment{\autoref{eq:rigid-quaternion-to-matrix}}

        \Phase{Compute Rotational Movement}
        \State $\vek{L} \gets \vek{L} + \Delta t \vek{\tau}_{rigid}$ \Comment{using $\vek{\tau}_{rigid}$ from \autoref{eq:rigid-net-torque}}
        \State $\vek{\omega} \gets \mathds{O} \mathds{I}^{-1}\br{t=0} \mathds{O}^T \vek{L}$ \Comment{\autoref{eq:rigid-update-omega}}

        \Phase{Update Particles}
        \For{$i \in \mathds{N}_0^{N_{rigid}-1}$ in parallel}
          \State $\vek{r}_{i,0} = \br{\vek{x}_i\br{t=0} - \vek{x}_{cm}\br{t=0}}$ \Comment{Initial body space position, auxiliary variable}
          \State $\vek{x}_i \gets \mathds{O} \vek{r}_{i,0} + \vek{x}_{cm}$ \Comment{\autoref{eq:rigid-update-particle-x}}
          \State $\vek{v}_i \gets \vek{v}_{cm} + \vek{\omega}\times \br{\mathds{O}\vek{r}_{i,0}}$ \Comment{\autoref{eq:rigid-update-particle-v}}
        \EndFor
      \end{algorithmic}
    \end{algorithm}
    
    

    
  \subsection{Exact Mass Moments of Watertight Meshes}\label{sec:mass-moments}
  In order to implement the rigid body solver in \autoref{sec:rigid-bodies}, prerequisite include knowing the total mass $M$, centre of mass $\vek{x}_{cm}$, volume $V$ and the inertia tensor $\mathds{I}$ of the rigid body being simulated. As an input to this computation, some arbitrary triangular mesh is given, which can be approximated by a watertight manifold using the algorithm discussed in \autoref{subsec:sampling-the-boundary}, which is in turn interpreted as the surface $\partial\Omega_{rigid}$ of some volume $\Omega_{rigid}$ that represents a rigid body of homogeneous density $\rho$. In other words, the mass moments necessary for the computation of $M,\mathds{I},\vek{x}_{cm}$ etc. of the closed triangular mesh of $\partial\Omega_{rigid}$ must be computed, in this case using a method by \autocite[Zhang and Chen]{efficient-feature-extraction}, which is outlined in the following. The mass density moments of the $\Omega_{rigid}$ are defined as an integral \autocite{efficient-feature-extraction}:
  \begin{equation}\label{eq:mass-density-moments}
    \mathcal{M}_{pqr} := \iiint_{\Omega_{rigid}} \rho x^p y^q z^r  \, dxdydz
  \end{equation}
  over the volume of the rigid body for some $p,q,r$, which means that the zero-th moment $p=q=r=0$ is the total mass of the object, the first moment $p=q=r=1$ is used to calculate the centre of mass, and the entries of the inertia tensor can be computed from some other $p,q,r \in \mathds{N}_0^2$ that are discussed in \autoref{subsubsec:inertia-tensor-computation}.

  \subsubsection{Volume Integrals as Sums over Triangles}
  
  Using the fact that the input triangular mesh is now a closed manifold, there exists a method for computing the analytic mass moment of the mesh. 
  
  The method makes use of the notion of signed volumes $V'$ of tetrahedra, where given some vertex $O$ of a tetrahedron and the triangle $\triangle_{ABC}$ formed by its remaining, ordered vertices, it is assigned either a positive or a negative volume depending on whether the normal of the triangle $\triangle_{ABC}$ faces towards or away from $O$. Note that since the input mesh is now a watertight manifold, each connected component of the mesh defines an interior and an exterior, consists of triangles and can be stored in a consistent winding order such that the normals $\frac{\overrightarrow{AB} \times \overrightarrow{AC}}{\abs{\overrightarrow{AB} \times \overrightarrow{AC}}}$ all point towards the exterior, as is convention, or all point towards the interior. In this instance, counter-clockwise winding was used, but differing input meshes can be accounted for by flipping the sign of the signed volume accordingly.
  
  More traditionally, the integration domain $\Omega_{rigid}$ could be partitioned into a set of tetrahedra $\{\tetrahedron_i\}$ such that $\Omega_{rigid} = \bigcup_i \tetrahedron_i$ and $\forall i,j: \tetrahedron_i \cap \tetrahedron_j = \emptyset$ so that the integral can be written as:
  \begin{equation}
    \int_{\Omega_{rigid}} \,dV = \sum_{\tetrahedron_i} V\br{\tetrahedron_i}
  \end{equation} where $V\br{\tetrahedron_i}$ is the volume of the $i$-th tetrahedron that can be computed analytically from its vertices. This would necessitate a decomposition into tetrahedra that is neither trivial nor efficient. 
  
  Instead, one fixed point $O$ anywhere in space can be chosen to construct tetrahedra $\tetrahedron_{ABCO}$ by simply summing over all the triangles $\triangle_{ABC}$ in the mesh and appending the vertex $O$ to form a tetrahedron. These tetrahedra no longer partition space, so a simple sum over their volumes would not yield the correct total volume of the domain but some possibly larger value. However, the crucial insight lies in the fact that summing up the signed volumes $V'\br{\tetrahedron_{ABCO}}$ with a consistent winding order as described above results in multiply accounted for volumes outside $\Omega_{rigid}$ exactly cancelling out, leaving only the volume inside $\Omega_{rigid}$, which is accounted for an odd number of times with alternating signs. This is conceptually simlar to how parity of the number of intersections along a ray is used to determine if a point lies inside a polygon as per the Jordan Curve theorem in \autoref{subsec:initial-conditions}, only the parity of faces with normals oriented towards or away from some vertex $O$ is now used to determine subvolumes inside or outside the mesh. \autoref{fig:volume-integral-triangle-mesh} might make this concept clearer by illustrating a concrete example in two dimensions. 
  
  %The intuition here is similar to the Jordan Curve theorem mentioned in \autoref{subsec:initial-conditions}: if a ray from an $O$ outside of the mesh to some point on $\triangle_{ABC}$ is imagined, then just as the number of intersections of the ray with $\partial\Omega_{rigid}$ indicates whether a point along the ray is inside or outside the domain depending on if the number of intersections is even or odd, the ray also alternates between intersecting faces with normals in or against the ray direction, and in consequence the volume containing some point on the ray is described by an even number of terms in the sum of signed volumes with alternating signs that therefore cancel to zero, or an odd number that is always negative or always positive. \autoref{fig:volume-integral-triangle-mesh} makes this much clearer by showing a concrete example in two dimensions. 

  \begin{figure}
    \begin{center}
      \begin{asy}
import geometry;
import patterns;
size(10cm);
pair[] P = {(-1,2),(2,2.5),(5,4),(4,1)};
pair[] directions = {W, NW, NE, SE};
pair O = (1,0);

pen hatchpen = black+opacity(0.8)+linewidth(0.1);
add("hatch1",hatch(H=1.1mm, dir=NE, p=hatchpen));
add("hatch2",hatch(H=1.1mm, dir=NW, p=hatchpen));

pen style1 = red+opacity(0.2);
pen style2 = blue+opacity(0.2);
pen poly_pen = linewidth(1.0) + black;

path poly = P[0];
for (int i = 0; i < P.length; ++i) {
pair dir = directions[i % directions.length];
dot(P[i]);
label("$V_"+string(i)+"$", P[i], dir);
poly = poly -- P[i];
}
poly = poly -- cycle;
draw(poly, poly_pen);

dot(O); label("$O$", O, S);

bool isNormalTowardsO(pair A, pair B, pair O) {
pair AB = B - A;
pair AO = O - A;
real cross = AB.x * AO.y - AB.y * AO.x;
return cross > 0;
}

for (int i = 0; i < P.length; ++i) {
pair A = P[i];
pair B = P[(i+1) % P.length]; 
if (isNormalTowardsO(A, B, O)) {
  filldraw(A -- B -- O -- cycle, style1);
} else {
  filldraw(A -- B -- O -- cycle, style2);
}
}
for (int i = 0; i < P.length; ++i) {
pair A = P[i];
pair B = P[(i+1) % P.length];
if (isNormalTowardsO(A, B, O)) {
  fill(A -- B -- O -- cycle, pattern("hatch1"));
} else {
  fill(A -- B -- O -- cycle, pattern("hatch2"));
}
}
        \end{asy}
        \caption{In a figure similar to the one by \autocite[Zhang and Chen]{efficient-feature-extraction}, a volume integral using the described method of signed volumes is illustrated for a polygon with vertices $V_0,V_1,V_2,V_3$ and an additional vertex $O$ at the origin. Whereas in three dimensions, volumes of tetrahedra are added, and we sum over the triangles of a mesh, in this two-dimensional setting the areas of triangles are added in a sum over edges of the mesh. Positive signed areas are coloured blue and hatched along one diagonal, negative areas are red and hatched along the other diagonal. The purple, cross-hatched area outside the mesh is therefore added once and subtracted once, resulting in a net contribution to the total area of zero, while the area inside the polygon does not cancel out and remains as the result of the computation.}
        \label{fig:volume-integral-triangle-mesh}
      \end{center}
  \end{figure}

  Since the fixed point $O$ is arbitrary, the most computationally efficient choice is $O=\br{0,0,0}^T$. With this choice, the signed volume of any tetrahedron $\tetrahedron_{ABCO}$ can be defined as \autocites{efficient-feature-extraction}{explicit-exact-tetrahedron-formulas}:
  \begin{align}
    V'\br{\tetrahedron_{ABCO}} &= \frac{1}{6}\det\br{J\br{\tetrahedron_{ABCO}}}\\
    &= \frac{1}{6}\det\br{\begin{bmatrix}
    A_x&B_x&C_x\\A_y&B_y&C_y\\A_z&B_z&C_z
    \end{bmatrix}}\\
    &= \frac{1}{6}\br{\vek{A} \times \vek{B}} \cdot \vek{C}\label{eq:tetrahedron-signed-volume-determinant}
  \end{align}
  where $J$ denotes the Jacobian of the tetrahedron and the position vector of each vertex $\overrightarrow{OA}=\vek{A}$ etc. can be used in the Jacobian since $O$ was chosen to be the origin. The triple scalar product used to compute the determinant is anticommutative, so the order of any two of the vectors $\vek{A},\vek{B},\vek{C}$ in the expression above can be swapped to alternate the sign of the result, for example to account for a different winding order. With this, the volume integral can be written as a sum \autocite{efficient-feature-extraction}:
  \begin{equation}\label{eq:tetrahedron-signed-volume-sum}
    V\br{\Omega_{rigid}} = \int_{\Omega_{rigid}} \,dV = \abs{\sum_{\triangle_{ABC}} V'\br{\tetrahedron_{ABCO}}}
  \end{equation}
  From now on, it will be assumed that the winding order and the order of the vectors in \autoref{eq:tetrahedron-signed-volume-determinant} were chosen such that the sum of signed volumes are always positive and the absolute value $\abs{\cdot}$ in \autoref{eq:tetrahedron-signed-volume-sum} could be dropped. In other words, $\sum_{\triangle_{ABC}} V'\br{\tetrahedron_{ABCO}}$ being a positive quantity defines some function $\sgn\br{\tetrahedron_{ABCO}}\in\{-1,1\}$, which assigns a positive or negative weight to any tetrahedron $\tetrahedron_{ABCO}$, yielding the sign of its signed volume $V'$.

  One of the greatest properties of this method, apart from its exact accuracy for closed meshes, is that it generalizes to any mass moments as defined in \autoref{eq:mass-density-moments} through the relation \autocite{efficient-feature-extraction}:
  \begin{equation}\label{eq:mass-density-moments-using-signed-volume}
    \mathcal{M}_{pqr} = \sum_{\triangle_{ABC}} \br{\sgn\br{\tetrahedron_{ABCO}} \cdot \iiint_{\Omega\br{ABCO}} \rho x^p y^q z^r  \,dV}
  \end{equation}
  where $\Omega\br{ABCO}$ is the domain of the tetrahedron $\tetrahedron_{ABCO}$, for which exact analytic solutions to the triple integral exist when homogeneous density is assumed, as outlined in \autoref{chp:analytic-tetrahedron-inertia}.
  % Instead of integrating a quantity over a volume inside a triangular mesh by, for example, partitioning it into tetrahedra and summing up the analytic expressions for the integral over each tetrahedron, one additional fixed point $O$ anywhere in space can be used to construct a sum over tetrahedra $OABC$ for each triangle $ABC$ in the triangular mesh. 
  
  
  % Whereas a volume integral $\int_{\Omega_{rigid}} \,dV$ can be written as the sum of volumes of tetrahedra $\sum_{ABCD} V_{ABCD}$ for each tetrahedron $ABCD$ that partitions $\Omega_{rigid}$, the integral can now also be written as the sum of signed volumes $\sum_{ABC} V_{ABCO}'$, which is simply a sum over the triangles $ABC$ in the triangular mesh and does not require a possibly expensive decomposition into tetrahedra. 
  
  % The crucial insight into why this method works is similar to the Jordan curve theorem, except the intuition is lifted from rays to volumes as per the Jordan any volume inside the rigid body domain is accounted for an odd number of times, volumes outside are accounted for an even number of times and the sign of the signed volume alternates at every intersection with the surface $\partial\Omega_{rigid}$.
  
  \subsubsection{Zeroth Mass Density Moment: Total Mass}
  It follows that the total mass of a rigid body of homogeneous density $\rho$ is:
  \begin{equation}\label{eq:total-rigid-body-mass}
    M\br{\Omega_{rigid}} = \int_{\Omega_{rigid}} \rho \,dV= \mathcal{M}_{000}  = \sum_{\triangle_{ABC}} V'\br{\tetrahedron_{ABCO}} \rho  =:  \sum_{\triangle_{ABC}} M'\br{\tetrahedron_{ABCO}}
  \end{equation}
  which can be efficiently and conveniently computed for each face of the mesh in parallel and summed up using a parallel reduction.

  \subsubsection{First Mass Density Moments: Centre of Mass}\label{subsubsec:centre-of-mass}
  Describing changes of linear momentum of a rigid body is conveniently done by applying forces to the centre of mass, which is defined as:
  \begin{equation}
    \vek{x}_{cm}\br{\Omega_{rigid}} = \frac{1}{M\br{\Omega_{rigid}}}\int_{\Omega_{rigid}} \vek{x}' \cdot \rho\br{\vek{x}'} \, dV\br{\vek{x}'} = \frac{1}{\mathcal{M}_{000}}\begin{bmatrix}
      \mathcal{M}_{100}\\
      \mathcal{M}_{010}\\
      \mathcal{M}_{001}
    \end{bmatrix}
  \end{equation}
  Since the centre of mass of any tetrahedron is its centroid, the signed centre of mass of each tetrahedron in the sum can be given in terms of the signed mass $M'\br{\tetrahedron_{ABCO}}$ from \autoref{eq:total-rigid-body-mass}:
  \begin{equation}
    \vek{x}_{cm}\br{\tetrahedron_{ABCO}} = \frac{M'\br{\tetrahedron_{ABCO}}}{4}\br{\vek{A}+\vek{B}+\vek{C}}
  \end{equation}
  which means the centre of mass of the entire rigid body can be computed as:
  \begin{equation}
    \vek{x}_{cm} = \frac{1}{M} \sum_{\triangle_{ABC}}\vek{x}_{cm}\br{\tetrahedron_{ABCO}}
  \end{equation}

  \subsubsection{Second Mass Density Moments: Inertia Tensor}\label{subsubsec:inertia-tensor-computation}
  To describe how torques create rotations of rigid bodies, the inertia of the body about any axis has to be known. The inertia can be thought of as the quotient of the angular momentum $\vek{L}$ and the angular velocity $\vek{\omega}$, yielding a second rank tensor $\mathds{I}$ which is a $3\times 3$ matrix in three dimensions with diagonal elements called \textit{moments of inertia} and non-diagonals referred to as \textit{products of inertia} \autocite{classical-mechanics}. The eigenvectors of the inertia tensor are the principal axes of rotation; it is a symmetric tensor with six degrees of freedom and is defined as \autocites{fast-accurate-polyhedral-mass-properties}{classical-mechanics}:
  \begin{equation}\label{eq:inertia-tensor-long-definition}
    \mathds{I} = \begin{bmatrix}
      \int_\Omega \rho\br{x,y,z} \br{y^2+z^2} \,dV&
      -\int_\Omega  \rho\br{x,y,z} xy \,dV&
      -\int_\Omega  \rho\br{x,y,z} xz \,dV\\
      -\int_\Omega  \rho\br{x,y,z} yx \,dV&
      \int_\Omega  \rho\br{x,y,z} \br{x^2+z^2} \,dV&
      -\int_\Omega  \rho\br{x,y,z} yz \,dV\\
      -\int_\Omega  \rho\br{x,y,z} zx \,dV&
      -\int_\Omega  \rho\br{x,y,z} zy \,dV&
      \int_\Omega   \rho\br{x,y,z} \br{x^2+y^2} \,dV\\
    \end{bmatrix}
  \end{equation}
  Note that the symmetry $I_{ab} = I_{ba}$ results in the six degrees of freedom and that for homogeneous density, $\rho$ can be moved out of the integral. Further, since $\int_\Omega x^2+y^2\,dV = \int_\Omega x^2\,dV + \int_\Omega y^2\,dV$, the inertia tensor can be written in terms of the mass density moments $\mathcal{M}_{pqr}$ as:
  \begin{align}
    \mathds{I} &= \begin{bmatrix}
      I_{xx}&-I_{xy}&-I_{xz}\\
      -I_{xy}&I_{yy}&-I_{yz}\\
      -I_{xz}&-I_{yz}&I_{zz}
    \end{bmatrix}\\
    I_{xx} &= \mathcal{M}_{200}\\
    I_{yy} &= \mathcal{M}_{020}\\
    I_{zz} &= \mathcal{M}_{002}\\
    I_{xy} &= \mathcal{M}_{100} + \mathcal{M}_{010}\\
    I_{xz} &= \mathcal{M}_{100} + \mathcal{M}_{001}\\
    I_{yz} &= \mathcal{M}_{010} + \mathcal{M}_{001}
  \end{align}
  where \autoref{eq:mass-density-moments-using-signed-volume} can be applied to calculate each invocation of $\mathcal{M}$ efficiently. In \autoref{eq:mass-density-moments-using-signed-volume}, the analytic solution to volume integrals over tetrahedra $\tetrahedron_{ABCO}$ are required, which in this instance have the form:
  \begin{align}
    \rho \int_{\Omega\br{ABCO}} x^2+y^2\, dV\\
    \rho \int_{\Omega\br{ABCO}} xy\, dV
  \end{align}
  and analogously for the other two axes. These exact analytic expressions are cited from \autocite[Tonon]{explicit-exact-tetrahedron-formulas} and moved to \autoref{chp:analytic-tetrahedron-inertia} since they are rather verbose. Using \autoref{eq:inertia-appendix-final-eq}, the final inertia tensor $\mathds{I}$ can be computed. With this, all quantities are known and a simple rigid body solver as discussed in \autoref{sec:rigid-bodies} can be implemented.


\chapter{Visualization}\label{chp:visualization}

  A crucial factor in the application of fluid solvers based on SPH to Computer Graphics is the visualization of the simulation results. Even if the application of the simulation is focused on engineering and the resultant data, some form of visual inspection is invariably part of the process of fluid simulation.
  
  While \autoref{subsec:sampling-the-boundary} is concerned with the sampling of particles on a triangular mesh, visualization of the simulated scene requires a reconstruction of triangular meshes for rendering. To this end, simulated rigid bodies must be correctly displayed, the surface of the fluid in the case of hydrodynamic simulations must be reconstructed (unlike the simulation of e.g. air, which would require some other form of visualization) and diffuse particles representing spray, foam and air bubbles that are particularly prominent in large-scale liquid simulations can be generated to greatly improve the visual realism of the scene \autocite{unified-spray-foam-bubbles}, all of which is outlined in the following chapter. The focus of this report lies on the synthesis of photorealistic videos of virtual scenes, while scientific visualizations such as sensor planes (as observed in e.g. \autocite[this paper]{compressed-neighbour-lists}) or streamline plots are not discussed.
  

  Note that the time step size $\Delta t$ of the simulation is typically on scales of temporal resolution much finer than what is required to display a smooth video ($\Delta t \ll 16\si{\mu\second} = \Delta t_{frame}$ for 60 frames per second), which means that only relatively few points in time must be visualized, which reduces computational cost. For sufficiently small step sizes $\Delta t$, one can simply visualize the simulation data at each of the smallest values of $t$ larger than some integer multiple of the frame time $t_{frame}$. Other methods, such as interpolating the simulation results temporally to the exact point in time $i\in\mathds{N}: i\cdot t_{frame}$ shown in the $i$-th frame are also conceivable.

  \subsubsection{Visualizing Boundaries}
  The visualization of boundaries is trivial in the case of static boundaries: exactly the triangular mesh that served as input to the particle sampling, e.g. as described in \autoref{subsec:sampling-the-boundary}, can be used for visualization.

  Procedural boundaries as described in \autoref{sec:dynamic-and-procedural-bdy} are similarly unproblematic as they simply require a translational offset defined by $\vek{x}_{cm}\br{t}$ or an additional rotation about their centre of mass (which was not described in this report). 

  Dynamic rigid bodies on the other hand, if implemented as described in \autoref{sec:rigid-bodies} should be noted to be described in body space with the centre of mass at the origin, not in the global coordinate system. Since transformation do not generally commute, this means that before the rotation encoded by the orientation matrix $\mathds{O}$, or equivalently the quaternion $\vek{q}$, can be applied to the body, the centre of mass of the object that the rotation refers to must be set as the origin. For a body with an initial centre of mass $\vek{x}_{cm}\br{t=0}$ in global coordinates, one must:
  \begin{enumerate}
    \item translate the centre of mass to the origin by adding $-\vek{x}_{cm}\br{t=0}$
    \item apply the rotation encoded by $\mathds{O}\br{t}$ or $\vek{q}\br{t}$
    \item translate the rotated body to its current position by adding $\vek{x}_{cm}\br{t}$
  \end{enumerate}
  to every vertex of the input mesh at $t=0$. Since only rigid body dynamics are described in this report and deformations of the mesh therefore do not occur, this procedure suffices to render the effects of the solver.


  \section{Surface Reconstruction}
  The fluid is less trivial to visualize, since an explicit representation of the fluid surface is not usually part of the simulation and must instead be reconstructed from the explicitly computed quantities in a post-processing step. Although methods that \autocite[explicitly operate on a surface representation]{surface-only-fluids} exist, most simulation methods in which field quantities are computed throughout the continuum rely on an isosurface reconstruction of a scalar field to construct a surface mesh from a set of particle positions. In this report, the \texttt{splashsurf} utility implementing \autocite[Weighted Laplacian Smoothing for Surface Recontruction]{laplacian-surf-reconst} was used, the main concepts of which are outlined in the following.
  
  It is intuitively clear that the density field, for example, is suited to indicating whether any point in space lies within or outside the fluid: a density near the rest density of the fluid certainly implies that a point is inside the fluid volume, while a density close to zero certainly belongs to a point outside the fluid. The fluid surface can then be approximated by an isosurface of some threshold value $t$, which can be set ad-hoc to yield visually plausible results. Assuming that particles have (approximately, see \autoref{subsec:initial-conditions}) uniform sizes and masses, a commonly used alternative to the density field that is suited for post-processing scenarios where only particle positions are known is the so-called \emphasis{colour field}, which can be defined as the standard SPH discretization (\autoref{eq:sph-sum}) of the indicator field of the fluid volume and which is therefore closely related to the density field \autocite{laplacian-surf-reconst}:
  \begin{equation}\label{eq:colour-field-def}
    \angled{1}\br{\vek{x}} = \sum_{j\in\mathcal{N}_{\vek{x}}} \frac{m_j}{\rho_j} W(\vek{x}-\vek{x}_j, \hbar)
  \end{equation}
  
  Recall from \autoref{sec:sph-discretization} that the SPH interpolation scheme is valid not only at particle positions $\vek{x}_i$ but at any position $\vek{x}$ in space, as observed in the definition of the colour field in \autoref{eq:colour-field-def}. This means that a common building block in surface reconstruction can be applied here, namely the \emphasis{Marching Cubes Algorithm} \autocite{marching-cubes}. This algorithm subdivides space into a uniform grid of cubes and classifies each of the resulting vertices as \textit{inside} or \textit{outside} of a volume depending on whether a scalar field at the vertex position is above or below the threshold $t$. It follows that the surface of the volume must lie between each adjacent pair of vertices that have differing in-out classifications, resulting in $2^3=256$ possible triangle topologies per 3-dimensional cube under consideration \autocite{marching-cubes}. The corresponding triangle topology can therefore simply be stored in a table and looked up. The approximate position of the surface vertex along each edge of the uniform grid can be found through linear interpolation. More conretely this means that the sign of a potential \autocite{laplacian-surf-reconst}:
  \begin{align}
    \Phi(\vek{x}) &= \angled{1}\br{\vek{x}} - t &\textit{see \autoref{eq:colour-field-def}}
  \end{align} 
  the zero-set of which is the surface, can be used to classify vertices as inside or outside ($\vek{x}_{in}, \vek{x}_{out}$). The surface vertex $\vek{x}_s$ between two such vertices can be approximated via linear interpolation as \autocite{laplacian-surf-reconst}:
  \begin{align}
    \vek{x}_s = \vek{x}_{out} + \frac{t-\Phi(\vek{x}_{out})}{\Phi(\vek{x}_{in})-\Phi(\vek{x}_{out})} \br{\vek{x}_{in}-\vek{x}_{out}}
  \end{align}
  
  Even if the Marching Cubes algorithm is executed at a resolution much finer than the expected particle spacing $h$ so that individual particles and splashes are resolved, the result can unfortunately include many \textit{bumps} resulting from the limited resolution of both the particle sampling and the reconstruction. To combat this, especially in regions that are supposed to represent flat, planar regions, explicit \emphasis{Laplacian Smoothing} can be applied to the resulting surface mesh \autocite{laplacian-surf-reconst}. In this procedure, vertices of the surface mesh obtained from marching cubes are interpolated using some weight $\lambda\in[0;1]$ towards the average position of all vertices that are immeadiate edge-neighbours of the vertex under consideration. While a diffusion process with uniform weights does result in less curvature, smoothing out planar regions of the surface, it can also cause volume shrinkage, especially for droplets and splashes that may collapse in on themselves \autocite{laplacian-surf-reconst}. Instead, as the name of the \autocite[Weighted Laplacian Smoothing]{laplacian-surf-reconst} implies, weights $\lambda$ can be employed that produce more smoothing in planar regions, where higher neighbour counts of particles can be measured, and less smoothing for isolated particles and splashes, where particles $i$ have a smaller number of neighbours $\abs{\mathcal{N}_{i}}$ \autocite{laplacian-surf-reconst}. The neighbour count can be interpolated to vertex positions via standard SPH interpolation $\angled{\cdot}$ (\autoref{eq:sph-sum}), clamped to a maximum value $N_{HS}$ that is achieved in a planar, half-space particle configuration and finally a smooth step function such as $S(x)=6x^5-15x^4+10x^3$ can be used to distribute the resulting values in the $[0;1]$ interval required of the weights $\lambda$, resulting in \autocite{laplacian-surf-reconst}:
  \begin{align}
    \angled{\abs{\mathcal{N}}} &= \sum_{j\in\mathcal{N}_{\vek{x}}} \frac{m_j}{p_j} \abs{\mathcal{N}_j} W\br{\vek{x}-\vek{x}_j, \hbar}\\
    \lambda\br{\vek{x}} &= S\br{\max\br{1, \frac{\angled{\abs{\mathcal{N}}}}{N_{HS}\angled{1}}}}
  \end{align}
  With these weights, each vertex $\vek{x}^k$ in the $k$-th smoothing iteration can be moved towards the average position $\bar{\vek{x}}$ of neighbouring vertices using \autocite{laplacian-surf-reconst}:
  \begin{equation}
    \vek{x}^{k+1} = \vek{x}^{k} + \lambda \bar{\vek{x}}
  \end{equation}
  yielding the final smoothing procedure applied to the Marching Cubes reconstruction.

  The \autocite[original paper]{laplacian-surf-reconst} goes more into depth about implementation details, as well as improvements to temporal coherence of resulting meshes in animations and an improved subsequent mesh decimation procedure. This report cites only the main principles required for a basic implementation and uses the \texttt{splashsurf} utility by the same authors as an optimized implementation.

  \section{Spray, Foam and Bubble Generation}
  In reality, large masses of water clashing against solid obstacles result in a misty spray, waves carry patches of foam that might slowly dissolve and moving water may trap air, creating air bubbles. Since water-air mixture effects originating from free surfaces are prevalent, especially for masses of water on the scale of meters and beyond, emulating such effects in simulations can have a significant impact on the perceived realism of an animated scene \autocite{spray-foam-bubbles}. An example of such an improvement in perceived realism is shown in \autoref{fig:foam-with-without}.

  \begin{figure}
    \centering
    \begin{subfigure}[t][0.5\textwidth]{0.5\textwidth}
      \includegraphics[width=\textwidth]{images/foam-side-without.png}
      \caption{Close-up rendering without diffuse particles}
      \label{fig:foam-a}
    \end{subfigure}%
    \begin{subfigure}[t][0.5\textwidth]{0.5\textwidth}
      \includegraphics[width=\textwidth]{images/foam-side-with.png}
      \caption{Close-up rendering with diffuse particles}
      \label{fig:foam-b}
    \end{subfigure}\vspace{1cm}


    \begin{subfigure}[t][0.5\textwidth]{0.5\textwidth}
      \includegraphics[width=\textwidth]{images/foam-top-without.png}
      \caption{Overview rendering without diffuse particles}
      \label{fig:foam-c}
    \end{subfigure}%
    \begin{subfigure}[t][0.5\textwidth]{0.5\textwidth}
      \includegraphics[width=\textwidth]{images/foam-top-with.png}
      \caption{Overview rendering with diffuse particles}
      \label{fig:foam-d}
    \end{subfigure}
    \vspace{0.5cm}
    \caption{Rendering comparison of a scene with a large dynamic battleship floating in water by a coast, moved by waves that crash upon the shore - while the right-hand \autoref{fig:foam-b} and \autoref{fig:foam-d} are rendered with the addition of diffuse particles representing spray, foam and air bubbles, \autoref{fig:foam-a} and \autoref{fig:foam-c} neglect such effects. The rendering with added diffuse particles can appear much more visually plausible, faithfully rendering the dynamicity of the scene, where patches of foam act as a visual clue to the movement present in the ocean. The plain rendering without spray, foam and bubbles on the other hand can seem still, since there is less visual indication of the dynamics of the waves and water surface, making the liquid appear too calm or even viscous. The effect is especially prominent at the shoreline and around the stern of the ship.}
    \label{fig:foam-with-without}
  \end{figure}
  
  To this end, for particle-based simulations and SPH fluid simulations in particular, a popular method was introduced by Ihmsen et al. in their paper \autocite[Unified Spray, Foam and Bubbles for Particle-Based Fluids]{spray-foam-bubbles}, with suggestions for improvement addressed by \autocite[Bender et al.]{turbulent-micropolar-foam} in the context of micropolar SPH fluids, which were implemented and are outlined in the following. No explicit interactions between diffuse particles are simulated, which makes the process trivially parallelizable and scale into millions of diffuse particles without prohibitive computational cost. The diffuse particles are instead generated, advected and destroyed as a post-processing step using the particle positions and velocity field obtained from the fluid simulation, according to physically and visually plausible but relatively ad-hoc criteria that have proven effective in practice \autocites{turbulent-micropolar-foam}{unified-spray-foam-bubbles}.

  Three main processes are considered in the generation of diffuse particles where water and air mix, resulting in so-called potentials $I$ that are used to determine the number of particles spawned \autocite{spray-foam-bubbles}:
  \begin{enumerate}
    \item Whitewater is created at wave crests, characterized by a convex surface with strong curvature, captured by the potential $I_{wc}$
    \item Air is trapped into bubbles and dragged underwater at sites of impact, where water clashes with particles of sufficiently opposing velocity, measured by a potential $I_{ta}$
    \item More spray, foam and bubbles are created when the formerly mentioned effects occur with higher kinetic energy of the water, while slow-moving water has a smaller potential $I_{kin}$ for water-air mixture. Higher kinetic energy equates for a simplified, constant surface tension to a higher Weber number, causing more interaction of the two phases \autocite{spray-foam-bubbles}
  \end{enumerate}

  \subsubsection{Generating Potentials}
  To identify strongly convex areas, the surface normal can be approximated using the  normalized, negative SPH kernel gradient as:
  \begin{equation}
    \vek{n}_i = - \sum_{j\in\mathcal{N}_i} \nabla W_{ij}
  \end{equation}
  which can be normalized to obtain $\hat{\vek{n}}_i$ or set to $\vek{0}$ if the magnitude $\abs{\vek{n}}$ is insufficiently large, such as inside the fluid volume. This basically leverages the particle deficiency phenomenon from \autoref{subsec:particle-deficiency} to detect surface normals from the incomplete particle samplings in the neighbourhood. 

  Since such irregular sampling at the free surface is not only expected but relied upon, an alternative Kernel function $K$ that better handles such scenarios is used instead of the cubic spline kernel $W$ (\autoref{eq:kernel-definition}, \autoref{eq:cubic-spline-form-func}) otherwise employed in this report, which is designed to be accurate within the fluid volume \autocite{spray-foam-bubbles}. The kernel is additionally normalized as per \autocite[later suggestions]{turbulent-micropolar-foam}, which makes the process more scale-invariant:
  \begin{equation}\label{eq:k-kernel-def}
    K\br{\vek{x}_{ij}, \hbar} = 
    \underbrace{ \frac{3}{\pi\hbar^3}}_{\text{normalization}} \cdot 
    \underbrace{1-\frac{\abs{\vek{x}_{ij}}}{\hbar}}_{\text{linear kernel}} \cdot 
   \underbrace{ \Theta\br{\hbar-\abs{\vek{x}_{ij}}}}_{= 1 \text{ if } \abs{\vek{x}_{ij}}<\hbar \text{ else } 0}
  \end{equation}

  Using this kernel, the curvature at some fluid particle $i$ can be measured and weighted using a step function $\Theta$ such that only convex curvature contributes to a value $\kappa$ \autocites{spray-foam-bubbles}{turbulent-micropolar-foam}:
  \begin{equation}
    \kappa_i = \sum_{j\in\mathcal{N}_i} 
    \underbrace{\br{1-\hat{\vek{n}}_i \cdot \hat{\vek{n}}_j}}_{\text{measure curvature}} 
    \cdot 
    K_{ij}\cdot 
    \underbrace{\Theta\br{\vek{x}_{ij}\cdot \hat{\vek{n}}_i}}_{\text{only when convex}}
  \end{equation}
  To identify wave crests, an additional criterion is used that excludes other strongly convex areas such as edges of the liquid volume: the particle under consideration should significantly move in the direction of the surface normal, i.e. \autocite{spray-foam-bubbles}:
  \begin{equation}
    \delta_i = \kappa_i \cdot \Theta\br{\hat{\vek{v}}_{i} \cdot \hat{\vek{n}}_i - 0.6}
  \end{equation}
  where $\hat{\vek{v}}_{i}$ is the normalized velocity of particle $i$ and the threshold of $0.6$ is set ad-hoc.

  The value of $\delta_i$ is rather arbitrary and scene-dependent, so it is clamped between minimum and maximum values and normalized to a percentage in $[0;1]$ therein for subsequent computations, using a clamping and normalization function $\Phi$ like:
  \begin{equation}
    \Phi\br{x,\tau_{min},\tau_{max}} = \frac{\min\br{x, \tau_{max}} - \min\br{x, \tau_{min}}}{\tau_{max}-\tau_{min}}
  \end{equation}
  where $\tau_{max}\geq\tau_{min}$ are user-defined. 

  This yields a final generating potential for wave-crest diffuse particles of:
  \begin{equation}
    I_{wc} = \Phi(\delta_i,\tau_{min}^{wc}, \tau_{max}^{wc})
  \end{equation}

  Similarly, a potential in $[0;1]$ can be derived that captures the effect of air bubbles becoming entrapped by a liquid where free surfaces of the liquid phase collide with sufficiently opposing velocities. The basis of this potential is simply the scaled velocity difference \autocite{spray-foam-bubbles}:
  \begin{align}
    v_i^{diff} &= \sum_{j\in\mathcal{N}_i} \abs{\vek{v}_{ij}} \br{1-\hat{\vek{v}}_{ij}\cdot \hat{\vek{x}}_{ij}} K_{ij}\\
    I_{ta} &= \Phi(v_i^{diff}, \tau_{min}^{ta}, \tau_{max}^{ta})
  \end{align}

  Lastly, the kinetic energy of a particle is used as basis of a potential. To make the process again more resolution-invariant, in this report the mass is normalized using the expected rest mass given some expected particle spacing $h$ and a rest density $\rho_0$, which yields:
  \begin{align}
    E_{kin,i} &= \frac{1}{2} \underbrace{\frac{m_i}{\rho_0 h^3}}_{=\frac{m_i}{m_0}} \abs{\vek{v}_i}^2\\
    I_{kin} &= \Phi(E_{kin,i},\tau_{min}^{kin}, \tau_{max}^{kin})
  \end{align}

  These potentials can then be used to determine the number of diffuse particles $N_d\in\mathds{N}$ spawned at some time step \autocite{turbulent-micropolar-foam}:
  \begin{equation}\label{eq:number-of-diffuse-particles}
    N_d = \round{k \cdot I_k \frac{I_{ta}+I_{wc}}{2} \Delta t}
  \end{equation}
  where $\Delta t$ is not necessarily related to the simulation's time step size but rather the simulated time between frames in which foam is generated (e.g. $60$ such frames per simulated second might be required for video) and the factor $k$ is the maximum number of diffuse particles spawned per second of simulated time by each fluid particle \autocite{turbulent-micropolar-foam}. Note that the kinetic energy influences both the amount of foam on wave crests and the amount of trapped air where liquids collide, which is reflected in its multiplicative contribution to the sum of other potentials in \autoref{eq:number-of-diffuse-particles}.


  % Since this leads to a lot of user-defined parameters, \autocite[subsequent authors suggest]{turbulent-micropolar-foam} always setting $\tau_{min}=0.1\cdot \tau_{max}$. The normalization of $K$ in \autoref{eq:k-kernel-def} as well as a normalization of the kinetic energy that is discussed later makes the process mostly scale-independent, meaning a cheap, low-resolution simulation of a scene can be observed to tune these parameters, which are then reused in a more expensive simulation. An alternative to this is 

\chapter{Analysis}\label{chp:analysis}

    \begin{figure}
      \centering
      \includegraphics[width=0.3\textwidth]{images/water_column_10m.png}
      \caption{An exemplary rendering of the static water column used as a scene for performance analysis, showing particles as opaque spheres of radius $\sfrac{1}{2}\cdot h$ in the particle spacing $h=5\text{cm}$. Pressure is colour coded from low pressures in blue through yellow and orange towards red. This particular square column measures $\sim 303K$ fluid particles and a size of $2\text{m}\times 2\text{m}\times 10\text{m}$, where the height of $10\text{m}$ is referred to as the scene's complexity. The same $2\text{m}\times 2\text{m}\times 20\text{m}$ container consisting of $\sim 218K$ boundary particles is used in every measurement. The rendering is taken from a simulation using the standard, cold-started IISPH solver in Alg. \ref{alg:fluid-sim} at $t=1\text{s}$.}
      \label{fig:water-column-analysis}
    \end{figure}

    \section{IISPH Performance Analysis}
      \subsection{Impact of Solver Warm Start}
      
      \begin{figure}[H]
        \centering
        \begin{tikzpicture}
            \begin{groupplot}[
                group style={
                    group size=1 by 2,
                    vertical sep=0.2cm
                },
                width=0.8\textwidth,
                height=0.5\textwidth,
                xtick={1,2,3,4,5,6,7,8,9,10}, 
                xticklabels={1,2,3,4,5,6,7,8,9,10}, 
                xmode = log,
                ymode = log,
                scale only axis,
                grid=both,
                major grid style={line width=0.5pt, opacity=0.6}, 
                minor grid style={line width=0.2pt, opacity=0.3},
                legend pos=north east,
                legend cell align=left,
            ]
            \nextgroupplot[
                ylabel={Maximum Average Performance $P_{ma}$ $\left[1\right]$},
                scaled y ticks=false, 
                log ticks with fixed point,
                ytick={0.1,0.2,0.3,0.4,0.5,0.6,0.7},
                xticklabels=\empty 
            ]
            % warm
            \addplot[color=Spectral1, thick, opacity=1.0, mark=+] 
              coordinates {
                (1,0.6798306206)
                (2,0.372243451)
                (3,0.2518277304)
                (4,0.15770651)
                (5,0.1249504856)
                (6,0.1048419902)
                (7,0.08748183575)
                (8,0.07311690697)
                (9,0.06252409248)
                (10,0.0531657203)
              };
            \addlegendentry{DI, $\omega_{ws} = \sfrac{1}{2}$}
            % cold
            \addplot[color=Spectral9, thick, opacity=1.0, mark=x] 
            coordinates {
              (1,0.4537626277)
              (2,0.2329950359)
              (3,0.1631942925)
              (4,0.1194333828)
              (5,0.08787928428)
              (6,0.07107035603)
              (7,0.05838867964)
              (8,0.04971676195)
              (9,0.04276539149)
              (10,0.03679585171)
            };
            \addlegendentry{DI, $\omega_{ws} = 0$}
            %warmdiv
            \addplot[color=Spectral3, thick, opacity=1.0, mark=x] 
            coordinates {
              (1,0.5385523929)
              (2,0.3020319067)
              (3,0.2153756888)
              (4,0.1505137617)
              (5,0.1201067319)
              (6,0.09287561123)
              (7,0.07688392422)
              (8,0.06381194204)
              (9,0.05724000061)
              (10,0.04579583137)
            };
            \addlegendentry{VD, $\omega_{ws} = \sfrac{1}{2}$}
            % pcisph
            \addplot[color=Spectral7, thick, opacity=1.0, mark=x] 
            coordinates {
              (1,0.401088082)
              (2,0.2043414331)
              (3,0.1357822858)
              (4,0.09314407363)
              (5,0.06570613774)
              (6,0.05052346743)
              (7,0.04319535045)
              (8,0.03651593125)
              (9,0.02979157969)
              (10,0.02605508325)
            };
            \addlegendentry{PCISPH}
            % swcsph
            % \addplot[color=Spectral0, thick, opacity=1.0, mark=x] 
            % coordinates {
            %   (1,0.4616346624)
            %   (2,0.337648426)
            %   (3,0.2338920145)
            %   (4,0.1862576063)
            %   (5,0.1469577925)
            %   (6,0.1234819562)
            %   (7,0.1037128491)
            %   (8,0.08794407351)
            %   (9,0.07719089099)
            %   (10,0.06821404544)
            % };
            % \addlegendentry{WCSPH}

            \nextgroupplot[
                ylabel={Improvement Factor $[1]$},
                xlabel={Water Column Height $\left[\text{m}\right]$},
                scaled y ticks=false,
                xmode=log,
                ymode=linear,
                height=0.15\textwidth,
                % y label style={at={(axis description cs:-0.15,.5)},anchor=south} 
            ]
            \addplot[color=Spectral1, thick, opacity=1.0, mark=*] 
              coordinates {
                % quotient warm di / cold di
                (1,1.498207607)
                (2,1.597645416)
                (3,1.54311604)
                (4,1.320455859)
                (5,1.421842321)
                (6,1.475185944)
                (7,1.498267066)
                (8,1.470669128)
                (9,1.462025491)
                (10,1.444883535)
              };
            \addplot[color=Spectral3, thick, opacity=1.0, mark=*] 
              coordinates {
                % quotient warm vd / cold di
                (1,1.186859296)
                (2,1.296301896)
                (3,1.31975013)
                (4,1.260231923)
                (5,1.366724057)
                (6,1.306812241)
                (7,1.316760795)
                (8,1.283509616)
                (9,1.338465489)
                (10,1.244592237)
              };
            % average values
            \addplot[mark=none, Spectral1, opacity=0.5, dashed] coordinates 
              {(1,1.4732298407266096) (10,1.4732298407266096)};
            \addplot[mark=none, Spectral3, opacity=0.5, dashed] coordinates 
              {(1,1.2920007678569807) (10,1.2920007678569807)};
            \end{groupplot}
        \end{tikzpicture}
        \caption{The relative performance of the standard IISPH fluid solver outlined in Alg. \ref{alg:fluid-sim} is compared between a warm start $\omega_{ws}=0.5$ as defined in \autoref{eq:iisph-warm-start}, which initializes pressure values to half the value obtained in the previous time step, and a cold start with $\omega_{ws}=0$ that resets the pressure values to zero at the start of each time step. To measure the performance impact, the long-time average of the estimated performance metric defined in \autoref{eq:perf-hat-defintion} is taken and its maximum value across a simulation of $60\sec$ of simulated time $P_{ma}\br{\omega_{ws}} = \max_K \frac{1}{K}\sum_{1}^{K}\hat{P}_k = \max_K \bar{P}_K$ is plotted in the upper graph. An example of the estimated average performance graph $\bar{P}(t)$ converging towards its maximum due to the time step size controller outlined in \autoref{sec:optimal-time-step-controller} can be observed in \autoref{fig:metropolis-dt-plots} - in this instance, the maximum performance that is converged towards is measured. This maximum average performance is measured for scenes of varying complexity using water columns as shown in \autoref{fig:water-column-analysis} of increasing heights. Assuming the curves in the graph are approximately linear and parallel, both variations of the IISPH solver decrease in performance exponentially as the scene complexity increases, but the warm start results in a speed-up modelled by a constant factor. This factor is shown in the lower subplot for varying scene complexities and seems to oscillate about an  improvement factor of about $\times 1.5$ when using a warm-started solver.}
        \label{fig:ana-cold-warm}
    \end{figure}
      

      \subsection{Impact of Alternative Source Terms}
    \section{Comparison of IISPH with Iterated WCSPH}

\begin{appendices}
    \chapter{Fixed Radius Neighbour Search}\label{appendix:fixed-radius-neighbour-search}
    There exist numerous methods for implementing the computation of the neighbour sets $\mathcal{N}_i = \{j : \abs{\vek{x}_{ij}}<\hbar\}$ referred to in \autoref{sec:sph-discretization}. In this implementation, a GPU-friendly index sort based on counting sort was implemented as an implicit representation of a uniform grid to speed up neighbour computation, following the description of \autocite[Hoetzlein]{hoetzlein-rama-counting-sort}. 

    It is apparent that a na\"ive neighbour search incurs a cost on the order of $\mathcal{O}\br{N^2}$ in the number $N$ of particles, where each of the (ordered) pairs $i,j$ is considered and included in the set based on the predicate $\abs{\vek{x}_{ij}}<\hbar$. Instead, space may be partitioned into a uniform, axis-aligned grid of cell size $\hbar$, such that for any particle $i$ in $d$ dimensions, only the particles within the $3^d$ grid cells immediately adjacent to the grid cell in which $i$ resides have to be considered, reducing the computational complexity to $\mathcal{O}(MN)$ for compactly supported kernels $\hbar<\infty$ with at most $M$ particles per grid cell \autocite{tutorial2019}. 
    
    It should be noted that an efficient and convenient, explicit uniform grid can be constructed in the Taichi language directly using \texttt{pointer}, \texttt{bitmasked}, \texttt{hashed} and \texttt{dynamic} nodes to create a tree structure that is compiled to behave like a simple 3-dimensional field of grid cells, which contain a list of particle indices, as outlined in \cite[this paper]{taichi-sparse}. While such an implementation yielded comparable results to the method described in the following and could have been further tuned, it was superseded in this instance by a method more common in the literature surrounding the problem at hand, which relies on no ad-hoc parameters specific to the simulation domain or implementation.

    \subsubsection{Construction}
    Firstly,  the grid cell a particle $i$ resides in is computed as \autocite{compressed-neighbour-lists}:
    \begin{equation}
      c(x_i) =  \left\lfloor\frac{x-x_{min}}{\hbar}\right\rfloor
    \end{equation}
    which can be point-wise lifted to a vectorial function $(k,l,m)^T = \vek{c}\br{\vek{x}_i}$ using the point $\vek{x}_{min}$ that defines the lowest extent of the axis-aligned bounding box of the simulation domain across each axis, with $(x_{max}, y_{max}, z_{max})^T = \vek{x}_{max}$ defined analogously. 
    
    The domain is discretized into a uniform grid that can be represented by a linear, one dimensional array using a space-filling curve. While Morton codes are a popular choice \autocite{compressed-neighbour-lists} for ensuring that spatial proximity is mirrored by proximity in memory, improving cache coherency and reducing scattered reads \autocite{hoetzlein-rama-counting-sort}, the XYZ curve or natural order was chosen instead since it guarantees that any neighbour search results in exactly $3^{d-1}$ coherent sections of memory to be read, simplifying an efficient implementation without use of a BigMin-LitMax algorithm \autocite{bigminlitmax} that a Z-order curve would necessitate.
    
    The indices $k,l,m$ along each axis can be flattened into a one-dimensional index using the XYZ curve:
    \begin{equation}\label{eq:counting-sort-xyz}
      I\br{\br{k,l,m}^T} = k + l\cdot K + m\cdot LM
    \end{equation}
    where $K=c(x_{max})+1, L=c(y_{max})+1, M=c(z_{max})+1$ are the  number of grid cells along the x,y and z-axis respectively. $I$ now acts as an index into a one-dimensional array of size $N_{grid} = K\cdot L\cdot M$. The remainder of the construction is performed as follows:
    \begin{enumerate}
      \item Let \texttt{indices} be an array of size $N$ representing the one-dimensional cell index of each particle and \texttt{counts} be an array of size $N_{grid}$ representing the number of particles in each cell.

      With one parallel loop over particles $i$, the cell index $I\br{\vek{c}(\vek{x}_i)}$ of each particle can be computed and saved to the $i$-th entry of \texttt{indices}, while the $I\br{\vek{c}(\vek{x}_i)}$-th entry of \texttt{counts} is simultaneously incremented using an atomic add operation.
      \item A parallel exclusive prefix sum or prescan of the particle counts per cell can then be performed, yielding an array \texttt{counts}$_<$ of size $N_{grid}$ of cumulative particle counts of all cells with strictly lower indices than the cell under consideration. In Taichi, a work-efficient and optimized parallel inclusive prefix sum that avoids bank conflicts \autocite{parallel-prefix-scan} and makes use of \autocite[Blelloch scans]{blelloch-scans} is already implemented. An inclusive prefix sum or scan can be converted to a prescan by point-wise subtracting \texttt{counts}.
      \item Finally, a counting sort can be performed, yielding an array \texttt{sorted} of size $N$ representing the particle indices stored in the XYZ-order of the cells they appear in.
      
      This is done by looping over all particles $i$ in parallel again, this time storing the particle index $i$ in \texttt{sorted} at the position calculated by looking up the cumulative particle count of cells with lesser indices than the cell of $i$ in $\texttt{counts}_<$ and adding the result of atomically decrementing the particle count in the cell of $i$ from a copy of \texttt{count}
    \end{enumerate}
    This algorithm is outlined in Algorithm \ref{alg:counting-sort-construction}

    \begin{algorithm}
      \caption{Counting Sort-based Uniform Grid Construction}
      \label{alg:counting-sort-construction}
      \begin{algorithmic}[1]
        \Phase{Compute Indices and Counts}
        \For{$i \in \mathds{N}_0^{N-1}$ in parallel}
          \State Compute index $I_i \gets I\br{\vek{c}\br{\vek{x}_i}}$\Comment{XYZ curve of \autoref{eq:counting-sort-xyz}}
          \State $\texttt{indices}\left[i\right] \gets I_i$
          \State $\texttt{counts}\left[I_i\right] \gets \texttt{counts}\left[I_i\right] + 1$\Comment{atomically increment}
        \EndFor
        
        \Phase{Exclusive Prefix Sum}
        \State Compute $\texttt{counts}_<$ from parallel prescan of $\texttt{counts}$\Comment{See \autocite[Harris et al.]{parallel-prefix-scan}}
    
        \Phase{Counting Sort}
        \For{$i \in \mathds{N}_0^{N-1}$ in parallel}
        \State $I_i \gets \texttt{indices}\left[ i \right] $
        \State $o_1 \gets \texttt{counts}_<\left[I_i \right]$\Comment{offset due to previous cells}
        \State $o_2,  \texttt{counts}\left[ I_i\right] \gets \texttt{counts}\left[I_i\right] - 1 $\Comment{offset within cell, atomically decrement!}
        \State $\texttt{sorted}\left[o_1 + o_2 - 1\right] \gets i$
        \EndFor
      \end{algorithmic}
    \end{algorithm}


    \subsubsection{Query}
    Using the array of sorted particle indices, the index function $I$ and the counts of particles per cell as well as its prefix, all particles in a given cell can be queried in constant time. For a particle in cell $I_i$, all $3^d$ surrounding cells are queried. For each such surrounding cell with index $I_j$, the first particle in that cell has the index $\texttt{sorted}[\texttt{counts}_<]$ and the remaining particles in the same cell are the $\texttt{counts}[I_j]$ subsequent indices. 

    In fact, one optimization due to the use of the XYZ curve is that to find the neighbours of a particle in cell $I(k,l,m)$ in three dimensions, only the first particle in $I(k-1, l, m)$ has to be found, after which the next $\sum_{i=-1}^{i=1}\texttt{counts}[I(k-i,l,m)]$ particles necessary to complete the query along the x-axis lie subsequent in memory, necessitating only nine coherent sections of memory to be read instead of $27$ in the worst case for three dimensions.

    \subsubsection{Discussion}
    The algorithm shown \autocite{hoetzlein-rama-counting-sort} is simple, elegant, can achieve high performance and is very much suited for massively parallel hardware. One disadvantage however, is that the representation contains arrays that scale with the size of the simulation domain, which is a limitation compared to data structures that can more easily adapt to infinite domains and consume less memory for very sparsely filled domains, such as compact hashing \autocites{tutorial2019}{compressed-neighbour-lists}. Since the explicit representation is small, with only a few 32-bit numbers stored per grid cell, this was not found to have a noticeable impact in this instance.

    The simulation bounds $\vek{x}_{min},\vek{x}_{max}$ must be known for the XYZ-curve to be applicable in the manner outlined here. In the implementation, the boundaries of the simulation domain are strictly enforced using clamping, such that $\vek{x}_i$ is guaranteed to lie in the axis aligned bounding box spanned by $\vek{x}_{min},\vek{x}_{max}$. A grid with two additional grid cells along each axis in both the positive and negative directions is then used, such that bound checks are unnecessary and branches in the hot code path of the query procedure are avoided.
    
    In a simulation using structure-of-arrays data layouts, as is common in high-performance applications, particle attributes $\vek{x}_i, \vek{v}_i, m_i$ etc. can be re-sorted to restore memory coherency and make particles that are likely to be neighbours likely to be adjacent in memory, leading to less cache misses and better performance. One advantage of this sorting-based approach is that the $\texttt{sorted}$ buffer is computed for neighbourhood computations and can be reused at no additional cost to perform such a resorting along the space-filling curve.

  \chapter{Exact Inertia Tensor Terms for Tetrahedra}\label{chp:analytic-tetrahedron-inertia}
  We repeat the definition of the inertia tensor, the mass moments used in it and the equation used to compute these moments from \autoref{sec:mass-moments}:
  \begin{align}
    \mathds{I} &= \begin{bmatrix}
      I_{xx}&-I_{xy}&-I_{xz}\\
      -I_{xy}&I_{yy}&-I_{yz}\\
      -I_{xz}&-I_{yz}&I_{zz}
    \end{bmatrix}\\
    I_{xx} &= \mathcal{M}_{200}\\
    I_{yy} &= \mathcal{M}_{020}\\
    I_{zz} &= \mathcal{M}_{002}\\
    I_{xy} &= \mathcal{M}_{100} + \mathcal{M}_{010}\\
    I_{xz} &= \mathcal{M}_{100} + \mathcal{M}_{001}\\
    I_{yz} &= \mathcal{M}_{010} + \mathcal{M}_{001}\\
    \mathcal{M}_{pqr} &= \sum_{\triangle_{ABC}} \br{\sgn\br{\tetrahedron_{ABCO}} \cdot \underbrace{\iiint_{\Omega\br{ABCO}} \rho x^p y^q z^r  \,dV}_{\text{anaytical solution for $\rho=$ const}}}\label{eq:appendix-tetra-mass-moment}
  \end{align}
  Only the solution to the volume integrals in \autoref{eq:appendix-tetra-mass-moment} are yet to be shown. Exact analytic formulas for this integral in terms of the vertex coordinates $\vek{A},\vek{B},\vek{C},\vek{D}$ of a general tetrahedron were taken from \autocite[Tonon]{explicit-exact-tetrahedron-formulas}, with the only difference being that a minor error in the paper ($b'$ and $c'$ are swapped) was fixed and that by using the origin $\vek{O} = \vek{D}$ as one of the four vertices, some terms drop out \autocite{explicit-exact-tetrahedron-formulas}:
  % the determinant of the Jacobian $\det\br{J\br{\tetrahedron_{ABCO}}}$ is used instead of the absolute value of the determinant to account for $V'$,
  \begin{align}
    \frac{I_{xx}\br{\tetrahedron_{ABCO}}}{\rho \abs{\det(J\br{\tetrahedron_{ABCO}})} } &= \frac{\br{
      A_y^2 + A_yB_y+B_y^2 + A_yC_y + B_yC_y + C_y^2 + 
      A_z^2 + A_zB_z+B_z^2 + A_zC_z + B_zC_z + C_z^2
    }}{60}\\
    \frac{I_{yy}\br{\tetrahedron_{ABCO}}}{\rho \abs{\det(J\br{\tetrahedron_{ABCO}})} } &= \frac{\br{
      A_x^2 + A_xB_x+B_x^2 + A_xC_x + B_xC_x + C_x^2 + 
      A_z^2 + A_zB_z+B_z^2 + A_zC_z + B_zC_z + C_z^2
    }}{60}\\
    \frac{I_{zz}\br{\tetrahedron_{ABCO}}}{\rho \abs{\det(J\br{\tetrahedron_{ABCO}})} } &= \frac{\br{
      A_x^2 + A_xB_x+B_x^2 + A_xC_x + B_xC_x + C_x^2 + 
      A_y^2 + A_yB_y+B_y^2 + A_yC_y + B_yC_y + C_y^2
    }}{60}\\
    \frac{I_{xy}\br{\tetrahedron_{ABCO}}}{\rho \abs{\det(J\br{\tetrahedron_{ABCO}})} } &= \frac{\br{
      2A_xA_y + B_xA_y + C_xA_y + A_xB_y + 
      2B_xB_y + C_xB_y + A_xC_y + B_xC_y + 
      2C_xC_y 
    }}{120}\\
    \frac{I_{xz}\br{\tetrahedron_{ABCO}}}{\rho \abs{\det(J\br{\tetrahedron_{ABCO}})} } &= \frac{\br{
      2A_xA_z + B_xA_z + C_xA_z + A_xB_z + 
      2B_xB_z + C_xB_z + A_xC_z + B_xC_z + 
      2C_xC_z 
    }}{120}\\
    \frac{I_{yz}\br{\tetrahedron_{ABCO}}}{\rho \abs{\det(J\br{\tetrahedron_{ABCO}})} } &= \frac{\br{
      2A_yA_z + B_yA_z + C_yA_z + A_yB_z + 
      2B_yB_z + C_yB_z + A_yC_z + B_yC_z + 
      2C_yC_z 
    }}{120}
  \end{align}
  where $\rho \abs{\det(J\br{\tetrahedron_{ABCO}})} $ was moved to the left side of the equations to save space. 
  
  Since the origin was chosen as one of the vertices and the density is homogeneous, these rather lengthy expressions can be greatly simplified using vectorial notation. At the same time, the absolute value around the determinant can be dropped if the determinant is computed using the triple scalar product of \autoref{eq:tetrahedron-signed-volume-determinant}, which already contains $\sgn\br{\tetrahedron_{ABCO}}$ and therefore yields values $I_{ij}', I'_{kk}$ that can be directly accumulated in a sum across all triangles $\triangle_{ABC}$.  Here, $i,j,k$ are used to index the components of the vertices. Let $\vek{V}=\br{A_i, B_i, C_i}^T$ and $\vek{W}=\br{A_j, B_j, C_j}^T$ be the vectors of the $i$-th and $j$-th components of $\vek{A}, \vek{B}, \vek{C}$ for $i\neq j \neq k \neq i$ and one can obtain:
  \begin{equation}
    I_{kk}'\br{\tetrahedron_{ABCO}} = \frac{\rho \det(J\br{\tetrahedron_{ABCO}})}{120}\br{
      \vek{V}^2 + \br{\abs{\vek{V}}_{L1}}^2 +\vek{W}^2 +  \br{\abs{\vek{W}}_{L1}}^2
    }
  \end{equation}
  for the moments of inertia on the diagonal using $\vek{V}^2=\vek{V}\cdot\vek{V}$ and the $L_1$ or sum norm $\abs{\vek{V}}_{L1} = A_i+B_i+C_i$ as well as: \begin{equation}
    I_{ij}'\br{\tetrahedron_{ABCO}}  = \frac{\rho \det(J\br{\tetrahedron_{ABCO}})}{120}\br{ \begin{bmatrix}
      2&1&1\\
      1&2&1\\
      1&1&2
    \end{bmatrix} \vek{V}} \cdot \vek{W}
  \end{equation}
  for the products of inertia using a matrix multiplication and a dot product, which is much more concise than the original formulation. These terms can trivially be computed on massively parallel hardware for each triangle $\triangle_{ABC}$ and summed using a parallel reduction to yield the components $I_{kk}^{sum}, I_{ij}^{sum}$ of the inertia tensor:
  \begin{align}
    I_{kk}^{sum} &= \sum_{\triangle_{ABC}} I_{kk}' \br{\tetrahedron_{ABCO}}\\
    I_{ij}^{sum} &= \sum_{\triangle_{ABC}} I_{ij}' \br{\tetrahedron_{ABCO}}
  \end{align}

  Lastly, note that the resulting elements of the tensor describe the inertia with respect to a rotation about the origin of the arbitrarily chosen coordinate system, whereas $\mathds{I}$ should in this case describe the inertia tensor for rotations about the centre of mass, which can be calculated as laid out in \autoref{subsubsec:centre-of-mass}. To transform the inertia tensor elements accordingly, the \textit{transfer-of-axis relations} can be applied: \autocite{fast-accurate-polyhedral-mass-properties}\begin{align}\label{eq:inertia-appendix-final-eq}
    I_{kk} &= I_{kk}^{sum} - M\br{r_i^2+r_j^2}\\
    I_{ij} &= I_{ij}^{sum} - Mr_ir_j
  \end{align}
  for $\vek{x}_{cm}=(r_x,r_y,r_z)^T$ and a body of mass $M$. 



\end{appendices}

\printbibliography[
  heading=bibintoc,
  title={Bibliography}
]
\end{document}